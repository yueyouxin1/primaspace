# Context Export
# Root: primaspace

# ========== FILE: src/app/engine/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/agent/__init__.py ==========
```python
# src/app/engine/agent/__init__.py
from .main import AgentEngineService
from .base import (
    AgentInput,
    AgentStep,
    AgentResult,
    AgentEngineCallbacks,
    BaseToolExecutor
)
```

# ========== FILE: src/app/engine/agent/base.py ==========
```python
# src/app/engine/agent/base.py

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from pydantic import BaseModel, Field
from ..model.llm.base import LLMRunConfig, LLMMessage, LLMTool, LLMToolCall, LLMUsage

# --- Agent 引擎的输入输出模型 ---
class AgentInput(BaseModel):
    messages: List[LLMMessage]
    # ... 未来可以添加更多配置，如 user_id, session_id 等

class AgentStep(BaseModel):
    """代表 Agent 思维链中的一个步骤，用于可观察性"""
    thought: Optional[str] = Field(None, description="模型的思考过程")
    action: LLMToolCall = Field(..., description="模型决定执行的动作")
    observation: Any = Field(..., description="执行动作后返回的观察结果")

class AgentResult(BaseModel):
    """Agent 引擎的最终输出"""
    message: LLMMessage
    steps: List[AgentStep] = Field([], description="完整的思维链步骤")
    usage: LLMUsage = Field(default_factory=LLMUsage, description="整个Agent执行过程中的总Token用量")

# --- Agent 引擎的回调协议 ---
class AgentEngineCallbacks(ABC):
    """
    定义了 Agent 引擎向外报告事件的接口。
    """
    @abstractmethod
    async def on_agent_start(self) -> None:
        ...

    @abstractmethod
    async def on_tool_calls_generated(self, tool_calls: List[LLMToolCall]) -> None:
        """当模型决定调用工具时调用。"""
        ...

    @abstractmethod
    async def on_agent_step(self, step: AgentStep) -> None:
        """当 Agent 完成一个完整的 思维-行动-观察 步骤时调用"""
        ...
    
    @abstractmethod
    async def on_final_chunk_generated(self, chunk: str) -> None:
        """当最终答案的文本块生成时调用（流式）"""
        ...
    
    @abstractmethod
    async def on_agent_finish(self, result: AgentResult) -> None:
        ...
    
    @abstractmethod
    async def on_agent_cancel(self, result: AgentResult) -> None:
        ...

    @abstractmethod
    async def on_agent_error(self, error: Exception) -> None:
        ...

    @abstractmethod
    async def on_usage(self, usage: LLMUsage) -> None:
        """在生成结束后，报告本次调用的token用量。"""
        ...
        
# --- Agent 引擎的插件化执行器协议 ---
class BaseToolExecutor(ABC):
    """
    工具执行器的协议
    """
    @abstractmethod
    async def execute(self, tool_name: str, tool_args: Dict[str, Any]) -> Any:
        ...
    
    @abstractmethod
    def get_llm_tools(self) -> List[LLMTool]:
        ...
```

# ========== FILE: src/app/engine/agent/example/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/agent/example/full_cycle_with_rag.py ==========
```python
# src/app/engine/agent/example/full_cycle_with_rag.py

import os
import asyncio
import json
from typing import List
from app.engine.agent import (
    AgentEngineService,
    AgentInput,
    AgentStep,
    AgentResult,
    BaseToolExecutor,
    AgentEngineCallbacks
)
from app.engine.model.llm import (
    LLMEngineService,
    LLMProviderConfig,
    LLMRunConfig,
    LLMMessage,
    LLMTool,
    LLMToolCall,
    LLMUsage
)

# --- Configuration ---
API_KEY = "sk-LAdEXTUw5P"
if not API_KEY:
    raise ValueError("API_KEY environment variable not set.")
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_NAME = "qwen-plus-2025-09-11"
PROVIDER_CONFIG = LLMProviderConfig(client_name="openai", base_url=BASE_URL, api_key=API_KEY)
# ---------------------

# --- 1. CONCRETE IMPLEMENTATIONS of the Agent's "Plugins" ---

class MyToolExecutor(BaseToolExecutor):
    """
    A concrete implementation of the tool executor protocol.
    This class knows how to execute a predefined set of tools.
    """
    def _get_weather(self, location: str, unit: str = "celsius"):
        """A mock tool that returns static weather data."""
        print(f"\n[Tool] Executing get_weather(location='{location}', unit='{unit}')")
        if "tokyo" in location.lower():
            return {"temperature": "10", "condition": "Cloudy"}
        return {"temperature": "25", "condition": "Sunny"}

    def _get_stock_price(self, ticker: str):
        """A mock tool that returns a static stock price."""
        print(f"\n[Tool] Executing get_stock_price(ticker='{ticker}')")
        if ticker == "PRISMA":
            return {"price": 125.50, "currency": "USD"}
        return {"price": "unknown", "currency": "USD"}
        
    async def execute(self, tool_name: str, tool_args: dict) -> dict:
        if tool_name == "get_weather":
            return self._get_weather(**tool_args)
        elif tool_name == "get_stock_price":
            return self._get_stock_price(**tool_args)
        else:
            return {"error": f"Tool '{tool_name}' not found."}

    def get_llm_tools(self) -> List[LLMTool]:
        # These definitions must match the implementations in MyToolExecutor
        WEATHER_TOOL = LLMTool(function={
            "name": "get_weather", "description": "Get the current weather in a location.",
            "parameters": {"type": "object", "properties": {"location": {"type": "string"}}, "required": ["location"]}
        })
        STOCK_TOOL = LLMTool(function={
            "name": "get_stock_price", "description": "Get the latest stock price for a ticker symbol.",
            "parameters": {"type": "object", "properties": {"ticker": {"type": "string"}}, "required": ["ticker"]}
        })
        return [WEATHER_TOOL, STOCK_TOOL]


class MyRAGProcessor:
    """
    A concrete implementation of the context processor protocol.
    This simulates a RAG (Retrieval-Augmented Generation) step by injecting
    relevant information into the context before the LLM sees it.
    """
    async def process(self, messages: list[LLMMessage]) -> list[LLMMessage]:
        last_user_message = next((m.content for m in reversed(messages) if m.role == 'user'), "")
        
        # Simulate a knowledge base lookup
        if "primaspace" in last_user_message.lower():
            print("\n[RAG] Found relevant context for 'Primaspace'. Injecting into context...")
            retrieved_context = (
                "Primaspace Inc. is a fictional company that develops advanced AI engine architectures. "
                "Its stock ticker is PRISMA."
            )
            rag_message = LLMMessage(role="system", content=f"CONTEXT: {retrieved_context}")
            # Prepend the retrieved context to the message history
            return [rag_message] + messages
            
        print("\n[RAG] No relevant context found. Proceeding without injection.")
        return messages


class MyAgentCallbacks(AgentEngineCallbacks):
    """
    A concrete implementation of the agent callbacks to provide observability.
    """
    async def on_agent_start(self) -> None:
        print("\n--- Agent Run Started ---")

    async def on_agent_step(self, step: AgentStep) -> None:
        """This is the most important callback for observing the ReAct loop."""
        print("\n--- Agent Step Completed ---")
        print(f"  Action: Call tool '{step.action.function['name']}'")
        print(f"  Arguments: {step.action.function['arguments']}")
        print(f"  Observation: {json.dumps(step.observation)}")
        print("--------------------------")

    async def on_tool_calls_generated(self, tool_calls: List[LLMToolCall]) -> None:
        pass

    async def on_final_chunk_generated(self, chunk: str) -> None:
        """Stream the final answer to the console."""
        print(chunk, end="", flush=True)

    async def on_agent_finish(self, result: AgentResult) -> None:
        print("\n\n--- Agent Run Finished ---")
        print(f"Total Steps: {len(result.steps)}")
        print("--------------------------")

    async def on_agent_cancel(self, result: AgentResult) -> None:
        print("\n\n--- Agent Run Cancel ---")
        print(f"Total Steps: {len(result.steps)}")
        print("--------------------------")

    async def on_agent_error(self, error: Exception) -> None:
        print(f"\n\n--- Agent Error ---\n{type(error).__name__}: {error}")

    async def on_usage(self, usage: LLMUsage) -> None:
        print(f"\n\n--- Step Usage ---\n{usage}")

async def main():
    """
    Main function to set up and run the AgentEngineService.
    """
    print(">>> Running Example: Full Agent Cycle with RAG and Tools <<<")

    # --- 2. DEFINE the tools the LLM can use ---

    # --- 3. INSTANTIATE all components ---
    llm_engine = LLMEngineService()
    my_tool_executor = MyToolExecutor()
    my_rag_processor = MyRAGProcessor()
    my_agent_callbacks = MyAgentCallbacks()

    # --- 4. INITIALIZE the Agent Engine, injecting its dependencies ---
    agent_engine = AgentEngineService(
        llm_engine=llm_engine
    )

    # --- 5. PREPARE the inputs for the agent run ---
    # This prompt is designed to trigger both the RAG processor and multiple tool calls
    user_prompt = "What is the stock price for Primaspace Inc., and what's the weather in Tokyo?"
    
    messages = await my_rag_processor.process([
        LLMMessage(role="user", content=user_prompt)
    ])
    agent_input = AgentInput(messages=messages)
    
    run_config = LLMRunConfig(
        model=MODEL_NAME,
        tools=my_tool_executor.get_llm_tools()
    )

    # --- 6. RUN the agent ---
    try:
        result: AgentResult = await agent_engine.run(
            agent_input=agent_input,
            provider_config=PROVIDER_CONFIG,
            run_config=run_config,
            callbacks=my_agent_callbacks,
            tool_executor=my_tool_executor
        )
        print(f"Agent RESULT: {result}")
    except Exception as e:
        print(f"\nAn exception occurred in the agent run: {e}")


if __name__ == "__main__":
    asyncio.run(main())
```

# ========== FILE: src/app/engine/agent/example/test_agent_json_mode.py ==========
```python
# src/app/engine/agent/example/test_agent_prompt_json.py

import os
import asyncio
import json
import re
from typing import List, Dict, Any, Optional
from app.engine.agent import (
    AgentEngineService,
    AgentInput,
    AgentStep,
    AgentResult,
    BaseToolExecutor,
    AgentEngineCallbacks
)
from app.engine.model.llm import (
    LLMEngineService,
    LLMProviderConfig,
    LLMRunConfig,
    LLMMessage,
    LLMTool,
    LLMToolCall,
    LLMUsage
)

# --- Configuration ---
API_KEY = "sk-LAdEXTUw5P" 
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_NAME = "qwen-plus-2025-09-11"

PROVIDER_CONFIG = LLMProviderConfig(
    client_name="openai", 
    base_url=BASE_URL, 
    api_key=API_KEY
)

# --- Mock Tool ---

class StockInfoTool(BaseToolExecutor):
    """
    模拟股票查询工具。
    """
    async def execute(self, tool_name: str, tool_args: dict) -> dict:
        symbol = tool_args.get("symbol", "").upper()
        print(f"\n    [Tool] 查询股票: {symbol}")
        
        # 模拟返回硬编码数据
        mock_db = {
            "BABA": {"price": 85.5, "currency": "USD", "change": "+1.2%"},
            "AAPL": {"price": 175.0, "currency": "USD", "change": "-0.5%"},
            "TENCENT": {"price": 300.0, "currency": "HKD", "change": "+2.0%"}
        }
        
        result = mock_db.get(symbol)
        if result:
            return result
        return {"error": "Stock symbol not found"}

    def get_llm_tools(self) -> List[LLMTool]:
        return [
            LLMTool(function={
                "name": "get_stock_price",
                "description": "获取股票的当前价格信息",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "symbol": {"type": "string", "description": "股票代码，如 BABA, AAPL"}
                    },
                    "required": ["symbol"]
                }
            })
        ]

class SimpleCallback(AgentEngineCallbacks):
    async def on_agent_start(self) -> None:
        pass # 保持输出简洁
    
    async def on_agent_step(self, step: AgentStep) -> None:
        print(f"  > [Agent 步骤] 调用工具: {step.action.function['name']}")

    async def on_tool_calls_generated(self, tool_calls: List[LLMToolCall]) -> None:
        pass
    async def on_final_chunk_generated(self, chunk: str) -> None:
        pass
    async def on_agent_finish(self, result: AgentResult) -> None:
        pass
    async def on_agent_cancel(self, result: AgentResult) -> None:
        pass
    async def on_agent_error(self, error: Exception) -> None:
        print(f"Agent Error: {error}")
    async def on_usage(self, usage: LLMUsage) -> None:
        pass

# --- Helper Function ---

def extract_json_from_text(text: str) -> Optional[Dict]:
    """
    从模型输出中提取 JSON。
    模型通过 Prompt 生成 JSON 时，经常会包含 Markdown 标记（```json ... ```）。
    此函数负责清洗这些标记。
    """
    # 1. 尝试直接解析
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # 2. 尝试提取代码块 ```json ... ```
    pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
    match = re.search(pattern, text)
    if match:
        json_str = match.group(1)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass
            
    # 3. 尝试寻找第一个 { 和最后一个 }
    start = text.find("{")
    end = text.rfind("}")
    if start != -1 and end != -1:
        json_str = text[start : end + 1]
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass
            
    return None

# --- Test Case ---

async def test_prompt_driven_json():
    print("\n" + "="*80)
    print(">>> Test: Prompt-Driven JSON Output (Without Native JSON Mode) <<<")
    print("="*80)

    llm_engine = LLMEngineService()
    agent_engine = AgentEngineService(llm_engine=llm_engine)
    tool_executor = StockInfoTool()
    
    # 1. 配置: 关键点 —— response_format=None
    # 让 Agent 像普通聊天一样自由运行，这样它就会正常调用工具。
    run_config = LLMRunConfig(
        model=MODEL_NAME,
        tools=tool_executor.get_llm_tools(),
        temperature=0.1, 
        response_format=None # 显式关闭 JSON 模式
    )

    # 2. Prompt: 显式要求 JSON
    system_prompt = """你是一个金融数据助手。
    
    任务：
    1. 接收用户的股票查询请求。
    2. 使用工具查询数据。
    3. 最终输出必须是严格的 JSON 格式。
    
    输出要求：
    - 不要输出任何闲聊、问候或多余的解释性文字。
    - 仅输出 JSON 对象。
    - 确保 JSON 可以被 Python 的 json.loads 解析。
    
    JSON 结构示例：
    {
        "portfolio": [
            {"symbol": "股票代码", "price": 数字, "currency": "货币"}
        ],
        "total_valuation_usd": 数字,
        "summary": "简短的一句话总结"
    }
    """

    user_message = "请帮我查一下 BABA 和 AAPL 的价格，并生成报告。"
    
    messages = [
        LLMMessage(role="system", content=system_prompt),
        LLMMessage(role="user", content=user_message)
    ]

    print(f"  [Input] 用户请求: {user_message}")

    try:
        result: AgentResult = await agent_engine.run(
            agent_input=AgentInput(messages=messages),
            provider_config=PROVIDER_CONFIG,
            run_config=run_config,
            callbacks=SimpleCallback(),
            tool_executor=tool_executor
        )

        print("-" * 50)
        print(f"  [Agent Raw Output]\n{result.message.content}")
        print("-" * 50)

        # 3. 验证步骤：检查是否使用了工具
        # 在普通模式下，Agent 应该先进行工具调用，然后才生成最终回答
        if len(result.steps) >= 2:
            print(f"  ✓ 成功: Agent 执行了 {len(result.steps)} 步 (预期行为)")
        else:
            print(f"  ⚠ 警告: 步骤数较少 ({len(result.steps)})，可能未完全调用所有工具")

        # 4. 验证格式：尝试提取并解析 JSON
        json_data = extract_json_from_text(result.message.content)
        
        if json_data:
            print("  ✓ 成功: 输出包含有效的 JSON 数据")
            print(f"  [Parsed Data] {json.dumps(json_data, indent=2, ensure_ascii=False)}")
            
            # 业务逻辑验证
            portfolio = json_data.get("portfolio", [])
            symbols = [item.get("symbol") for item in portfolio]
            if "BABA" in symbols and "AAPL" in symbols:
                print("  ✓ 数据验证: 包含了请求的所有股票")
            else:
                print("  ✗ 数据验证: 缺少部分股票数据")
        else:
            print("  ✗ 失败: 无法从输出中提取 JSON")

    except Exception as e:
        print(f"测试异常: {e}")

async def main():
    await test_prompt_driven_json()

if __name__ == "__main__":
    asyncio.run(main())
```

# ========== FILE: src/app/engine/agent/main.py ==========
```python
# src/app/engine/agent/main.py

import asyncio
import logging
import json
from typing import List, Optional

from .base import (
    AgentInput, AgentResult, AgentStep, AgentEngineCallbacks, 
    BaseToolExecutor
)
from ..model.llm import LLMEngineService, LLMProviderConfig, LLMRunConfig, LLMMessage, LLMToolCall, LLMUsage, LLMResult, LLMEngineCallbacks

class AgentEngineService:
    """
    一个有状态的、可插拔的 Agent 引擎，负责编排 LLM、工具和上下文。
    """
    def __init__(
        self,
        # [可选] 传入 (为了测试和高级配置)
        llm_engine: Optional[LLMEngineService] = None,
        max_iterations: int = 5
    ):
        self.llm_engine = llm_engine or LLMEngineService()
        self.max_iterations = max_iterations

    async def run(
        self,
        agent_input: AgentInput,
        # LLM 相关的配置需要透传
        provider_config: LLMProviderConfig,
        run_config: LLMRunConfig,
        tool_executor: BaseToolExecutor,
        callbacks: Optional[AgentEngineCallbacks] = None,
    ) -> AgentResult:
        
        try:
            # --- 步骤 1: 上下文准备 ---
            if callbacks: await callbacks.on_agent_start()
            
            message_history = agent_input.messages.copy()
            intermediate_steps = []

            # 初始化总用量计数器
            total_usage = LLMUsage()

            # --- 步骤 2: 启动 ReAct 循环 ---
            for _ in range(self.max_iterations):

                # --- 内部 LLM 回调处理器 ---
                # 这个回调类现在只负责"透传"流式状态给上层 UI，不再负责控制流逻辑
                class _InternalLLMCallbacks(LLMEngineCallbacks):
                    async def on_chunk_generated(self, chunk: str):
                        # 流式输出最终答案
                        if callbacks: await callbacks.on_final_chunk_generated(chunk)
                    
                    async def on_tool_calls_generated(self, tool_calls: List[LLMToolCall]):
                        # 通知 UI 模型正在请求工具
                        if callbacks: await callbacks.on_tool_calls_generated(tool_calls)

                    async def on_cancel(self, result: LLMResult):
                        # 处理取消事件
                        if callbacks:
                            if result.usage:
                                total_usage.prompt_tokens += result.usage.prompt_tokens
                                total_usage.completion_tokens += result.usage.completion_tokens
                                total_usage.total_tokens += result.usage.total_tokens
                            agent_result = AgentResult(
                                message=result.message,
                                steps=intermediate_steps,
                                usage=total_usage # 返回累加后的总用量
                            )                            
                            await callbacks.on_agent_cancel(agent_result)

                    async def on_usage(self, usage: LLMUsage):
                        # 可选：通知 UI 单次 LLM 调用的消耗
                        if callbacks: await callbacks.on_usage(usage)

                    # 其他回调保持为空或做简单日志
                    async def on_start(self): pass
                    async def on_success(self, result: LLMResult): pass
                    async def on_error(self, error: Exception): pass

                try:
                    # 调用纯粹的 LLM 引擎
                    llm_result: LLMResult = await self.llm_engine.run(
                        provider_config=provider_config,
                        run_config=run_config,
                        messages=message_history,
                        callbacks=_InternalLLMCallbacks()
                    )

                except asyncio.CancelledError:
                    # [关键] 捕获 LLM 层的取消
                    raise # 直接向外抛出，中断循环

                # 累加 Token 用量
                if llm_result.usage:
                    total_usage.prompt_tokens += llm_result.usage.prompt_tokens
                    total_usage.completion_tokens += llm_result.usage.completion_tokens
                    total_usage.total_tokens += llm_result.usage.total_tokens

                # --- 步骤 3: 决策与行动 ---
                assistant_msg = llm_result.message
                
                # 将 Assistant 的回复加入历史
                message_history.append(assistant_msg)

                # 检查是否包含工具调用
                # LLMMessage.tool_calls 是 List[Dict]
                if assistant_msg.tool_calls:
                    # 转换字典列表为 Pydantic 对象列表，方便处理
                    tool_calls_objects = [LLMToolCall(**tc) for tc in assistant_msg.tool_calls]
                    
                    # --- 并行工具执行 ---
                    tasks = []
                    for tool_call in tool_calls_objects:
                        async def execute_tool_task(tc: LLMToolCall):
                            tool_name = tc.function['name']
                            arguments_str = tc.function.get('arguments', '{}')
                            try:
                                tool_args = json.loads(arguments_str)
                            except json.JSONDecodeError:
                                return tc, {"error": "Invalid JSON arguments provided."}
                            
                            # 执行工具
                            observation = await tool_executor.execute(tool_name, tool_args)
                            return tc, observation

                        tasks.append(execute_tool_task(tool_call))

                    # 并发执行所有工具
                    tool_results = await asyncio.gather(*tasks, return_exceptions=True)

                    # 处理结果并更新历史
                    for i, result in enumerate(tool_results):
                        original_tool_call = tool_calls_objects[i]
                        
                        if isinstance(result, Exception):
                            observation = f'{{"error": "Tool execution failed unexpectedly.", "details": "{str(result)}"}}'
                        else:
                            _, observation = result

                        # 记录步骤
                        step = AgentStep(action=original_tool_call, observation=observation)
                        intermediate_steps.append(step)
                        if callbacks: await callbacks.on_agent_step(step)
                        
                        # 更新消息历史
                        message_history.append(LLMMessage(
                            role="tool",
                            tool_call_id=original_tool_call.id,
                            content=json.dumps(observation, ensure_ascii=False)
                        ))
                    
                    # 完成工具调用后，continue 进入下一轮循环，将工具结果发回给 LLM
                    continue 
                
                else: 
                    # --- 结束条件 ---
                    # 如果模型没有调用工具，说明生成了最终答案
                    result = AgentResult(
                        message=assistant_msg,
                        steps=intermediate_steps,
                        usage=total_usage # 返回累加后的总用量
                    )
                    if callbacks: await callbacks.on_agent_finish(result)
                    return result

            # 如果循环结束还没有返回最终答案
            raise Exception("Agent reached maximum iterations without a final answer.")

        except asyncio.CancelledError:
            # [关键] 整个 Agent 任务被取消
            logging.info("Agent execution cancelled.")
            raise # 继续向上抛出给 Service 层

        except Exception as e:
            if callbacks: await callbacks.on_agent_error(e)
            raise
```

# ========== FILE: src/app/engine/agent/utils/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/agent/utils/json_parser.py ==========
```python
import json
import re
from typing import Any, Dict, Optional

def parse_json_from_llm_output(text: str) -> Dict[str, Any]:
    """
    Robustly parse JSON from LLM output, handling Markdown code blocks and raw text.
    
    Args:
        text (str): The raw string output from the LLM.
        
    Returns:
        Dict[str, Any]: The parsed JSON object.
        
    Raises:
        ValueError: If valid JSON cannot be found or parsed.
    """
    cleaned_text = text.strip()
    
    # 1. Try parsing directly (fastest)
    try:
        return json.loads(cleaned_text)
    except json.JSONDecodeError:
        pass
    
    # 2. Extract from Markdown code blocks (```json ... ```)
    # This regex handles ```json, ```JSON, or just ```
    pattern = r"```(?:json|JSON)?\s*([\s\S]*?)\s*```"
    match = re.search(pattern, cleaned_text)
    if match:
        json_str = match.group(1)
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass
            
    # 3. Last resort: Find the outermost curly braces
    start = cleaned_text.find("{")
    end = cleaned_text.rfind("}")
    if start != -1 and end != -1:
        json_str = cleaned_text[start : end + 1]
        try:
            return json.loads(json_str)
        except json.JSONDecodeError:
            pass
            
    raise ValueError(f"Could not parse valid JSON from output: {text[:100]}...")
```

# ========== FILE: src/app/engine/model/__init__.py ==========
```python
from . import llm
from . import embedding
```

# ========== FILE: src/app/engine/model/embedding/__init__.py ==========
```python
# src/app/engine/model/embedding/__init__.py
from .main import EmbeddingEngineService, VectorCache
from .base import (
    EmbeddingProviderConfig,
    EmbeddingRunConfig,
    EmbeddingResult,
    BatchEmbeddingResult,
    EmbeddingEngineError,
    EmbeddingAuthenticationError,
    EmbeddingRateLimitError,
    EmbeddingProviderNotFoundError,
    EmbeddingBadRequestError
)

# 确保客户端被加载和注册
from . import clients
```

# ========== FILE: src/app/engine/model/embedding/base.py ==========
```python
# src/app/engine/model/embedding/base.py

from abc import ABC, abstractmethod
from typing import List, Optional
from pydantic import BaseModel, Field, HttpUrl

# --- 引擎层标准异常 ---

class EmbeddingEngineError(Exception):
    """Embedding引擎所有错误的基类"""
    pass

class EmbeddingAuthenticationError(EmbeddingEngineError):
    """凭证无效或权限不足"""
    pass

class EmbeddingRateLimitError(EmbeddingEngineError):
    """达到API频率限制"""
    pass

class EmbeddingBadRequestError(EmbeddingEngineError):
    """请求参数无效"""
    pass

class EmbeddingProviderNotFoundError(EmbeddingEngineError):
    """当找不到指定的 Embedding provider 时抛出"""
    pass

# --- 数据模型 (Data Models) ---

class EmbeddingProviderConfig(BaseModel):
    """
    定义了调用 Embedding 模型所需的客户端和凭证信息。
    """
    client_name: str = Field(..., description="客户端的唯一标识符, e.g., 'openai', 'zhipu'")
    api_key: str = Field(..., description="API Key")
    base_url: Optional[HttpUrl] = Field(None, description="API的基础URL，用于代理或私有部署")
    timeout: int = Field(60, description="API请求的超时时间（秒）")
    max_retries: int = Field(2, description="API请求的最大重试次数")

class EmbeddingRunConfig(BaseModel):
    """
    定义单次 Embedding 运行的配置。
    """
    model: str
    dimensions: Optional[int] = Field(None, description="（可选）嵌入向量的维度，仅部分新模型支持")
    max_batch_size: int = Field(1, description="The maximum number of texts that can be processed in a single batch.")
    max_batch_tokens: int = Field(1024, description="The maximum total tokens allowed in a single batch request.")

class EmbeddingResult(BaseModel):
    """
    标准化的单个嵌入结果。
    """
    index: int = Field(..., description="结果在原始输入列表中的索引")
    vector: Optional[List[float]] = Field(None, description="生成的嵌入向量，如果失败则为None")
    error_message: Optional[str] = Field(None, description="如果嵌入失败，记录错误信息")

class BatchEmbeddingResult(BaseModel):
    """
    标准化的批量嵌入结果，包含结果列表和用量信息。
    """
    results: List[EmbeddingResult]
    total_tokens: int

# --- 客户端接口 (Client Interface - Strategy Pattern) ---

class BaseEmbeddingClient(ABC):
    """
    所有具体 Embedding SDK 实现的统一接口。
    """
    def __init__(self, config: EmbeddingProviderConfig):
        ...

    @abstractmethod
    async def embed_batch(
        self,
        texts: List[str],
        run_config: EmbeddingRunConfig
    ) -> BatchEmbeddingResult:
        """
        执行批量嵌入的核心方法。
        接收一个文本列表，返回一个标准的批量结果对象。
        """
        ...
```

# ========== FILE: src/app/engine/model/embedding/clients/__init__.py ==========
```python
# src/app/engine/model/embedding/clients/__init__.py
# 动态导入所有客户端，以确保它们被注册
from . import openai_client
# from . import zhipu_client  # 未来添加
```

# ========== FILE: src/app/engine/model/embedding/clients/openai_client.py ==========
```python
# src/app/engine/model/embedding/clients/openai_client.py

import openai
from openai import APIError, RateLimitError, AuthenticationError, APITimeoutError, BadRequestError
from typing import List
from ..base import (
    EmbeddingProviderConfig, EmbeddingRunConfig, EmbeddingResult, 
    BatchEmbeddingResult, EmbeddingEngineError, EmbeddingAuthenticationError, 
    EmbeddingRateLimitError, EmbeddingBadRequestError
)
# 从 main.py 导入注册器 (稍后创建)
from ..main import register_embedding_client

@register_embedding_client("openai")
class OpenAIEmbeddingClient:
    """使用 'openai' Python SDK 的 Embedding 客户端实现。"""

    def __init__(self, config: EmbeddingProviderConfig):
        try:
            self.client = openai.AsyncOpenAI(
                api_key=config.api_key,
                base_url=str(config.base_url) if config.base_url else None,
                timeout=config.timeout,
                max_retries=config.max_retries,
            )
        except Exception as e:
            raise EmbeddingEngineError(f"Failed to initialize OpenAI client: {e}")

    async def embed_batch(
        self,
        texts: List[str],
        run_config: EmbeddingRunConfig
    ) -> BatchEmbeddingResult:
        
        # 准备 API 请求参数，并移除 None 值
        api_params = {
            "model": run_config.model,
            "input": texts,
            "dimensions": run_config.dimensions
        }
        api_params = {k: v for k, v in api_params.items() if v is not None}

        try:
            # 发起 API 调用
            response = await self.client.embeddings.create(**api_params)
            
            # 将 SDK 的返回结果转换为我们标准化的 EmbeddingResult 列表
            embedding_results = [
                EmbeddingResult(
                    index=data.index,
                    vector=data.embedding
                )
                for data in response.data
            ]
            
            # 封装成标准的批量结果对象
            return BatchEmbeddingResult(
                results=embedding_results,
                total_tokens=response.usage.total_tokens
            )

        except AuthenticationError as e:
            raise EmbeddingAuthenticationError(f"OpenAI authentication failed: {e.message}")
        except RateLimitError as e:
            raise EmbeddingRateLimitError(f"OpenAI rate limit exceeded: {e.message}")
        except BadRequestError as e:
            raise EmbeddingBadRequestError(f"Invalid request to OpenAI: {e.message}")
        except (APIError, APITimeoutError) as e:
            raise EmbeddingEngineError(f"OpenAI API error: {e.message}")
        except Exception as e:
            # 捕获任何其他意外错误
            raise EmbeddingEngineError(f"An unexpected error occurred in OpenAIEmbeddingClient: {str(e)}")
```

# ========== FILE: src/app/engine/model/embedding/example/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/model/embedding/example/basic_embedding.py ==========
```python
# src/app/engine/model/embedding/example/basic_embedding.py

import os
import asyncio
from app.engine.model.embedding import (
    EmbeddingEngineService,
    EmbeddingProviderConfig,
    EmbeddingRunConfig,
)

# --- Configuration ---
# IMPORTANT: Set your OpenAI API key in your environment variables.
API_KEY = "sk-LAdEXTUw5P"
if not API_KEY:
    raise ValueError("API_KEY environment variable not set.")
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_NAME = "text-embedding-v4"
PROVIDER_CONFIG = EmbeddingProviderConfig(client_name="openai", base_url=BASE_URL, api_key=API_KEY)

# Using a modern, cost-effective embedding model.
# The 'dimensions' parameter is optional and only supported by newer models like text-embedding-3-small/large.
# It allows you to truncate the vectors to a smaller size, saving storage and potentially speeding up similarity search.
RUN_CONFIG = EmbeddingRunConfig(
    model=MODEL_NAME,
    # dimensions=256  # Uncomment this line to get smaller vectors
)
# ---------------------

async def main():
    """Demonstrates a simple batch embedding process."""
    print(">>> Running Example: Basic Batch Embedding <<<")

    # 1. Initialize the engine
    engine = EmbeddingEngineService()

    # 2. Define the list of texts to be embedded
    texts_to_embed = [
        "The cat sat on the mat.",
        "A quick brown fox jumps over the lazy dog.",
        "Vector embeddings represent text in a high-dimensional space.",
        "PrismaSpace provides a robust engine architecture."
    ]
    print(f"\nInput texts ({len(texts_to_embed)} total):")
    for i, text in enumerate(texts_to_embed):
        print(f"  {i}: '{text}'")

    # 3. Run the engine
    try:
        print("\n[Engine] Calling the embedding service...")
        result = await engine.run_batch(
            provider_config=PROVIDER_CONFIG,
            run_config=RUN_CONFIG,
            texts=texts_to_embed
        )
        print("[Engine] Call successful. Processing results...")

        # 4. Print the results in a formatted way
        print("\n--- Embedding Results ---")
        print(f"Total Tokens Billed: {result.total_tokens}")
        print(f"Number of Vectors Generated: {len(result.results)}")
        print("-------------------------")
        for res in result.results:
            # Show a small preview of the vector to avoid flooding the console
            vector_preview = f"[{', '.join(map(str, res.vector[:5]))}, ...]"
            print(f"  - Index: {res.index}")
            print(f"    Text: '{texts_to_embed[res.index]}'")
            print(f"    Vector Dimensions: {len(res.vector)}")
            print(f"    Vector Preview: {vector_preview}")
        print("-------------------------\n")


    except Exception as e:
        print(f"\n--- An Error Occurred ---\n{type(e).__name__}: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

# ========== FILE: src/app/engine/model/embedding/main.py ==========
```python
# src/app/engine/model/embedding/main.py

import hashlib
import logging # 引入日志
from typing import Optional, Dict, Type, List, Tuple
from .base import (
    BaseEmbeddingClient, EmbeddingProviderConfig, EmbeddingRunConfig,
    EmbeddingResult, BatchEmbeddingResult, EmbeddingEngineError, EmbeddingProviderNotFoundError
)

# --- 客户端注册表 ---
_embedding_clients_registry: Dict[str, Type[BaseEmbeddingClient]] = {}

def register_embedding_client(client_name: str):
    """一个装饰器，用于将具体的客户端实现注册到工厂中。"""
    def decorator(cls: Type[BaseEmbeddingClient]):
        _embedding_clients_registry[client_name] = cls
        return cls
    return decorator


class VectorCache:
    """
    [Request-Scoped Cache]
    一个简单的内存缓存容器，在一次请求（Trace）生命周期内共享 EmbeddingResult。
    """
    def __init__(self):
        # Key: f"{module_version_id}:{md5(text)}"
        # Value: EmbeddingResult (Snapshot)
        self._cache: Dict[str, EmbeddingResult] = {}
        self.hits: int = 0
        self.misses: int = 0

    def _get_key(self, version_id: int, text: str) -> str:
        text_hash = hashlib.md5(text.encode("utf-8")).hexdigest()
        return f"{version_id}:{text_hash}"

    def get(self, version_id: int, text: str) -> Optional[EmbeddingResult]:
        """
        获取缓存的 EmbeddingResult。
        注意：返回的是原始缓存对象，调用者需要根据当前上下文修正其 `index` 字段。
        """
        key = self._get_key(version_id, text)
        val = self._cache.get(key)
        if val:
            self.hits += 1
        else:
            self.misses += 1
        return val

    def set(self, version_id: int, text: str, result: EmbeddingResult):
        key = self._get_key(version_id, text)
        # 我们缓存一份拷贝，以防外部修改
        self._cache[key] = result

    def clear(self):
        self._cache.clear()
        self.hits = 0
        self.misses = 0

class EmbeddingEngineService:
    """
    纯粹的、无状态的 Embedding 执行引擎。
    它根据传入的配置动态选择并初始化正确的客户端来执行任务。
    """
    
    def _get_client(self, config: EmbeddingProviderConfig) -> BaseEmbeddingClient:
        """
        工厂方法：根据提供商名称查找并实例化客户端。
        """
        client_name = config.client_name
        client_class = _embedding_clients_registry.get(client_name)
        
        if not client_class:
            raise EmbeddingProviderNotFoundError(f"No Embedding client registered for provider '{client_name}'. "
                             f"Available providers: {list(_embedding_clients_registry.keys())}")
        
        # 每次调用都创建一个新的客户端实例，以确保配置隔离
        return client_class(config)

    def _plan_batches(
        self,
        texts: List[str],
        max_batch_size: int,
        max_batch_tokens: int
    ) -> Tuple[List[List[Tuple[int, str]]], List[EmbeddingResult]]:
        """
        智能地规划批处理任务。

        返回:
            - 一个包含`(原始索引, 文本)`元组的批处理列表。
            - 一个因为文本超长而预先失败的 EmbeddingResult 列表。
        """
        valid_batches: List[List[Tuple[int, str]]] = []
        pre_failed_results: List[EmbeddingResult] = []
        
        current_batch: List[Tuple[int, str]] = []
        current_batch_tokens = 0
        max_tokens = max_batch_tokens

        for i, text in enumerate(texts):
            token_estimate = len(text)

            # 预检查单个文本是否超长
            if token_estimate > max_tokens:
                pre_failed_results.append(EmbeddingResult(
                    index=i,
                    error_message=f"Text is too long ({token_estimate} chars) to be processed. Maximum is {max_tokens}."
                ))
                continue

            is_first_item = not current_batch
            batch_not_full = len(current_batch) < max_batch_size
            tokens_not_exceeded = (current_batch_tokens + token_estimate) <= max_tokens

            if is_first_item or (batch_not_full and tokens_not_exceeded):
                current_batch.append((i, text))
                current_batch_tokens += token_estimate
            else:
                valid_batches.append(current_batch)
                current_batch = [(i, text)]
                current_batch_tokens = token_estimate

        if current_batch:
            valid_batches.append(current_batch)

        return valid_batches, pre_failed_results

    async def run_batch(
        self,
        provider_config: EmbeddingProviderConfig,
        run_config: EmbeddingRunConfig,
        texts: List[str],
    ) -> BatchEmbeddingResult:
        if not texts:
            return BatchEmbeddingResult(results=[], total_tokens=0)
            
        client = self._get_client(provider_config)
        valid_batches, pre_failed_results = self._plan_batches(texts, run_config.max_batch_size, run_config.max_batch_tokens)

        all_results: List[EmbeddingResult] = pre_failed_results
        total_tokens = 0

        for batch_with_indices in valid_batches:
            batch_texts = [text for _, text in batch_with_indices]
            
            try:
                # [关键新增] 捕获批处理级别的异常
                batch_result = await client.embed_batch(texts=batch_texts, run_config=run_config)
                
                # [关键新增] 校验返回结果长度是否匹配
                if len(batch_result.results) != len(batch_texts):
                    error_msg = f"API returned {len(batch_result.results)} embeddings for {len(batch_texts)} inputs."
                    raise EmbeddingEngineError(error_msg)

                # 成功：重新计算原始索引并添加到总结果中
                for res in batch_result.results:
                    original_index = batch_with_indices[res.index][0]
                    res.index = original_index
                    all_results.append(res)
                
                total_tokens += batch_result.total_tokens

            except Exception as e:
                # 失败：为这个批次的所有文本创建失败结果
                error_msg = f"Batch failed due to API error: {e}"
                logging.warning(error_msg, exc_info=True)
                for original_index, _ in batch_with_indices:
                    all_results.append(EmbeddingResult(
                        index=original_index,
                        error_message=error_msg
                    ))

        # 确保结果按原始索引排序
        all_results.sort(key=lambda r: r.index)
        
        return BatchEmbeddingResult(results=all_results, total_tokens=total_tokens)
```

# ========== FILE: src/app/engine/model/llm/__init__.py ==========
```python
# src/app/engine/model/llm/__init__.py
from .main import LLMEngineService
from .base import (
    LLMProviderConfig,
    LLMRunConfig,
    LLMMessage,
    LLMToolFunction,
    LLMTool,
    LLMUsage,
    LLMToolCall,
    LLMResult,
    LLMEngineCallbacks,
    LLMEngineError,
    LLMAuthenticationError,
    LLMRateLimitError,
    LLMContextLengthExceededError,
    LLMProviderNotFoundError,
    LLMBadRequestError
)

# 确保客户端被加载和注册
from . import clients
```

# ========== FILE: src/app/engine/model/llm/base.py ==========
```python
# src/app/engine/model/llm/base.py

from abc import ABC, abstractmethod
from typing import Dict, Any, Literal, List, Optional, Union, AsyncGenerator
from pydantic import BaseModel, Field, HttpUrl

# --- 数据模型 (Data Models) ---

class LLMProviderConfig(BaseModel):
    """
    定义了调用模型所需的客户端和凭证信息。
    """
    client_name: str = Field(..., description="客户端的唯一标识符, e.g., 'openai', 'azure', 'dashscope'")
    api_key: str = Field(..., description="API Key")
    base_url: Optional[HttpUrl] = Field(None, description="API的基础URL，用于代理或私有部署")
    timeout: int = Field(60, description="API请求的超时时间（秒）")
    max_retries: int = Field(2, description="API请求的最大重试次数")

class LLMToolFunction(BaseModel):
    name: str
    description: str
    parameters: Dict[str, Any] # JSON Schema

class LLMTool(BaseModel):
    type: Literal["function"] = "function"
    function: LLMToolFunction

class LLMMessage(BaseModel):
    role: Literal["system", "user", "assistant", "tool"]
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None # 模型生成的 tool_calls
    tool_call_id: Optional[str] = None # role='tool' 时需要

class LLMRunConfig(BaseModel):
    """定义单次模型运行的配置"""
    model: str
    temperature: float = Field(0.7, ge=0.0, le=2.0)
    top_p: float = Field(1.0, ge=0.0, le=1.0)
    presence_penalty: float = Field(0.0, ge=-2.0, le=2.0)
    frequency_penalty: float = Field(0.0, ge=-2.0, le=2.0)
    max_context_window: Optional[int] = Field(None, description="上下文的最大token数，用于成本控制和防超长")
    max_tokens: int = Field(default=2048, ge=1, description="生成的最大token数")
    enable_thinking: bool = Field(default=False, description="深度思考开关 (如支持)")
    thinking_budget: Optional[int] = Field(None, ge=1, description="最大思考长度 (仅当开启深度思考时有效)")
    stream: bool = True
    response_format: Optional[Dict[str, Any]] = Field(
        default_factory=lambda: {"type": "text"},
        description="""返回内容的格式。支持三种模式：
        1. 文本模式: {"type": "text"}
        2. JSON Object模式: {"type": "json_object"} (需在提示词中包含JSON关键词)
        3. JSON Schema模式: {"type": "json_schema", "json_schema": {...}, "strict": true/false}
        """
    )
    tools: Optional[List[LLMTool]] = None
    tool_choice: Optional[Union[Literal["auto", "none"], Dict]] = "auto"

class LLMUsage(BaseModel):
    """标准化的用量统计"""
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0

class LLMToolCall(BaseModel):
    """标准化的工具调用请求结构"""
    id: str
    type: Literal["function"] = "function"
    function: Dict[str, str] # e.g., {"name": "get_weather", "arguments": '{"location": "beijing"}'}

class LLMResult(BaseModel):
    """标准化的最终返回结果"""
    message: LLMMessage
    usage: LLMUsage

# --- 引擎层异常 ---

class LLMEngineError(Exception):
    """LLM引擎所有错误的基类"""
    pass

class LLMAuthenticationError(LLMEngineError):
    """凭证无效或权限不足"""
    pass

class LLMRateLimitError(LLMEngineError):
    """达到API频率限制"""
    pass

class LLMContextLengthExceededError(LLMEngineError):
    """上下文长度超过模型限制"""
    pass

class LLMBadRequestError(LLMEngineError):
    """请求参数无效（非上下文长度问题）"""
    pass

class LLMProviderNotFoundError(LLMEngineError):
    """当找不到指定的 LLM provider 时抛出"""
    pass
    
# --- 回调接口 (Callback) ---

class LLMEngineCallbacks(ABC):
    """
    定义了LLM引擎在执行过程中向外报告事件的接口。
    业务逻辑层需要实现这个接口，并将其注入引擎。
    """
    @abstractmethod
    async def on_start(self) -> None:
        """在生成开始时调用。"""
        ...

    @abstractmethod
    async def on_chunk_generated(self, chunk: str) -> None:
        """每当生成一个新的文本块时调用（流式模式）。"""
        ...
    
    @abstractmethod
    async def on_tool_calls_generated(self, tool_calls: List[LLMToolCall]) -> None:
        """当模型决定调用工具时调用。"""
        ...

    @abstractmethod
    async def on_success(self, result: LLMResult) -> None:
        """在整个生成过程成功完成时调用。"""
        ...
        
    @abstractmethod
    async def on_error(self, error: Exception) -> None:
        """在发生错误时调用。"""
        ...
    
    @abstractmethod
    async def on_usage(self, usage: LLMUsage) -> None:
        """在生成结束后，报告本次调用的token用量。"""
        ...

    @abstractmethod
    async def on_cancel(self, result: LLMResult) -> None:
        """当生成任务被外部中止时调用。"""
        ...

# --- 客户端接口 (Client Interface - Strategy Pattern) ---

class BaseLLMClient(ABC):
    """
    所有具体SDK实现的统一接口。
    LLMEngineService将与此接口交互，而不是具体的实现类。
    """
    def __init__(self, config: LLMProviderConfig):
        ...

    @abstractmethod
    async def generate(
        self,
        run_config: LLMRunConfig,
        messages: List[LLMMessage],
        callbacks: Optional[LLMEngineCallbacks] = None,
    ) -> None:
        """
        执行模型生成的核心方法。
        注意：此方法不直接返回结果，而是通过回调函数向外通知状态。
        """
        ...
```

# ========== FILE: src/app/engine/model/llm/clients/__init__.py ==========
```python
# src/app/engine/model/llm/clients/__init__.py
# 这个文件是空的，但它的存在使 'clients' 成为一个包

# 动态导入所有客户端，以确保它们被注册
from . import openai_client
# from . import azure_client # 未来添加
# from . import dashscope_client # 未来添加
```

# ========== FILE: src/app/engine/model/llm/clients/_base.py ==========
```python
# src/app/engine/model/llm/clients/_base.py

from typing import List, Dict, Any
from ..base import LLMMessage, LLMTool, LLMUsage
from ....utils.tokenizer.manager import tokenizer_manager

class LLMClientBase:
    """提供一些具体客户端可以复用的辅助方法。"""
    
    def _messages_to_dict(self, messages: List[LLMMessage]) -> List[Dict[str, Any]]:
        """将Pydantic模型转换为符合OpenAI API格式的字典列表。"""
        results = []
        for msg in messages:
            # Pydantic的 model_dump 方法可以很好地处理这个问题，并排除None值
            results.append(msg.model_dump(exclude_none=True))
        return results

    def _tools_to_dict(self, tools: List[LLMTool]) -> List[Dict[str, Any]]:
        """将工具列表转换为字典列表。"""
        if not tools:
            return None
        return [tool.model_dump() for tool in tools]

    def _estimate_usage(
        self, 
        provider: str, 
        model: str, 
        messages: List[LLMMessage], 
        generated_content: str = ""
    ) -> LLMUsage:
        """
        [兜底策略] 当 API 未返回 usage 时，使用本地 tokenizer 进行估算。
        """
        tokenizer = tokenizer_manager.get_tokenizer(provider, model)
        # 1. 计算 Prompt Tokens
        # 将消息列表还原为文本 (这里做一个简单的拼接，生产环境可以使用更精确的 chat format 估算)
        prompt_text = ""
        for msg in messages:
            content = msg.content or ""
            if msg.tool_calls:
                content += str(msg.tool_calls)
            prompt_text += content

        # 2. 调用 Tokenizer
        prompt_tokens = tokenizer.count(prompt_text)
        completion_tokens = tokenizer.count(generated_content)

        # 3. 累加到总用量
        return LLMUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens
        )
```

# ========== FILE: src/app/engine/model/llm/clients/openai_client.py ==========
```python
# src/app/engine/model/llm/clients/openai_client.py

import openai
import asyncio
import json
from typing import List, AsyncGenerator, Optional
from openai import APIError, RateLimitError, AuthenticationError, APITimeoutError, BadRequestError

from ..base import (
    LLMProviderConfig, LLMRunConfig, LLMMessage, LLMEngineCallbacks, 
    LLMToolCall, LLMUsage, LLMResult, LLMEngineError, LLMAuthenticationError, 
    LLMRateLimitError, LLMContextLengthExceededError, LLMBadRequestError
)
from ._base import LLMClientBase
from ..main import register_llm_client

@register_llm_client("openai")
class OpenAIClient(LLMClientBase):
    """使用 'openai' Python SDK 的、经过增强的 LLM 客户端实现。"""
    
    def __init__(self, config: LLMProviderConfig):
        self.config = config
        self.client = openai.AsyncOpenAI(
            api_key=config.api_key,
            base_url=str(config.base_url) if config.base_url else None,
            timeout=config.timeout,
            max_retries=config.max_retries,
        )

    async def _handle_streamed_response(
        self, 
        stream: AsyncGenerator, 
        run_config: LLMRunConfig,
        messages: List[LLMMessage], 
        callbacks: Optional[LLMEngineCallbacks]
    ) -> LLMResult:
        """私有方法，专门处理流式响应。"""
        full_content = ""
        tool_calls_buffer = []
        final_message = LLMMessage(role="assistant")
        final_usage = LLMUsage()

        try:
            async for chunk in stream:
                if chunk.choices:
                    choice = chunk.choices[0]
                    # 1. 处理工具调用
                    tool_calls = choice.delta.tool_calls
                    if tool_calls:
                        for tool_call_chunk in tool_calls:
                            if len(tool_calls_buffer) <= tool_call_chunk.index:
                                tool_calls_buffer.append({"id": "", "type": "function", "function": {"name": "", "arguments": ""}})
                            
                            current_call = tool_calls_buffer[tool_call_chunk.index]
                            if tool_call_chunk.id: current_call["id"] = tool_call_chunk.id
                            if tool_call_chunk.function.name: current_call["function"]["name"] = tool_call_chunk.function.name
                            if tool_call_chunk.function.arguments: current_call["function"]["arguments"] += tool_call_chunk.function.arguments

                    # 2. 处理文本块
                    content_chunk = choice.delta.content
                    if content_chunk:
                        full_content += content_chunk
                        if callbacks: await callbacks.on_chunk_generated(content_chunk)
                
                    # 3. 检查结束原因
                    finish_reason = choice.finish_reason
                    if finish_reason == "tool_calls":
                        parsed_tool_calls = [LLMToolCall(**tc) for tc in tool_calls_buffer]
                        if callbacks: await callbacks.on_tool_calls_generated(parsed_tool_calls)
                        final_message.tool_calls = tool_calls_buffer
                    elif finish_reason == "stop":
                        final_message.content = full_content

                # 4. 报告用量 (只有在最新版openai库中，最后一个chunk才有)
                if hasattr(chunk, 'usage') and chunk.usage:
                    final_usage = LLMUsage(**chunk.usage.model_dump())
                    if callbacks: await callbacks.on_usage(final_usage)

            # 返回最终的 LLMResult
            return LLMResult(message=final_message, usage=final_usage)

        except Exception as e:
            # === 统一兜底逻辑 ===
            if callbacks: 
                # 1. 补全 Usage (引擎层最知道怎么算)
                if final_usage.total_tokens == 0:
                    generated_so_far = full_content + json.dumps(tool_calls_buffer)
                    final_usage = self._estimate_usage(
                        self.config.client_name, run_config.model, messages, generated_so_far
                    )
                    # 触发 on_usage 确保 Side Channel (Accumulator) 被更新
                    await callbacks.on_usage(final_usage)

                # 2. 构造 Partial Result
                partial_message = LLMMessage(role="assistant", content=full_content, tool_calls=tool_calls_buffer or None)
                
                partial_result = LLMResult(message=partial_message, usage=final_usage)

                # 3. 触发 on_cancel 作为统一的 "临终遗言" 出口
                # 无论是因为 CancelledError 还是 ConnectionResetError
                await callbacks.on_cancel(partial_result)

            # 4. 重新抛出异常，让上层感知错误类型 (Service 层据此判断是 Cancel 还是 Error)
            raise e
            
    async def _handle_non_streamed_response(
        self, 
        response, 
        run_config: LLMRunConfig,
        messages: List[LLMMessage], 
        callbacks: Optional[LLMEngineCallbacks]
    ) -> LLMResult:
        """私有方法，专门处理非流式响应。"""
        final_message = LLMMessage(role="assistant")
        final_usage = LLMUsage()
        try:
            if response.choices:
                choice = response.choices[0]
                message = choice.message

                if message.tool_calls:
                    parsed_tool_calls = [
                        LLMToolCall(id=tc.id, type=tc.type, function={"name": tc.function.name, "arguments": tc.function.arguments}) 
                        for tc in message.tool_calls
                    ]
                    if callbacks: await callbacks.on_tool_calls_generated(parsed_tool_calls)
                    final_message.tool_calls = [tc.model_dump() for tc in message.tool_calls]
                    
                if message.content:
                    if callbacks: await callbacks.on_chunk_generated(message.content)
                    final_message.content = message.content

            if hasattr(response, 'usage') and response.usage:
                final_usage = LLMUsage(**response.usage.model_dump())
                if callbacks: await callbacks.on_usage(final_usage)

            return LLMResult(message=final_message, usage=final_usage)
        except Exception as e:
            if callbacks:
                if final_usage.total_tokens == 0:
                    estimate_content = ""
                    final_usage = self._estimate_usage(
                        self.config.client_name, run_config.model, messages, estimate_content
                    )
                    await callbacks.on_usage(final_usage)
                partial_result = LLMResult(message=final_message, usage=final_usage)
                await callbacks.on_cancel(partial_result)
            raise e

    async def generate(
        self,
        run_config: LLMRunConfig,
        messages: List[LLMMessage],
        callbacks: Optional[LLMEngineCallbacks] = None,
    ) -> LLMResult:
        api_params = {
            "model": run_config.model,
            "messages": self._messages_to_dict(messages),
            "temperature": run_config.temperature,
            "top_p": run_config.top_p,
            "presence_penalty": run_config.presence_penalty,
            "frequency_penalty": run_config.frequency_penalty,
            "max_tokens": run_config.max_tokens,
            "stream": run_config.stream,
        }

        if run_config.stream:
            api_params["stream_options"] = {"include_usage": True}

        if run_config.response_format:
            api_params["response_format"] = run_config.response_format

        tools = self._tools_to_dict(run_config.tools)

        if tools:
            api_params["tools"] = tools
            api_params["tool_choice"] = run_config.tool_choice

        extra_body = {}

        if run_config.enable_thinking == True and run_config.thinking_budget > 0:
            extra_body["enable_thinking"] = run_config.enable_thinking
            extra_body["thinking_budget"] = run_config.thinking_budget

        try:
            response = await self.client.chat.completions.create(**api_params, extra_body=extra_body)
            
            if run_config.stream:
                return await self._handle_streamed_response(response, run_config, messages, callbacks)
            else:
                return await self._handle_non_streamed_response(response, run_config, messages, callbacks)

        except asyncio.CancelledError:
            raise
        except AuthenticationError as e:
            raise LLMAuthenticationError(f"OpenAI authentication failed: {e.message}")
        except RateLimitError as e:
            raise LLMRateLimitError(f"OpenAI rate limit exceeded: {e.message}")
        except BadRequestError as e:
            if "context_length_exceeded" in e.code:
                raise LLMContextLengthExceededError(f"Context length exceeded for model {run_config.model}: {e.message}")
            raise LLMBadRequestError(f"Invalid request to OpenAI: {e.message}")
        except (APIError, APITimeoutError) as e:
            raise LLMEngineError(f"OpenAI API error: {e.message}")
        except Exception as e:
            # 捕获任何其他意外错误
            raise LLMEngineError(f"An unexpected error occurred in OpenAIClient: {str(e)}")
```

# ========== FILE: src/app/engine/model/llm/context.py ==========
```python
import logging
import copy
from typing import List, Dict, Any, Union, Optional, Set
from ...utils.tokenizer.manager import tokenizer_manager

logger = logging.getLogger(__name__)

class LLMContextManager:
    """
    [Production-Ready] 智能 LLM 上下文管理器
    
    核心策略：
    1. 原子性 (Atomicity): 严格保证 Assistant Call 与 Tool Result 同生共死，避免 API 报错。
    2. 锚点保护 (Anchoring): 始终保留 System Prompt 和 最后一轮对话。
    3. 动态压缩 (Dynamic Compression): 仅在总 Budget 不足时，才去压缩历史记录中的 Tool Output，
       最大程度利用 Context Window。
    """
    
    def __init__(self):
        pass

    def _get_tokenizer(self, provider: str, model: str):
        return tokenizer_manager.get_tokenizer(provider, model)

    def _get_attr(self, obj: Any, key: str, default: Any = None):
        """兼容 Pydantic v1/v2 和 Dict 的属性获取"""
        if isinstance(obj, dict):
            return obj.get(key, default)
        return getattr(obj, key, default)

    def _count_text_tokens(self, text: str, tokenizer) -> int:
        if not text: return 0
        return len(tokenizer.encode(str(text)))

    def _count_msg_tokens(self, msg: Any, tokenizer) -> int:
        """计算单条消息 Token (包含 overhead 估算)"""
        num_tokens = 4  # 基础协议开销
        
        content = self._get_attr(msg, 'content')
        if content:
            num_tokens += self._count_text_tokens(content, tokenizer)
        
        tool_calls = self._get_attr(msg, 'tool_calls')
        if tool_calls:
            for tool_call in tool_calls:
                func = self._get_attr(tool_call, 'function') or tool_call.get('function', {})
                if func:
                    f_name = self._get_attr(func, 'name') or func.get('name', '')
                    f_args = self._get_attr(func, 'arguments') or func.get('arguments', '')
                    num_tokens += self._count_text_tokens(f_name, tokenizer)
                    num_tokens += self._count_text_tokens(f_args, tokenizer)
        return num_tokens

    def _clone_and_update_msg(self, msg: Any, new_content: str) -> Any:
        """克隆消息并更新内容，保持原始对象类型"""
        if hasattr(msg, 'model_copy'):
            # Pydantic v2
            return msg.model_copy(update={"content": new_content})
        elif hasattr(msg, 'copy'):
            # Pydantic v1 or Dict
            new_msg = msg.copy()
            if isinstance(new_msg, dict):
                new_msg['content'] = new_content
            else:
                # Pydantic v1 usually supports copy but fields might be immutable, 
                # strictly speaking model_copy is safer for pydantic.
                # Here we assume if it's object, we try setattr for fallback
                try:
                    setattr(new_msg, 'content', new_content)
                except Exception:
                    # Fallback for immutable objects: return dict representation implies losing class type
                    # In strict usage, rely on Pydantic models being properly defined.
                    pass 
            return new_msg
        else:
            # Fallback for unknown objects: try deepcopy
            new_msg = copy.deepcopy(msg)
            try:
                setattr(new_msg, 'content', new_content)
            except:
                logger.warning(f"Could not update content for message type {type(msg)}")
            return new_msg

    def _compress_text(self, text: str, tokenizer, target_tokens: int) -> str:
        """智能截断文本，保留头尾"""
        tokens = tokenizer.encode(str(text))
        if len(tokens) <= target_tokens:
            return text
        
        # 至少保留头尾各一点，避免 target_tokens 过小
        keep_each_side = max(10, target_tokens // 2)
        if keep_each_side * 2 > len(tokens):
             return text # 无法压缩更多
            
        head = tokenizer.decode(tokens[:keep_each_side])
        tail = tokenizer.decode(tokens[-keep_each_side:])
        return f"{head}\n...[Content Compressed: {len(tokens)-target_tokens} tokens hidden]...\n{tail}"

    def _group_into_turns(self, messages: List[Any]) -> List[List[Any]]:
        """
        [关键逻辑] 将消息列表转化为原子对话轮次 (Turns)。
        规则：Assistant (with tool_calls) 必须和后续的 Tool (results) 绑定在一起。
        """
        turns = []
        i = 0
        n = len(messages)
        while i < n:
            msg = messages[i]
            role = self._get_attr(msg, 'role')
            tool_calls = self._get_attr(msg, 'tool_calls')
            
            # 只有 Assistant 发起调用时，才启动“粘滞”模式，吸附后续的 tool 消息
            if role == "assistant" and tool_calls:
                current_turn = [msg]
                j = i + 1
                while j < n:
                    next_msg = messages[j]
                    next_role = self._get_attr(next_msg, 'role')
                    if next_role == "tool":
                        current_turn.append(next_msg)
                        j += 1
                    else:
                        break
                turns.append(current_turn)
                i = j 
            else:
                turns.append([msg])
                i += 1
        return turns

    def manage(
        self,
        messages: List[Any],
        provider: str,
        model: str,
        max_context_tokens: int,
        reserve_tokens: int = 500
    ) -> List[Any]:
        """
        主入口。
        :param messages: 消息列表 (Dict 或 Pydantic)
        :param model: 模型名称
        :param max_context_tokens: 允许的最大上下文 Token 数
        :param reserve_tokens: 额外预留的安全空间
        """
        if not messages: return []
        
        tokenizer = self._get_tokenizer(provider, model)
        budget = max_context_tokens - reserve_tokens
        
        # 1. 转化为原子轮次
        turns = self._group_into_turns(messages)
        if not turns: return []

        # 2. 识别必须保留的锚点 (System + Last Turn)
        # 使用 Set 防止只有一条消息时重复添加
        kept_indices: Set[int] = set()
        kept_indices.add(len(turns) - 1) 
        
        if self._get_attr(turns[0][0], 'role') == 'system':
            kept_indices.add(0)
        
        # 3. 计算锚点开销 & 构建基础结果
        current_tokens = 0
        final_turns_map: Dict[int, List[Any]] = {}
        
        for idx in kept_indices:
            turn = turns[idx]
            turn_cost = sum(self._count_msg_tokens(m, tokenizer) for m in turn)
            final_turns_map[idx] = turn
            current_tokens += turn_cost
        
        # 极端情况防御：仅锚点就超标 (例如 Last Turn 的 Tool Output 极大)
        if current_tokens > budget:
            logger.warning("System Prompt + Last Turn exceeds budget. Performing aggressive compression on Last Turn.")
            # 对 Last Turn 中的 Tool Output 进行强力压缩
            last_idx = len(turns) - 1
            if last_idx in final_turns_map:
                compressed_turn = []
                for msg in final_turns_map[last_idx]:
                    if self._get_attr(msg, 'role') == 'tool':
                        # 强行压缩到很小，比如 500 tokens
                        content = self._get_attr(msg, 'content') or ""
                        new_content = self._compress_text(content, tokenizer, 500)
                        compressed_turn.append(self._clone_and_update_msg(msg, new_content))
                    else:
                        compressed_turn.append(msg)
                final_turns_map[last_idx] = compressed_turn
                # 重新计算 token (略过，为了性能假设它变小了)
            
            # 返回最小集合
            return [msg for idx in sorted(kept_indices) for msg in final_turns_map[idx]]

        # 4. 历史回填 (从倒数第二轮开始向前)
        # 逻辑：对于每一轮，先看完整放入是否超标。
        # 如果超标，且该轮包含 Tool 消息，尝试压缩 Tool 消息后再放入。
        # 如果还放不下，则停止。
        
        for i in range(len(turns) - 2, -1, -1):
            if i in kept_indices: continue
            
            raw_turn = turns[i]
            
            # 预计算
            turn_cost_full = 0
            tool_indices = []
            non_tool_cost = 0
            
            for m_idx, msg in enumerate(raw_turn):
                cost = self._count_msg_tokens(msg, tokenizer)
                turn_cost_full += cost
                if self._get_attr(msg, 'role') == 'tool':
                    tool_indices.append(m_idx)
                else:
                    non_tool_cost += cost
            
            # 策略 A: 完整放入
            if current_tokens + turn_cost_full <= budget:
                final_turns_map[i] = raw_turn
                current_tokens += turn_cost_full
                continue
            
            # 策略 B: 动态压缩放入 (仅当存在 Tool 消息时)
            if tool_indices:
                remaining_budget = budget - current_tokens
                
                # 如果连非 Tool 部分 (Assistant问句等) 都放不下，那这轮没法要了
                if non_tool_cost >= remaining_budget:
                    break 
                
                # 计算可分配给 Tool 的额度
                available_for_tools = remaining_budget - non_tool_cost
                # 设置一个最小阈值，如果每个 tool 分不到 100 token，压缩意义不大，直接丢弃
                if available_for_tools < 100 * len(tool_indices):
                    break
                
                token_per_tool = available_for_tools // len(tool_indices)
                
                # 构建压缩后的轮次
                squeezed_turn = list(raw_turn) # 浅拷贝列表结构
                actual_squeezed_cost = non_tool_cost
                
                for t_idx in tool_indices:
                    original_msg = squeezed_turn[t_idx]
                    content = self._get_attr(original_msg, 'content') or ""
                    
                    # 压缩
                    new_content = self._compress_text(content, tokenizer, token_per_tool)
                    new_msg = self._clone_and_update_msg(original_msg, new_content)
                    
                    squeezed_turn[t_idx] = new_msg
                    actual_squeezed_cost += self._count_msg_tokens(new_msg, tokenizer)
                
                # 双重检查 (防止估算误差)
                if current_tokens + actual_squeezed_cost <= budget:
                    final_turns_map[i] = squeezed_turn
                    current_tokens += actual_squeezed_cost
                else:
                    break # 即使压缩也放不下
            else:
                # 纯文本对话放不下，停止
                break

        # 5. 组装并按原始顺序返回
        final_messages = []
        for i in sorted(final_turns_map.keys()):
            final_messages.extend(final_turns_map[i])
            
        return final_messages
```

# ========== FILE: src/app/engine/model/llm/example/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/model/llm/example/_callbacks.py ==========
```python
# src/app/engine/model/llm/example/_callbacks.py

from app.engine.model.llm import LLMEngineCallbacks, LLMToolCall, LLMMessage, LLMUsage, LLMResult

class PrintCallbacks(LLMEngineCallbacks):
    """A simple implementation of the callbacks protocol that prints events to the console."""
    
    async def on_start(self) -> None:
        print("\n--- Engine Started ---\n")

    async def on_chunk_generated(self, chunk: str) -> None:
        """Prints text chunks as they are generated."""
        print(chunk, end="", flush=True)
    
    async def on_tool_calls_generated(self, tool_calls: list[LLMToolCall]) -> None:
        """Prints the details of the tool calls requested by the model."""
        print("\n\n--- Tool Calls Requested ---")
        for tool_call in tool_calls:
            print(f"  - ID: {tool_call.id}")
            print(f"    Function: {tool_call.function['name']}")
            print(f"    Arguments: {tool_call.function['arguments']}")
        print("--------------------------\n")

    async def on_success(self, result: LLMResult) -> None:
        """Prints a confirmation when the generation is complete."""
        print(f"\n\n--- Generation Successful ---\n\n{result}")

    async def on_cancel(self, result: LLMResult) -> None:
        """Prints a confirmation when the generation is cancel."""
        print(f"\n\n--- Generation Cancel ---\n\n{result}\n\nEND")

    async def on_error(self, error: Exception) -> None:
        """Prints any errors that occur during the generation."""
        print(f"\n\n--- An Error Occurred ---\n{type(error).__name__}: {error}")
        
    async def on_usage(self, usage: LLMUsage) -> None:
        """Prints the token usage statistics."""
        print(f"\n\n--- Usage Stats ---\nPrompt Tokens: {usage.prompt_tokens}\nCompletion Tokens: {usage.completion_tokens}\nTotal Tokens: {usage.total_tokens}\n-------------------")
```

# ========== FILE: src/app/engine/model/llm/example/simple_streaming.py ==========
```python
# src/app/engine/model/llm/example/simple_streaming.py

import os
import asyncio
from app.engine.model.llm import (
    LLMEngineService,
    LLMProviderConfig,
    LLMRunConfig,
    LLMMessage,
    LLMResult
)
from ._callbacks import PrintCallbacks

# --- Configuration ---
# IMPORTANT: Set your OpenAI API key in your environment variables.
API_KEY = "sk-LAdEXTUw5P"
if not API_KEY:
    raise ValueError("API_KEY environment variable not set.")
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_NAME = "qwen-plus-2025-09-11"
PROVIDER_CONFIG = LLMProviderConfig(client_name="openai", base_url=BASE_URL, api_key=API_KEY)
RUN_CONFIG = LLMRunConfig(model=MODEL_NAME, stream=True, temperature=0.5, response_format={"type": "text"})
# ---------------------

async def main():
    """Demonstrates a simple streaming text generation."""
    print(">>> Running Example 1: Simple Streaming <<<")

    # 1. Initialize the engine and callbacks
    engine = LLMEngineService()
    callbacks = PrintCallbacks()

    # 2. Define the conversation messages
    messages = [
        LLMMessage(role="system", content="You are a helpful assistant."),
        LLMMessage(role="user", content="9.11和9.9哪个大?"),
    ]

    # 3. Run the engine
    try:
        task = asyncio.create_task(engine.run(
            provider_config=PROVIDER_CONFIG,
            run_config=RUN_CONFIG,
            messages=messages,
            callbacks=callbacks
        ))
        #await asyncio.sleep(2)
        #task.cancel()
        result: LLMResult = await task
        print(f"LLM RESULT: {result}")
    except Exception as e:
        print(f"\nAn exception occurred in the engine run: {e}")

if __name__ == "__main__":
    asyncio.run(main())
```

# ========== FILE: src/app/engine/model/llm/example/test_json_object.py ==========
```python
# src/app/engine/model/llm/example/test_json_object.py

import os
import asyncio
import json
from app.engine.model.llm import (
    LLMEngineService,
    LLMProviderConfig,
    LLMRunConfig,
    LLMMessage,
    LLMResult
)
from ._callbacks import PrintCallbacks

# --- Configuration ---
API_KEY = "sk-LAdEXTUw5P"
if not API_KEY:
    raise ValueError("API_KEY environment variable not set.")
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_NAME = "qwen-plus-2025-09-11"

# JSON Object 模式配置
PROVIDER_CONFIG = LLMProviderConfig(
    client_name="openai", 
    base_url=BASE_URL, 
    api_key=API_KEY
)

RUN_CONFIG_JSON_OBJECT = LLMRunConfig(
    model=MODEL_NAME,
    stream=True,  # 结构化输出通常不需要流式
    temperature=0.1,  # 降低温度以获得更确定性的输出
    response_format={"type": "json_object"}
)
# ---------------------

async def test_json_object():
    """测试 JSON Object 模式的结构化输出"""
    print(">>> Testing JSON Object Mode <<<")
    
    # 1. 初始化引擎和回调
    engine = LLMEngineService()
    callbacks = PrintCallbacks()
    
    # 2. 定义包含 JSON 关键词的消息（必须包含 "json" 关键词）
    messages = [
        LLMMessage(
            role="system", 
            content="""你是一个数据提取助手。请将用户提供的信息提取为JSON格式。
            输出必须是有效的JSON对象，包含以下字段：
            - "answer": 直接回答用户的问题
            - "explanation": 提供简要解释
            - "confidence": 0到1之间的置信度评分
            记住：输出必须是JSON格式。"""
        ),
        LLMMessage(
            role="user", 
            content="比较9.11和9.9的大小，并告诉我哪个更大。"
        ),
    ]
    
    # 3. 运行引擎
    try:
        result: LLMResult = await engine.run(
            provider_config=PROVIDER_CONFIG,
            run_config=RUN_CONFIG_JSON_OBJECT,
            messages=messages,
            callbacks=callbacks
        )
        
        # 4. 验证输出是否为有效的 JSON
        if result:
            print(f"\n原始输出: {result.message.content}")
            
            try:
                json_data = json.loads(result.message.content)
                print(f"解析为JSON: {json.dumps(json_data, ensure_ascii=False, indent=2)}")
                
                # 验证基本结构
                required_keys = ["answer", "explanation", "confidence"]
                missing_keys = [key for key in required_keys if key not in json_data]
                
                if missing_keys:
                    print(f"警告: 缺少必要的键: {missing_keys}")
                else:
                    print("✓ JSON 结构验证通过")
                    
            except json.JSONDecodeError as e:
                print(f"✗ 输出不是有效的 JSON: {e}")
                
        print(f"Tokens 使用情况: {result.usage}")
        
    except Exception as e:
        print(f"\n测试过程中发生异常: {e}")
        import traceback
        traceback.print_exc()

async def test_json_object_with_different_prompts():
    """测试不同提示词对 JSON Object 模式的影响"""
    print("\n>>> Testing JSON Object with Different Prompts <<<")
    
    engine = LLMEngineService()
    
    test_cases = [
        {
            "name": "简单JSON输出",
            "system": "输出必须是JSON格式。",
            "user": "告诉我北京和上海的人口，用JSON格式回复。"
        },
        {
            "name": "复杂数据结构",
            "system": "你是一个天气API。总是以JSON格式回复，包含city, temperature, condition, humidity字段。",
            "user": "今天北京的天气如何？"
        }
    ]
    
    for test_case in test_cases:
        print(f"\n--- 测试: {test_case['name']} ---")
        
        messages = [
            LLMMessage(role="system", content=test_case["system"]),
            LLMMessage(role="user", content=test_case["user"])
        ]
        
        try:
            result = await engine.run(
                provider_config=PROVIDER_CONFIG,
                run_config=RUN_CONFIG_JSON_OBJECT,
                messages=messages,
                callbacks=None
            )

            if result:
            
                print(f"输出: {result.message.content[:200]}...")  # 只显示前200字符
                
                # 尝试解析JSON
                try:
                    json.loads(result.message.content)
                    print("✓ 有效的JSON")
                except:
                    print("✗ 无效的JSON")
                
        except Exception as e:
            print(f"错误: {e}")

async def main():
    await test_json_object()
    await test_json_object_with_different_prompts()

if __name__ == "__main__":
    print("=" * 60)
    print("JSON Object 模式测试")
    print("=" * 60)
    
    # 运行测试
    asyncio.run(main())
```

# ========== FILE: src/app/engine/model/llm/example/test_json_schema.py ==========
```python
# src/app/engine/model/llm/example/test_json_schema.py

import os
import asyncio
import json
from app.engine.model.llm import (
    LLMEngineService,
    LLMProviderConfig,
    LLMRunConfig,
    LLMMessage,
    LLMResult
)
from ._callbacks import PrintCallbacks

# --- Configuration ---
API_KEY = "sk-LAdEXTUw5P"
if not API_KEY:
    raise ValueError("API_KEY environment variable not set.")
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_NAME = "qwen-plus-2025-09-11"

PROVIDER_CONFIG = LLMProviderConfig(
    client_name="openai", 
    base_url=BASE_URL, 
    api_key=API_KEY
)

# 定义 JSON Schema - 根据阿里云官方文档格式
MATH_COMPARISON_SCHEMA = {
    "type": "object",
    "properties": {
        "larger_number": {
            "type": "number",
            "description": "较大的数字"
        },
        "smaller_number": {
            "type": "number",
            "description": "较小的数字"
        },
        "difference": {
            "type": "number",
            "description": "两个数字的差值"
        },
        "explanation": {
            "type": "string",
            "description": "比较过程的解释"
        },
        "is_significant": {
            "type": "boolean",
            "description": "差异是否显著"
        }
    },
    "required": ["larger_number", "smaller_number", "difference", "explanation", "is_significant"],
    "additionalProperties": False
}

# 根据阿里云官方示例格式
RUN_CONFIG_MATH = LLMRunConfig(
    model=MODEL_NAME,
    stream=True,
    temperature=0.1,
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "number_comparison_schema",
            "strict": True,
            "schema": MATH_COMPARISON_SCHEMA
        }
    }
)

# 第二个测试用例的 Schema
PRODUCT_SCHEMA = {
    "type": "object",
    "properties": {
        "products": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "price": {"type": "number"},
                    "category": {"type": "string"},
                    "in_stock": {"type": "boolean"}
                },
                "required": ["name", "price", "category", "in_stock"]
            }
        },
        "total_value": {"type": "number"},
        "most_expensive": {"type": "string"}
    },
    "required": ["products", "total_value", "most_expensive"],
    "additionalProperties": False
}

RUN_CONFIG_PRODUCT = LLMRunConfig(
    model=MODEL_NAME,
    stream=True,
    temperature=0.1,
    response_format={
        "type": "json_schema",
        "json_schema": {
            "name": "product_catalog_schema",
            "strict": True,
            "schema": PRODUCT_SCHEMA
        }
    }
)
# ---------------------

async def test_json_schema_math():
    """测试 JSON Schema 模式 - 数学比较"""
    print(">>> Testing JSON Schema Mode - Math Comparison <<<")
    
    engine = LLMEngineService()
    callbacks = PrintCallbacks()
    
    # JSON Schema 模式不需要在提示词中包含 "JSON" 关键词
    messages = [
        LLMMessage(
            role="system", 
            content="你是一个数学比较助手。请比较用户提供的两个数字。"
        ),
        LLMMessage(
            role="user", 
            content="请比较 9.11 和 9.9 这两个数字，告诉我哪个更大，并计算它们的差值。"
        ),
    ]
    
    try:
        result: LLMResult = await engine.run(
            provider_config=PROVIDER_CONFIG,
            run_config=RUN_CONFIG_MATH,
            messages=messages,
            callbacks=callbacks
        )
        
        if result:
            print(f"\n原始输出: {result.message.content}")
            
            try:
                json_data = json.loads(result.message.content)
                print(f"解析为JSON:\n{json.dumps(json_data, ensure_ascii=False, indent=2)}")
                
                # 验证是否符合 schema
                validation_result = validate_against_schema(json_data, MATH_COMPARISON_SCHEMA)
                if validation_result["valid"]:
                    print("✓ 符合 JSON Schema")
                    
                    # 逻辑验证
                    if json_data["larger_number"] == 9.11 and json_data["smaller_number"] == 9.9:
                        print("✓ 数字比较正确")
                    if abs(json_data["difference"] - 0.21) < 0.01:
                        print("✓ 差值计算正确")
                        
                else:
                    print(f"✗ 不符合 JSON Schema: {validation_result['errors']}")
                    
            except json.JSONDecodeError as e:
                print(f"✗ 输出不是有效的 JSON: {e}")
                
        print(f"Tokens 使用情况: {result.usage}")
        
    except Exception as e:
        print(f"\n测试过程中发生异常: {e}")

async def test_json_schema_product_catalog():
    """测试 JSON Schema 模式 - 产品目录"""
    print("\n>>> Testing JSON Schema Mode - Product Catalog <<<")
    
    engine = LLMEngineService()
    
    messages = [
        LLMMessage(
            role="system", 
            content="你是一个产品目录管理系统。请根据用户描述生成产品列表。"
        ),
        LLMMessage(
            role="user", 
            content="我有以下产品：苹果手机价格8999元，有库存；华为笔记本价格6999元，缺货；小米电视价格3299元，有库存。请整理成产品目录。"
        ),
    ]
    
    try:
        result: LLMResult = await engine.run(
            provider_config=PROVIDER_CONFIG,
            run_config=RUN_CONFIG_PRODUCT,
            messages=messages,
            callbacks=None
        )
        
        if result:
            print(f"\n原始输出: {result.message.content}")
            
            try:
                json_data = json.loads(result.message.content)
                print(f"解析为JSON:\n{json.dumps(json_data, ensure_ascii=False, indent=2)}")
                
                # 验证是否符合 schema
                validation_result = validate_against_schema(json_data, PRODUCT_SCHEMA)
                if validation_result["valid"]:
                    print("✓ 符合 JSON Schema")
                    
                    # 业务逻辑验证
                    products = json_data.get("products", [])
                    print(f"✓ 找到 {len(products)} 个产品")
                    
                    # 计算总价值验证
                    calculated_total = sum(p.get("price", 0) for p in products)
                    if abs(json_data.get("total_value", 0) - calculated_total) < 0.01:
                        print("✓ 总价值计算正确")
                        
                else:
                    print(f"✗ 不符合 JSON Schema: {validation_result['errors']}")
                    
            except json.JSONDecodeError as e:
                print(f"✗ 输出不是有效的 JSON: {e}")
                
    except Exception as e:
        print(f"\n测试过程中发生异常: {e}")

async def test_json_schema_variations():
    """测试 JSON Schema 的不同变体"""
    print("\n>>> Testing JSON Schema Variations <<<")
    
    engine = LLMEngineService()
    messages = [LLMMessage(role="user", content="9.11和9.9哪个大？")]
    
    variations = [
        {
            "name": "严格模式 (strict=True)",
            "response_format": {
                "type": "json_schema",
                "json_schema": {
                    "name": "strict_comparison",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "larger": {"type": "number"},
                            "smaller": {"type": "number"}
                        },
                        "required": ["larger", "smaller"],
                        "additionalProperties": False
                    }
                }
            }
        },
        {
            "name": "非严格模式 (strict=False)",
            "response_format": {
                "type": "json_schema",
                "json_schema": {
                    "name": "non_strict_comparison",
                    "strict": False,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "larger": {"type": "number"},
                            "smaller": {"type": "number"}
                        },
                        "required": ["larger", "smaller"]
                    }
                }
            }
        },
        {
            "name": "不带 name 字段",
            "response_format": {
                "type": "json_schema",
                "json_schema": {
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "larger": {"type": "number"},
                            "smaller": {"type": "number"}
                        },
                        "required": ["larger", "smaller"]
                    }
                }
            }
        }
    ]
    
    for variation in variations:
        print(f"\n--- 测试: {variation['name']} ---")
        
        run_config = LLMRunConfig(
            model=MODEL_NAME,
            stream=False,
            temperature=0.1,
            response_format=variation["response_format"]
        )
        
        try:
            result = await engine.run(
                provider_config=PROVIDER_CONFIG,
                run_config=run_config,
                messages=messages,
                callbacks=None
            )
            
            print(f"输出: {result.message.content}")
            
            try:
                json_data = json.loads(result.message.content)
                print(f"JSON 结构: {list(json_data.keys())}")
            except:
                print("输出不是有效的 JSON")
                
        except Exception as e:
            print(f"错误: {e}")

async def test_json_object_comparison():
    """对比测试 JSON Object 模式和 JSON Schema 模式"""
    print("\n>>> Testing JSON Object vs JSON Schema <<<")
    
    engine = LLMEngineService()
    user_message = "比较9.11和9.9的大小，用JSON格式回复，包含larger和smaller字段。"
    
    # JSON Object 模式
    print("\n1. JSON Object 模式:")
    messages_json_object = [
        LLMMessage(role="system", content="请以JSON格式输出。"),
        LLMMessage(role="user", content=user_message)
    ]
    
    run_config_json_object = LLMRunConfig(
        model=MODEL_NAME,
        stream=False,
        temperature=0.1,
        response_format={"type": "json_object"}
    )
    
    try:
        result_json = await engine.run(
            provider_config=PROVIDER_CONFIG,
            run_config=run_config_json_object,
            messages=messages_json_object,
            callbacks=None
        )
        print(f"输出: {result_json}")
    except Exception as e:
        print(f"错误: {e}")
    
    # JSON Schema 模式
    print("\n2. JSON Schema 模式:")
    messages_json_schema = [
        LLMMessage(role="user", content=user_message)
    ]
    
    run_config_json_schema = LLMRunConfig(
        model=MODEL_NAME,
        stream=False,
        temperature=0.1,
        response_format={
            "type": "json_schema",
            "json_schema": {
                "name": "simple_comparison",
                "strict": True,
                "schema": {
                    "type": "object",
                    "properties": {
                        "larger": {"type": "number"},
                        "smaller": {"type": "number"}
                    },
                    "required": ["larger", "smaller"],
                    "additionalProperties": False
                }
            }
        }
    )
    
    try:
        result_schema = await engine.run(
            provider_config=PROVIDER_CONFIG,
            run_config=run_config_json_schema,
            messages=messages_json_schema,
            callbacks=None
        )
        print(f"输出: {result_schema}")
    except Exception as e:
        print(f"错误: {e}")

def validate_against_schema(data, schema):
    """简单的 schema 验证函数"""
    errors = []
    
    # 检查必需字段
    if "required" in schema:
        for field in schema["required"]:
            if field not in data:
                errors.append(f"缺少必需字段: {field}")
    
    # 检查不允许的额外字段
    if schema.get("additionalProperties") is False:
        allowed_properties = set(schema.get("properties", {}).keys())
        actual_properties = set(data.keys())
        extra_properties = actual_properties - allowed_properties
        if extra_properties:
            errors.append(f"不允许的额外字段: {extra_properties}")
    
    # 简单的类型检查
    if "properties" in schema:
        for field, field_schema in schema["properties"].items():
            if field in data:
                expected_type = field_schema.get("type")
                if expected_type:
                    actual_type = type(data[field]).__name__
                    type_mapping = {
                        "int": "integer",
                        "float": "number",
                        "str": "string",
                        "bool": "boolean",
                        "list": "array",
                        "dict": "object"
                    }
                    actual_type_normalized = type_mapping.get(actual_type, actual_type)
                    
                    if expected_type == "number" and actual_type in ["int", "float"]:
                        continue  # int 和 float 都是 number
                    elif expected_type == "array" and actual_type == "list":
                        continue  # list 对应 array
                    elif expected_type == "object" and actual_type == "dict":
                        continue  # dict 对应 object
                    elif expected_type != actual_type_normalized:
                        errors.append(f"字段 '{field}' 类型错误: 期望 {expected_type}, 实际 {actual_type_normalized}")
    
    return {
        "valid": len(errors) == 0,
        "errors": errors
    }

async def main():
    await test_json_schema_math()
    await test_json_schema_product_catalog()
    await test_json_schema_variations()
    await test_json_object_comparison()

if __name__ == "__main__":
    print("=" * 60)
    print("JSON Schema 模式测试 (阿里云官方格式)")
    print("=" * 60)
    
    # 运行测试
    asyncio.run(main())
```

# ========== FILE: src/app/engine/model/llm/example/tool_calling.py ==========
```python
# src/app/engine/model/llm/example/tool_calling.py

import os
import asyncio
import json
from app.engine.model.llm import (
    LLMEngineService,
    LLMProviderConfig,
    LLMRunConfig,
    LLMMessage,
    LLMTool,
    LLMToolCall,
    LLMResult,
    LLMEngineCallbacks, # We'll create a custom callback handler for this
)
from ._callbacks import PrintCallbacks # We can inherit from it to reduce boilerplate

# --- Configuration ---
API_KEY = "sk-LAdEXTUw5P"
if not API_KEY:
    raise ValueError("API_KEY environment variable not set.")
BASE_URL = "https://dashscope.aliyuncs.com/compatible-mode/v1"
MODEL_NAME = "qwen-plus-2025-09-11"
PROVIDER_CONFIG = LLMProviderConfig(client_name="openai", base_url=BASE_URL, api_key=API_KEY)

# Define the tool the model can use
WEATHER_TOOL = LLMTool(
    function={
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "The city and state, e.g., San Francisco, CA"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
            },
            "required": ["location"],
        },
    }
)
RUN_CONFIG = LLMRunConfig(model=MODEL_NAME, stream=True, tools=[WEATHER_TOOL])
# ---------------------

class ToolOrchestrator(PrintCallbacks):
    """
    An advanced callback handler that simulates an agentic loop.
    It handles tool calls by executing them and re-invoking the engine.
    """
    def __init__(self, engine: LLMEngineService):
        self.engine = engine
        self.message_history = []
        self._loop_finished = asyncio.Event()

    def _get_current_weather(self, location: str, unit: str = "celsius") -> dict:
        """A mock tool implementation."""
        print(f"\n[Tool Execution] Getting weather for {location} in {unit}...")
        if "tokyo" in location.lower():
            return {"location": "Tokyo", "temperature": "10", "unit": unit}
        elif "san francisco" in location.lower():
            return {"location": "San Francisco", "temperature": "72", "unit": unit}
        else:
            return {"location": location, "temperature": "unknown", "unit": unit}

    async def on_success(self, final_result: LLMResult) -> None:
        """Override on_success to stop the loop only on a final text answer."""
        if not final_result.message.tool_calls:
            await super().on_success(final_result)
            self.message_history.append(final_result.message)
            self._loop_finished.set()

    async def on_tool_calls_generated(self, tool_calls: list[LLMToolCall]) -> None:
        """The core orchestration logic."""
        await super().on_tool_calls_generated(tool_calls)
        
        # Step 1: Add the assistant's tool-call request to history
        assistant_message = LLMMessage(role="assistant", tool_calls=[tc.model_dump() for tc in tool_calls])
        self.message_history.append(assistant_message)

        # Step 2: Execute tools and collect results
        for tool_call in tool_calls:
            function_name = tool_call.function['name']
            arguments = json.loads(tool_call.function['arguments'])
            
            if function_name == "get_current_weather":
                result = self._get_current_weather(**arguments)
            else:
                result = {"error": f"Unknown tool: {function_name}"}

            # Step 3: Add the tool's result to history
            self.message_history.append(
                LLMMessage(role="tool", tool_call_id=tool_call.id, content=json.dumps(result))
            )
        
        # Step 4: Re-call the engine with the updated history
        print("\n[Orchestrator] Re-calling engine with tool results...")
        await self.engine.run(
            provider_config=PROVIDER_CONFIG,
            run_config=RUN_CONFIG,
            messages=self.message_history,
            callbacks=self
        )

    async def wait_for_completion(self):
        """Waits until the loop is explicitly finished."""
        await self._loop_finished.wait()


async def main():
    """Demonstrates a full tool-calling loop."""
    print("\n>>> Running Example 2: Tool Calling <<<")
    
    engine = LLMEngineService()
    orchestrator = ToolOrchestrator(engine)
    
    messages = [
        LLMMessage(role="user", content="What's the weather like in San Francisco?")
    ]
    orchestrator.message_history = messages.copy()
    
    try:
        # Initial call to the engine
        await engine.run(
            provider_config=PROVIDER_CONFIG,
            run_config=RUN_CONFIG,
            messages=orchestrator.message_history,
            callbacks=orchestrator
        )
        # Wait for the entire multi-step process to finish
        await orchestrator.wait_for_completion()
    except Exception as e:
        print(f"\nAn exception occurred in the engine run: {e}")


if __name__ == "__main__":
    asyncio.run(main())
```

# ========== FILE: src/app/engine/model/llm/main.py ==========
```python
# src/app/engine/model/llm/main.py

import asyncio
import logging
from typing import Dict, Type, List, Optional
from .base import (
    BaseLLMClient, LLMProviderConfig, LLMRunConfig, LLMMessage, LLMResult, 
    LLMEngineCallbacks, LLMEngineError, LLMProviderNotFoundError 
)
from .context import LLMContextManager

# --- 客户端注册表 ---
_llm_clients_registry: Dict[str, Type[BaseLLMClient]] = {}

def register_llm_client(client_name: str):
    """一个装饰器，用于将具体的客户端实现注册到工厂中。"""
    def decorator(cls: Type[BaseLLMClient]):
        _llm_clients_registry[client_name] = cls
        return cls
    return decorator


class LLMEngineService:
    """
    纯粹的、无状态的LLM执行引擎。
    它现在集成了上下文管理器，以确保请求在发送前是合规的。
    """
    def __init__(self):
        self.context_manager = LLMContextManager() # 实例化上下文管理器
    
    def _get_client(self, config: LLMProviderConfig) -> BaseLLMClient:
        """
        工厂方法：根据提供商名称查找并实例化客户端。
        """
        client_name = config.client_name
        client_class = _llm_clients_registry.get(client_name)
        
        if not client_class:
            raise LLMProviderNotFoundError(
                f"No LLM client registered for provider '{client_name}'. "
                f"Available providers: {list(_llm_clients_registry.keys())}"
            )
        
        # 每次调用都创建一个新的客户端实例，以确保配置隔离
        return client_class(config)

    async def run(
        self,
        provider_config: LLMProviderConfig,
        run_config: LLMRunConfig,
        messages: List[LLMMessage],
        callbacks: Optional[LLMEngineCallbacks] = None,
    ) -> LLMResult:
        try:
            # 步骤 1: 使用上下文管理器处理消息 (如果设置了 max_context_window)
            managed_messages = messages
            if run_config.max_context_window:
                managed_messages = self.context_manager.manage(
                    messages=messages,
                    provider=config.client_name,
                    model=run_config.model,
                    max_context_tokens=run_config.max_context_window,
                    max_tokens=run_config.max_tokens
                )

            # 步骤 2: 获取客户端实例
            client = self._get_client(provider_config)

            # 步骤 3: 调用客户端的 generate 方法
            if callbacks: await callbacks.on_start()
            result: LLMResult = await client.generate(
                run_config=run_config,
                messages=managed_messages, # 使用处理过的消息
                callbacks=callbacks
            )
            if callbacks: await callbacks.on_success(result)
            return result
        except asyncio.CancelledError:
            # [关键] 捕获取消信号
            # 1. 记录日志
            logging.info(f"LLM generation cancelled for model {run_config.model}")
            # 2. 我们不调用 callbacks.on_error，因为这不是一个错误
            # 3. 重新抛出异常，以便上层（AgentEngine）也能感知并停止循环
            raise 
        except LLMEngineError as e:
            # 捕获我们自己定义的、可预期的引擎错误
            if callbacks: await callbacks.on_error(e)
            raise
        except Exception as e:
            # 捕获在客户端选择或上下文管理期间的任何意外错误
            err = LLMEngineError(f"An unexpected error occurred in LLMEngineService: {str(e)}")
            if callbacks: await callbacks.on_error(err)
            raise err
```

# ========== FILE: src/app/engine/parsing/__init__.py ==========
```python
from .parsers import *
from .chunkers import *
```

# ========== FILE: src/app/engine/parsing/base.py ==========
```python
# engine/parsing/base.py
from abc import ABC, abstractmethod
from typing import Optional, Type, List, Dict, Any, Literal, NamedTuple, Union

# --- 1. 定义标准化的输出结构 (The "Chunk" Contract) ---
# 这是所有解析器必须返回的统一格式
class Document(NamedTuple):
    content: Union[str, list]
    content_type: Literal["text", "list", "html", "xml"] # 可扩展
    mime_type: str
    source_parser: str
    metadata: Dict[str, Any]

# 这是所有分段器必须返回的统一格式
class DocumentChunk(NamedTuple):
    content: str
    chunk_type: Literal["text", "qa"] # 可扩展
    chunk_length: int
    source_chunker: str
    metadata: Dict[str, Any] # e.g., length, page_number, source_file, bounding_box

# --- 2. 定义解析策略的输入结构 (The "Policy" Contract) ---
# 这是调用者告诉引擎“如何解析”的指令
class ParserPolicy(NamedTuple):
    parser_name: str # e.g., 'simple_text', 'gpt4v_ocr'
    allowed_mime_types: list
    params: Dict[str, Any] = {} # 特定于解析器的参数

class ChunkerPolicy(NamedTuple):
    chunker_name: str # e.g., simple_text_v1, simple_qa_v1
    params: Dict[str, Any] = {} # 特定于解析器的参数

class BasePolicy(NamedTuple):
    parser: Optional[ParserPolicy] = None
    chunkers: Optional[List[ChunkerPolicy]] = []

# --- 定义主引擎的接口 (The "Engine" Contract) ---
class BaseEngine(ABC):
    """文档解析引擎的统一门面。"""

    @abstractmethod
    async def execute(
        self,
        file_url: Optional[str],
        file_content: Optional[bytes],
        mime_type: Optional[str],
        policy: BasePolicy
    ) -> List[DocumentChunk]:
        """
        引擎的主入口点。
        它接收文件内容和解析策略，并调度正确的解析器来完成工作。
        """
        raise NotImplementedError

# --- 定义具体解析器的接口 (The "Parser" Contract) ---
class BaseParser(ABC):
    """所有具体解析器（Tika, Whisper, VLM等）都必须实现的接口。"""

    # 静态属性，用于在引擎中注册和查找
    name: str = "base_parser"

    support_mime_types: list = []

    @abstractmethod
    async def run(self, file_content: bytes, mime_type: str, **kwargs) -> Document:
        """
        接收原始文件内容，返回标准化的 DocumentChunk 列表。
        **kwargs 用于接收来自 ParsingPolicy.params 的特定参数。
        """
        raise NotImplementedError

class BaseChunker(ABC):
    # 策略的唯一名称，用于在数据库和配置中引用
    name: str 

    @abstractmethod
    async def run(
        self, 
        document: Document, 
        **kwargs # 允许传入特定于策略的参数，如 chunk_size
    ) -> List[DocumentChunk]:
        """
        接收原始文件字节流，返回标准化的 DocumentChunk 列表。
        所有的解析、结构化、分割逻辑都封装在此方法内部。
        """
        raise NotImplementedError

ALL_PARSERS: Dict[str, Type[BaseParser]] = {}
ALL_CHUNKERS: Dict[str, Type[BaseChunker]] = {}
ALL_MIME_TYPE: List[str] = []

def register_parser(cls: Type[BaseParser]):
    if cls.name in ALL_PARSERS:
        raise ValueError(f"Parser with name '{cls.name}' already registered.")
    ALL_MIME_TYPE.extend(mime for mime in cls.support_mime_types 
                        if mime not in ALL_MIME_TYPE)
    ALL_PARSERS[cls.name] = cls
    return cls

def register_chunker(cls: Type[BaseChunker]):
    if cls.name in ALL_CHUNKERS:
        raise ValueError(f"Chunker with name '{cls.name}' already registered.")
    ALL_CHUNKERS[cls.name] = cls
    return cls
```

# ========== FILE: src/app/engine/parsing/chunkers/__init__.py ==========
```python
# engine/parsing/chunkers/__init__.py
from .simple_chunker import SimpleChunker
from .html_chunker import HtmlChunker
```

# ========== FILE: src/app/engine/parsing/chunkers/context_chunker.py ==========
```python
# src/app/engine/parsing/chunkers/context_chunker.py

import logging
from typing import List
import tiktoken
from ..base import BaseChunker, Document, DocumentChunk, register_chunker
from ..utils import clean_text

logger = logging.getLogger(__name__)

@register_chunker
class ContextChunker(BaseChunker):
    """
    [Deep Memory] 专门用于切分对话上下文的 Chunker。
    它不仅基于字符长度，还基于 Token 数量进行安全切分，确保符合 Embedding 模型的限制。
    """
    name = "context_chunker_v1"

    def __init__(self):
        # 默认使用 cl100k_base (GPT-4/3.5/Embedding-v3 标准)
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except Exception:
            # Fallback if download fails (rare in prod, but safe)
            self.tokenizer = None

    def _count_tokens(self, text: str) -> int:
        if not self.tokenizer:
            return len(text)
        return len(self.tokenizer.encode(text))

    def _split_text_with_overlap(self, text: str, max_tokens: int, overlap_tokens: int) -> List[str]:
        """
        基于 Token 的重叠切分算法。
        """
        if self._count_tokens(text) <= max_tokens:
            return [text]
        
        chunks = []
        tokens = self.tokenizer.encode(text)
        total_tokens = len(tokens)
        start = 0
        
        while start < total_tokens:
            end = min(start + max_tokens, total_tokens)
            chunk_tokens = tokens[start:end]
            chunk_text = self.tokenizer.decode(chunk_tokens)
            chunks.append(chunk_text)
            
            if end == total_tokens:
                break
                
            start += (max_tokens - overlap_tokens)
            
        return chunks

    async def run(self, document: Document, **kwargs) -> List[DocumentChunk]:
        """
        Args:
            document: 包含整轮对话文本的 Document 对象
            kwargs:
                max_tokens: 每个 Chunk 的最大 Token 数 (default: 512)
                overlap_tokens: 重叠 Token 数 (default: 50)
        """
        content = document.content
        if not isinstance(content, str) or not content.strip():
            return []

        max_tokens = kwargs.get('max_tokens', 512)
        overlap_tokens = kwargs.get('overlap_tokens', 50)

        # 清理文本，移除无意义的空白符
        cleaned_text = clean_text(content)
        
        # 执行切分
        text_chunks = self._split_text_with_overlap(cleaned_text, max_tokens, overlap_tokens)
        
        doc_chunks = []
        for i, text in enumerate(text_chunks):
            doc_chunks.append(DocumentChunk(
                content=text,
                chunk_type="text",
                chunk_length=len(text),
                source_chunker=self.name,
                metadata={
                    "chunk_index": i,
                    "total_chunks": len(text_chunks)
                }
            ))
            
        return doc_chunks
```

# ========== FILE: src/app/engine/parsing/chunkers/html_chunker.py ==========
```python
# engine/parsing/chunkers/xml_chunker.py
from typing import Optional, Type, List, Dict, Any, Literal, NamedTuple, Union
from bs4 import BeautifulSoup
from ..base import BaseChunker, Document, DocumentChunk, register_chunker
from ..utils import clean_text

@register_chunker
class HtmlChunker(BaseChunker):
    name = "html_chunker_v1"

    async def chunk_pdf(self, document: Document) -> List[DocumentChunk]:
        # 使用BeautifulSoup解析XHTML内容
        soup = BeautifulSoup(document.content, 'lxml')

        # 查找所有class为"page"的div元素
        pages = soup.find_all('div', class_='page')

        # 提取每一页的内容
        document_chunks = []
        for i, page in enumerate(pages):
            page_text = page.get_text(strip=True)
            
            clean_content = clean_text(page_text)
            document_chunks.append(
                DocumentChunk(
                    content=clean_content,
                    chunk_type="text",
                    chunk_length=len(clean_content),
                    source_chunker=self.name,
                    metadata={"page_number": i + 1}
                )
            )

        return document_chunks 

    async def run(self, document: Document, **kwargs) -> List[DocumentChunk]:
        if not isinstance(document.content, str) or document.content_type != "xml":
            return [] # 此策略只处理 XML
        mime_type = document.mime_type
        document_chunks = []
        # 暂时只处理PDF文件，可扩展
        if mime_type == "application/pdf":
            document_chunks = await self.chunk_pdf(document)

        return document_chunks
```

# ========== FILE: src/app/engine/parsing/chunkers/simple_chunker.py ==========
```python
# engine/parsing/chunkers/simple_chunker.py
import re
from typing import Optional, Type, List, Dict, Any, Literal, NamedTuple, Union
from ..base import BaseChunker, Document, DocumentChunk, register_chunker
from ..utils import clean_text

@register_chunker
class SimpleChunker(BaseChunker):
    name = "simple_chunker_v1"

    async def run(self, document: Document, **kwargs) -> List[DocumentChunk]:
        if isinstance(document.content, list):
            # 未来应该由专门的ListChunker负责
            return [DocumentChunk(
                        content=item_content,
                        chunk_type="text",
                        chunk_length=len(item_content),
                        source_chunker=self.name,
                        metadata={}
                    ) for item_content in document.content] # 此策略只处理 list

        elif not isinstance(document.content, str) or document.content_type != "text":
            return [] # 此策略只处理 text

        chunk_size = kwargs.get('chunk_size', 600)

        document_chunks = []

        # 正则表达式，用于匹配句子结束的标点符号和换行符
        delimiter_pattern = re.compile(r'([?!。！？；;\n]+)')
        sentences = delimiter_pattern.split(document.content)

        current_paragraph = ''
        next_sentence_buffer = ''  # 用于存储无法完整加入当前段落的句子剩余部分

        for sentence in sentences:
            # 如果当前有未完成的句子内容，先添加到新句子开始
            if next_sentence_buffer:
                sentence = next_sentence_buffer + sentence
                next_sentence_buffer = ''  # 清空缓冲区

            sent_len = len(sentence)
            # 判断加上新句子后是否超过最大词数
            if len(current_paragraph) + sent_len <= chunk_size:
                current_paragraph += sentence
            else:
                # 查找句子中的分割符位置，确保在标点处截断
                last_delimiter_match = delimiter_pattern.search(sentence)
                if last_delimiter_match:
                    # 如果找到分割符，截取到分割符前并处理
                    split_pos = last_delimiter_match.start() + 1
                    current_paragraph += sentence[:split_pos].strip()
                    # 剩余部分留到下一次循环作为开头
                    next_sentence_buffer = sentence[split_pos:].strip()
                else:
                    # 如果没有找到合适的分割位置，整个句子移到下一个段落
                    if current_paragraph:  # 确保当前段落非空再添加
                        current_content = clean_text(current_paragraph.strip())
                        document_chunks.append(
                            DocumentChunk(
                                content=current_content,
                                chunk_type="text",
                                chunk_length=len(current_content),
                                source_chunker=self.name,
                                metadata={}
                            )
                        )
                    current_paragraph = sentence
                    next_sentence_buffer = ''  # 无需缓冲，因为整个句子已加入新段落

        # 处理最后一个段落或剩余内容
        if current_paragraph or next_sentence_buffer:
            last_content = clean_text((current_paragraph + next_sentence_buffer).strip())
            document_chunks.append(
                DocumentChunk(
                    content=last_content,
                    chunk_type="text",
                    chunk_length=len(last_content),
                    source_chunker=self.name,
                    metadata={}
                )
            )

        return document_chunks
```

# ========== FILE: src/app/engine/parsing/main.py ==========
```python
# engine/parsing/main.py
import asyncio
from typing import Optional, Type, List, Dict, Any, Literal, NamedTuple, Union
from .base import BaseEngine, BasePolicy, ParserPolicy, ChunkerPolicy, BaseParser, BaseChunker, Document, DocumentChunk, ALL_PARSERS, ALL_CHUNKERS, ALL_MIME_TYPE
from .utils import get_file_bytes_and_mime, get_mime_by_file_bytes

# 示例运行时配置：
base_policy = {
    "parser": {
        "parser_name": "simple_parser_v1",
        "allowed_mime_types": ["application/pdf", "application/vnd.openxmlformats-officedocument.wordprocessingml.document"],
        "params": {}
    },
    "chunkers": [
        {
            "chunker_name": "simple_chunker_v1",
            "params": {"chunk_size": 600}
        },
        {
            "chunker_name": "xml_chunker_v1",
            "params": {}
        },
        {
            "chunker_name": "qa_chunker_v1",
            "params": {"model": "gpt-4o"}
        }
    ]
}

class ParsingEngine:
    """纯粹的解析引擎"""
    def __init__(self):
        self._parsers_cache: Dict[str, BaseParser] = {}

    def _get_parser(self, parser_name: str) -> BaseParser:
        if parser_name in self._parsers_cache:
            return self._parsers_cache[parser_name]

        parser_class = ALL_PARSERS.get(parser_name)
        
        if not parser_class:
            raise ValueError(f"No parser registered for parser name '{parser_name}'.")

        print(f"Lazy loading and caching parser for parser name: '{parser_name}'")
        parser_instance = parser_class()

        self._parsers_cache[parser_name] = parser_instance
        
        return parser_instance

    async def execute(
        self,
        policy: ParserPolicy,
        file_content: Any = None,
        mime_type: str = None,
    ) -> Document:

        parser_name = policy.parser_name
        parser = self._get_parser(parser_name)

        empty_document = Document(content="", content_type="text", mime_type="", source_parser="", metadata={})

        if mime_type not in policy.allowed_mime_types:
            return empty_document

        if mime_type not in parser.support_mime_types:
            return empty_document

        try:
            # 调用具体的解析器，并传递特定参数
            # 第一阶段输出文本内容
            return await parser.run(
                file_content=file_content,
                mime_type=mime_type,
                **policy.params
            )
        except Exception as e:
            raise ValueError(f"Error executing parser '{parser_name}': {e}")

class ChunkingEngine:
    """纯粹的分块引擎"""
    def __init__(self):
        self._chunkers_cache: Dict[str, BaseChunker] = {}

    def _get_chunker(self, chunker_name: str) -> BaseChunker:
        if chunker_name in self._chunkers_cache:
            return self._chunkers_cache[chunker_name]

        chunker_class = ALL_CHUNKERS.get(chunker_name)
        
        if not chunker_class:
            raise ValueError(f"No chunker registered for chunker name '{chunker_name}'.")

        print(f"Lazy loading and caching chunker for parser name: '{chunker_name}'")
        chunker_instance = chunker_class()

        self._chunkers_cache[chunker_name] = chunker_instance
        
        return chunker_instance

    async def execute(self, document: Document, policy: ChunkerPolicy) -> List[DocumentChunk]:
        chunker_name = policy.chunker_name
        chunker = self._get_chunker(chunker_name)
        try:
            # 调用具体的分块器，并传递特定参数
            # 第二阶段生成分块列表  
            return await chunker.run(
                document=document,
                **policy.params
            )
            # [未来] 如果是串行，可以 current_input_doc = chunks ...
        except Exception as e:
            raise ValueError(f"Error executing chunker '{chunker_name}': {e}")
            # 可以在这里添加更复杂的错误处理逻辑

class ProcessingPipeline(BaseEngine):
    """
    最高级别的编排器
    """
    def __init__(self):
        self.parsing_engine = ParsingEngine()
        self.chunking_engine = ChunkingEngine()

    async def execute(
        self,
        policy: BasePolicy,
        file_url: str = None,
        file_content: Any = None,
        mime_type: str = None,
    ) -> List[DocumentChunk]:

        if file_content:
            # 优先使用直接提供的内容
            if isinstance(file_content, bytes):
                if not mime_type:
                    mime_type = get_mime_by_file_bytes(file_content)
            else:
                # 如果是文本，且没有指定解析器，直接创建文档
                if not policy.parser:
                    document = Document(
                        content=file_content,
                        content_type="list" if isinstance(file_content, list) else "text",
                        mime_type=mime_type or "text/plain",
                        source_parser="",
                        metadata={}
                    )
        elif file_url:
            # 下载内容
            file_content, mime_type = await get_file_bytes_and_mime(file_url)
        else:
            raise ValueError(f"file_content is empty")

        # --- 阶段一：解析 ---
        if policy.parser:
            document = await self.parsing_engine.execute(
                policy=policy.parser,
                file_content=file_content,
                mime_type=mime_type
            )

        if not document.content:
            return [DocumentChunk(content="", chunk_type="text", chunk_length=0, source_chunker="", metadata={})]

        # --- 阶段二：分块 ---
        if not policy.chunkers:
            # 如果没有分块策略，将整个文档内容作为一个块返回
            return [DocumentChunk(content=document.content, chunk_type="text", chunk_length=len(document.content), source_chunker="", metadata=document.metadata)]

        # 目前我们只支持并行处理，即所有分块器都作用于同一个原始 Document
        all_chunks = []
        tasks = [
            self.chunking_engine.execute(document, chunker_policy) 
            for chunker_policy in policy.chunkers
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, Exception):
                # 任何一步失败直接中止，后续再探索容错机制，但目前我们只需成功，不需失败
                raise ValueError(f"A chunker failed during execution: {result}")

            all_chunks.extend(result)

        return all_chunks
```

# ========== FILE: src/app/engine/parsing/parsers/__init__.py ==========
```python
# engine/parsing/parsers/__init__.py
from .simple_parser import SimpleParser
```

# ========== FILE: src/app/engine/parsing/parsers/simple_parser.py ==========
```python
# engine/parsing/parsers/simple_parser.py
import httpx 
from ..base import BaseParser, Document, register_parser

@register_parser
class SimpleParser(BaseParser):

    name: str = "simple_parser_v1"

    support_mime_types: list = [
        # 常见文档类型
        "application/pdf",
        "application/msword",
        "application/vnd.ms-excel",
        "application/vnd.ms-powerpoint",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        "application/vnd.openxmlformats-officedocument.presentationml.presentation",
        "application/rtf",
        "text/plain",
        "text/csv",
        "text/html",
        "​​text/markdown",
        "application/xhtml+xml",
        # 常见编程相关
        "text/x-java-source",
        "text/x-c++src",
        "application/java-archive",
        "application/xml",
        # 其他常见类型
        "application/json",
        "application/javascript",
        "application/octet-stream",
    ]

    async def run(self, file_content: bytes, mime_type: str, **kwargs) -> Document:
        if not isinstance(file_content, bytes):
            raise ValueError("File content type not bytes")

        tika_url = kwargs.get('tika_url')

        if not tika_url:
            raise ValueError("Tika Service Url is empty")

        is_xml = False

        is_pdf = mime_type == "application/pdf"

        if is_pdf:
            # 如果是pdf，则返回可结构化解析的XML内容
            is_xml = True

        accept_header = "text/html" if is_xml else "text/plain"

        content_type = "xml" if is_xml else "text"

        content = ""

        async with httpx.AsyncClient(timeout=30) as client:

            # 调用 tika 的解析端点
            try:
                response = await client.put(
                    tika_url,
                    headers={
                        "Accept": accept_header,
                        "X-Tika-OCRLanguage": "chi_sim",
                        "X-Tika-PDFextractInlineImages": "true"
                    },
                    content=file_content
                )
                response.raise_for_status()  # 检查 HTTP 状态码
                content = response.text
            except httpx.RequestError as e:
                raise ValueError(f"Tika request failed: {str(e)}")    
        
        if not content:
            return Document(content="", content_type=content_type, mime_type=mime_type, metadata={})

        return Document(
                    content=content,
                    content_type=content_type,
                    mime_type=mime_type,
                    source_parser=self.name,
                    metadata={}
                )    
```

# ========== FILE: src/app/engine/parsing/utils.py ==========
```python
# engine/parsing/utils.py
import re
import httpx
import magic
from typing import Optional, Type, List, Dict, Any, Literal, NamedTuple, Union

magic_mime = magic.Magic(mime=True)

def get_mime_by_file_bytes(file_content: bytes) -> Optional[str]:
    return magic_mime.from_buffer(file_content)

async def get_file_bytes_and_mime(url: str) -> tuple[bytes, Optional[str]]:
    async with httpx.AsyncClient(timeout=30) as client:
        response = await client.get(url)
        response.raise_for_status()
        
        content_bytes = response.content
        mime_type = response.headers.get('content-type')
        
        if mime_type is None:
            mime_type = get_mime_by_file_bytes(content_bytes)
        
        return content_bytes, mime_type

def clean_text(text: str) -> str:
    # 中英文标点符号的Unicode范围
    chinese_punctuation = r'[\u3000-\u303F\uFF01-\uFF5E]'
    # 匹配连续的空白字符（包括空格、制表符、换页符等）和中英文标点
    pattern = f'〔\s+{chinese_punctuation}〕+'

    # 使用正则表达式替换匹配到的序列为空格
    cleaned_text = re.sub(pattern, ' ', text)
    # 再次替换多个连续空格为单个空格
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    return cleaned_text
```

# ========== FILE: src/app/engine/protocols/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/schemas/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/schemas/form_schema.py ==========
```python
from typing import List, Dict, Any, Optional, Union, Literal
from pydantic import BaseModel, Field, ConfigDict

# ==============================================================================
# 表单协议 (Form Protocol)
# ==============================================================================
class FormProperty(BaseModel):
    """
    前端动态表单的渲染协议。
    用于描述如何渲染 NodeData.config 的编辑界面。
    """
    label: str = Field(..., description="表单项标签")
    desc: Optional[str] = Field(None, description="表单项描述/提示")
    type: Literal['form', 'action'] = Field('form', description="类型：常规表单项 或 动作按钮")
    form_type: str = Field(..., description="具体的控件类型 (e.g., 'input', 'select', 'code_editor')")
    
    # 传递给前端组件的 props
    props: Dict[str, Any] = Field(default_factory=dict)
    
    # 嵌套结构
    children: Optional[List['FormProperty']] = None
    
    # 绑定值的路径 (相对于 NodeData.config)
    output_key: str = Field(..., description="绑定值的键路径")
    
    # 动态控制表达式
    show_expr: Union[str, bool] = Field(True, description="显隐控制表达式")
    disabled_expr: Union[str, bool] = Field(False, description="禁用控制表达式")
    required_expr: Union[str, bool] = Field(False, description="必填控制表达式")
    
    form_role: str = Field('default', description="表单角色")

    model_config = ConfigDict(extra='ignore')

# 解决递归引用
FormProperty.model_rebuild()
```

# ========== FILE: src/app/engine/schemas/parameter_schema.py ==========
```python
# engine/schemas/parameter_schema.py

from pydantic import BaseModel, Field
from typing import Literal, Optional, List, Any, Dict

# [核心] 用于定义 'value' 字段的结构
class ParameterValue(BaseModel):
    """
    实例级别的配置值，在设计时设置。
    用于覆盖 `default` 后备值，其优先级高于 `default`。
    """
    type: Literal["literal", "expr", "ref"] = Field(..., description="值的类型")
    content: Any = Field(..., description="字面量内容、表达式字符串或引用路径")

# [核心] 对应于 TypeScript 中的 SchemaBlueprint
class SchemaBlueprint(BaseModel):
    """
    仅用于描述一个纯粹的数据结构或“形状”，不包含任何上下文相关的元数据
    (如名称、标签、顺序等)。它是可重用的、独立的结构定义。
    """
    # ========================================================================
    # A. 核心字段 (与 JSON Schema 兼容)
    # ========================================================================
    type: Literal['string', 'number', 'integer', 'boolean', 'object', 'array'] = Field(..., description="参数的数据类型")
    uid: Optional[int] = Field(None, description="参数唯一ID (可选)")
    description: Optional[str] = Field(None, description="对参数的详细描述，供开发者或LLM理解")
    enum: Optional[List[Any]] = Field(None, description="参数的枚举值列表")
    default: Optional[Any] = Field(None, description="Schema级别的静态后备值（默认值）")

    # 任何 type 为 'object' 的蓝图都需要它。
    # 我们需要使用 List['ParameterSchema'] 因为对象的属性是具名的。
    properties: Optional[List['ParameterSchema']] = Field(None, description="当 type 为 'object' 时，定义其子属性")

    # --- 结构化字段 (使用前向引用来处理递归) ---
    items: Optional['SchemaBlueprint'] = Field(None, description="当 type 为 'array' 时，定义数组元素的结构蓝图")
    # [注意] 在 Pydantic 中，对于列表中的递归/前向引用，我们需要特殊处理
    # 为了与您的设计保持一致，我们将 properties 字段放在 ParameterSchema 中
    
    
# [核心] 对应于 TypeScript 中的 ParameterSchema
class ParameterSchema(SchemaBlueprint):
    """
    定义了一个参数的完整元数据。
    它融合了 JSON Schema 的核心概念和特定领域的扩展字段。
    """
    # ========================================================================
    # B. 领域特定扩展字段 (自定义)
    # ========================================================================
    name: str = Field(..., description="参数的名称，在对象类型中作为其属性键")
    required: bool = Field(False, description="是否必需")
    open: bool = Field(True, description="此参数是否对外部世界（LLM、最终用户）开放")
    role: Optional[str] = Field(None, description="参数在其上下文中扮演的功能性角色 (e.g., 'http.query')")
    label: Optional[str] = Field(None, description="在UI中展示的标签或名称")
    value: Optional[ParameterValue] = Field(None, description="实例级别的配置值")
    meta: Optional[Dict[str, Any]] = Field(None, description="用于存储非标准的、特定于实现的附加信息")

# [关键] 更新前向引用
# 在定义完所有模型后，调用 model_rebuild() 来解析字符串形式的前向引用
SchemaBlueprint.model_rebuild()
ParameterSchema.model_rebuild()
```

# ========== FILE: src/app/engine/tool/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/tool/callbacks.py ==========
```python
# engine/tool/callbacks.py

from typing import Protocol, Dict, Any

class ToolEngineCallbacks(Protocol):
    """
    定义了 ToolEngineService 在执行过程中向外报告事件的接口。
    [修正] 所有参数均为原生 Python 类型。
    """
    async def on_start(self, context: Dict[str, Any], inputs: Dict[str, Any]) -> None:
        """在执行开始时调用。context 用于传递日志所需的元数据。"""
        ...

    async def on_log(self, message: str, metadata: Dict[str, Any] = None) -> None:
        ...

    async def on_success(self, result: Dict[str, Any], raw_response: Any) -> None:
        ...
        
    async def on_error(self, error: Exception) -> None:
        ...
```

# ========== FILE: src/app/engine/tool/main.py ==========
```python
# engine/tool/main.py

import httpx
import jsonschema
from typing import Any, Dict, List, Literal

from ..schemas.parameter_schema import ParameterSchema
from ..utils.parameter_schema_utils import schemas2obj, build_json_schema_node
from .callbacks import ToolEngineCallbacks

class ToolEngineService:
    """
    纯粹的、无状态的 Tool 执行引擎。
    它完全不依赖任何数据库模型，只接收原生数据类型。
    """
    def __init__(self):
        self.http_client = httpx.AsyncClient(timeout=30.0)

    async def run(
        self,
        # --- [核心修正] 引擎的所有输入都是原生数据类型 ---
        method: Literal["GET", "POST", "PUT", "DELETE", "PATCH"],
        url: str,
        inputs_schema: List[ParameterSchema],
        outputs_schema: List[ParameterSchema],
        runtime_arguments: Dict[str, Any],
        callbacks: ToolEngineCallbacks,
        execution_context: Dict[str, Any], # 用于日志和追踪的上下文
        return_raw_response: bool = False
    ) -> Dict[str, Any] | Any:
        """
        [统一方法] 执行一个 Tool 定义。
        """
        await callbacks.on_start(execution_context, runtime_arguments)

        try:

            await self._validate_inputs(inputs_schema, runtime_arguments)
            await callbacks.on_log("Input validation successful.")
            
            request_parts = await self._build_request_parts(inputs_schema, runtime_arguments)
            formatted_url = self._format_url(url, request_parts['url_params'])
            await callbacks.on_log(f"Executing {method} request to {formatted_url}", metadata=request_parts)
            
            raw_response = await self._execute_request(
                method=method, url=formatted_url, headers=request_parts['headers'],
                params=request_parts['query_params'], json_body=request_parts['body']
            )
            await callbacks.on_log("Request successful.", metadata={"raw_response_preview": str(raw_response)[:500]})
            
            if return_raw_response:
                await callbacks.on_success(raw_response, raw_response)
                return raw_response

            await self._validate_outputs(outputs_schema, raw_response)
            await callbacks.on_log("Output validation successful.")

            shaped_output = await schemas2obj(outputs_schema, context={}, real_data=raw_response)
            
            await callbacks.on_success(shaped_output, raw_response)
            return shaped_output

        except Exception as e:
            await callbacks.on_error(e)
            raise

    # ... _validate_inputs, _validate_outputs, _build_request_parts, _format_url, _execute_request 等私有方法保持不变 ...
    # 它们已经只依赖原生数据类型，无需修改。

    async def _validate_inputs(self, inputs_schema: List[ParameterSchema], inputs: Dict[str, Any]):
        """使用 jsonschema 验证输入是否符合规范。"""
        if not inputs_schema:
            return # 没有定义 schema，跳过验证
        properties = {}
        required = []
        # 将我们的 ParameterSchema 转换为标准 JSON Schema
        for param in inputs_schema:
            if param.name:
                properties[param.name] = build_json_schema_node(param)
            if param.required:
                required.append(param.name)

        schema_root = {
            "type": "object",
            "properties": properties,
            "required": required
        }
        
        try:
            jsonschema.validate(instance=inputs, schema=schema_root)
        except jsonschema.ValidationError as e:
            # 抛出一个更具体的、可被上层捕获的异常
            raise ValueError(f"Input validation failed: {e.message}")

    async def _validate_outputs(self, outputs_schema: List[ParameterSchema], response: Dict[str, Any]):
        """验证工具的输出是否符合规范。"""
        # 逻辑与 _validate_inputs 类似，但针对 outputs_schema
        if not outputs_schema:
            return
        
        schema_root = {
            "type": "object",
            "properties": {param.name: build_json_schema_node(param) for param in outputs_schema if param.name},
            # 输出通常不强制要求所有字段都存在
        }
        
        try:
            jsonschema.validate(instance=response, schema=schema_root)
        except jsonschema.ValidationError as e:
            raise ValueError(f"Output validation failed: {e.message}")

    async def _build_request_parts(self, inputs_schema: List[ParameterSchema], runtime_arguments: Dict) -> Dict:
        """
        Builds all parts of the HTTP request by shaping a complete object from the inputs_schema,
        which merges defaults and runtime arguments, and then distributing the values based on role.
        """
        full_request_obj = await schemas2obj(inputs_schema, context={}, real_data=runtime_arguments)
        url_params, headers, query_params = {}, {}, {}
        body_obj = {}

        for param in inputs_schema:
            name = param.name
            role = param.role
            if not name or not role: continue

            value = full_request_obj.get(name)
            if value is None: continue

            if role == 'http.path': url_params[name] = str(value)
            elif role == 'http.header': headers[name] = str(value)
            elif role == 'http.query': query_params[name] = value
            elif role == 'http.body': body_obj[name] = value
        
        # Now handle complex body. If only one body param and it's an object, it might be the whole body
        # This logic can be refined, but it's a solid start.
        final_body = None
        if body_obj:
            # If multiple fields have role 'http.body', wrap them in an object.
            final_body = body_obj

        return {
            "url_params": url_params, "headers": headers,
            "query_params": query_params, "body": final_body
        }

    def _format_url(self, base_url: str, url_params: Dict[str, str]) -> str:
        """
        如果模板需要的参数没有被提供，它会立即因 KeyError 而失败。
        """
        try:
            for name, value in url_params.items():
                base_url = base_url.replace(f"{{{name}}}", value)
            return base_url
        except KeyError as e:
            raise ValueError(
                f"URL substitution failed. The URL template requires path parameter '{e.args[0]}', "
                "which was not found in the provided inputs with role 'http.path'."
            )

    async def _execute_request(self, method: str, url: str, headers: Dict, params: Dict, json_body: Dict) -> Any:
        """核心的 HTTP 请求执行逻辑。"""
        try:
            response = await self.http_client.request(
                method=method.upper(), url=url, headers=headers,
                params=params, json=json_body
            )
            response.raise_for_status()
            # 假设所有工具都返回 JSON
            return response.json()
        except httpx.HTTPStatusError as e:
            # 捕获 HTTP 错误并提供更多上下文
            raise IOError(f"HTTP error {e.response.status_code} for {e.request.url}: {e.response.text}")
        except httpx.RequestError as e:
            # 捕获网络层面的错误
            raise IOError(f"Request failed for {e.request.url}: {e}")
```

# ========== FILE: src/app/engine/utils/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/utils/data_parser.py ==========
```python
# engine/utils/data_parser.py

from pydantic import BaseModel
from typing import Optional, List, Dict, Any, AsyncGenerator
from hashlib import md5
import json, time, re
from datetime import datetime
import asyncio

# ====================================================================
# ===== 便捷函数 =====
# ====================================================================

async def merge_dicts_vanilla(dict_list: list):
    merged = {}
    if not dict_list:
        return merged
    for d in dict_list:
        for key, value in d.items():
            if key not in merged:
                merged[key] = []
            merged[key].append(value)
    return merged

async def array_to_object(arr, key_field):
    return {obj[key_field]: obj for obj in arr}

async def find_key_by_order(data, order):
    # 遍历字典的键值对
    for key, value in data.items():
        if value.get("order") == order:  # 检查 order 是否匹配
            return key  # 返回匹配的键名
    return None  # 如果没有找到，返回 None

async def split_expression(expression: str) -> list:
    """将表达式文本按照 {{ 和 }} 分割成数组"""
    parts = re.split(r'(\{\{.*?\}\})', expression)
    parts = [part for part in parts if part]
    return parts

async def get_value_by_path(data, path):
    if isinstance(path, list):
        path = '.'.join(path)

    if not isinstance(path, str):
        raise TypeError("path must be a string")

    # 使用正则表达式解析路径，保留数组索引
    keys = re.findall(r'\w+|\[\d+\]', path)

    if not keys:
        raise TypeError("path match is empty")
    
    value = data

    for i, key in enumerate(keys):
        if value is None:
            return None  # 路径不存在，返回 None

        if key.startswith('[') and key.endswith(']'):
            # 处理数组索引
            index = int(key[1:-1])
            if isinstance(value, list) and 0 <= index < len(value):
                value = value[index]
            else:
                return None  # 数组索引超出范围或类型不匹配
        else:
            # 处理字典键
            if isinstance(value, dict):
                value = value.get(key)
            elif isinstance(value, list):
                # 处理嵌套数组，没有提供具体索引
                if i < len(keys) - 1:
                    next_key = keys[i + 1]
                    if next_key.startswith('[') and next_key.endswith(']'):
                        # 如果下一个键是数组索引，则继续遍历
                        value = [await get_value_by_path(item, '.'.join(keys[i:])) for item in value]
                        break
                    else:
                        # 否则，遍历数组并获取每个元素的值
                        value = [item.get(key) if isinstance(item, dict) else await get_value_by_path(item, '.'.join(keys[i:])) for item in value]
                else:
                    # 如果已经是路径的最后一部分，遍历数组并获取每个元素的值
                    value = [item.get(key) if isinstance(item, dict) else await get_value_by_path(item, '.'.join(keys[i:])) for item in value]
            else:
                return None  # 键不存在或类型不匹配

    return value  # 返回最终值

async def get_value_by_expr_template(template, data):
    """
    异步地替换模板字符串中的 {{ }} 表达式。
    
    :param template: 模板字符串
    :param data: 包含数据的字典
    :return: 替换后的字符串
    """
    if not isinstance(template, str):
        return template
    pattern = r'\{\{([^}]+)\}\}'
    matches = re.findall(pattern, template)
    
    # 如果没有占位符，直接返回原模板
    if not matches:
        return template
    
    # 如果只有一个占位符且整个模板就是占位符本身
    if len(matches) == 1 and template == f'{{{{{matches[0]}}}}}':
        value = await get_value_by_path(data, matches[0])
        return str(value) if value is not None else template

    async def process_match(match):
        value = await get_value_by_path(data, match)
        if value is None:
            return f'{{{{{match}}}}}'  # 如果找不到路径，保留原字符串
        return str(value)
    
    # 创建任务列表
    tasks = [process_match(match) for match in matches]
    
    # 并发执行任务
    results = await asyncio.gather(*tasks)
    
    # 构建替换后的字符串
    def replacement(match):
        index = matches.index(match.group(1))
        return results[index]
    
    return re.sub(pattern, replacement, template)

def get_default_value_by_type(type_str: str):
    """
    根据类型字符串返回对应类型的默认值。

    :param type_str: 类型字符串，如 'string', 'number', 'integer', 'boolean', 'date', 'object', 'array', 'file'
    :return: 对应类型的默认值
    """
    type_str = type_str.lower()
    if type_str == 'string':
        return ""
    elif type_str == 'number':
        return 0
    elif type_str == 'integer':
        return 0
    elif type_str == 'boolean':
        return False
    elif type_str == 'date':
        return datetime.now().date()  # 返回当前日期作为默认值
    elif type_str == 'object':
        return {}
    elif type_str == 'array':
        return []
    elif type_str == 'file':
        return None  # 文件类型通常没有默认值，返回 None
    else:
        return None  # 未知类型返回 None

def smart_cast_to_number(value: Any) -> Any:
    """
    Intelligently casts a value to a number (int or float).
    If it can be an integer without data loss, it returns an int.
    Otherwise, it returns a float.
    If it cannot be converted to a number, it returns a default value (0).
    """
    try:
        float_val = float(value)
        if float_val == int(float_val):
            return int(float_val)
        else:
            return float_val
    except (ValueError, TypeError):
        return 0 # Using 0 as the default for number.

def convert_value_by_type(value: Any, type_str: str) -> Any:
    """
    Safely converts a value to a specified type.
    If conversion fails, it returns a safe default value for that type.
    """
    if value is None:
        # None is a valid value for any type if not required, but here we are casting.
        # Returning the default is safer if None is unexpected.
        return get_default_value_by_type(type_str)

    # 1. Check if the type is already correct
    py_type_map = {'string': str, 'integer': int, 'number': float, 'boolean': bool, 'object': dict, 'array': list}
    if type_str in py_type_map and isinstance(value, py_type_map[type_str]):
        return value

    # 2. If not, attempt safe conversion        
    try:
        type_str_lower = type_str.lower()
        if type_str_lower == 'string':
            return str(value)
        elif type_str_lower == 'integer':
            return int(float(value))
        elif type_str_lower == 'number':
            return smart_cast_to_number(value)
        elif type_str_lower == 'boolean':
            if isinstance(value, str):
                return value.lower() == 'true'
            return bool(value)
        elif type_str_lower in ['object', 'array']:
            if isinstance(value, str):
                return json.loads(value)
            # If it's some other non-string, non-dict/list type, conversion is ambiguous.
            # Fallback to default.
            return get_default_value_by_type(type_str)
        else:
            # For unknown types, return the value as is.
            return value
    except Exception as e:
        return get_default_value_by_type(type_str)
```

# ========== FILE: src/app/engine/utils/parameter_schema_utils.py ==========
```python
# engine/utils/parameter_schema_utils.py

import json
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
from ..schemas.parameter_schema import ParameterSchema, SchemaBlueprint, ParameterValue
from .stream import Streamable
from .data_parser import get_value_by_path, get_value_by_expr_template, get_default_value_by_type, convert_value_by_type

# ========================================================================
# build_json_schema_node (from previous step, unchanged and correct)
# ========================================================================

def build_json_schema_node(param_schema: Union[ParameterSchema, SchemaBlueprint]) -> Dict[str, Any]:
    node_type = param_schema.type
    if not node_type:
        raise ValueError("Parameter properties must have a 'type'.")

    result = {"description": param_schema.description or ''}
    
    # [FIX] Add default value if it exists
    if 'default' in param_schema.model_fields_set:
        result['default'] = param_schema.default

    type_map = {'string': 'string', 'number': 'number', 'integer': 'integer', 'boolean': 'boolean'}

    if node_type in type_map:
        result['type'] = type_map[node_type]
        if param_schema.enum: result['enum'] = param_schema.enum
    
    elif node_type == 'object':
        result['type'] = 'object'
        properties = {}
        required = []
        for sub_param in param_schema.properties or []:
            prop_name = sub_param.name
            if not prop_name: continue
            properties[prop_name] = build_json_schema_node(sub_param)
            if sub_param.required: required.append(prop_name)
        result['properties'] = properties
        if required: result['required'] = required

    elif node_type == 'array':
        result['type'] = 'array'
        items_schema = param_schema.items
        if not items_schema: raise ValueError("Array type must have an 'items' properties.")
        result['items'] = build_json_schema_node(items_schema)
    
    else:
        raise ValueError(f"Unsupported parameter type: {node_type}")

    return result

# ========================================================================
# schema_filler (The new, powerful, universal implementation)
# ========================================================================

async def schemas2obj(target_schema: List[ParameterSchema], context: Optional[Dict[str, Any]] = None, real_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    The main entry point. Creates a structured object based on the target_schema.
    It intelligently fills the structure using a combination of:
    1. `real_data` (e.g., an API response) - Highest priority.
    2. `value` definitions within the properties (for refs, literals, exprs) - Medium priority.
    3. `default` values within the properties - Lowest priority.
    4. Type-based defaults (e.g., "" for string) - Fallback.
    
    This function is adapted from the robust `schemas2obj` logic.
    """
    if real_data is None: real_data = {}
    if context is None: context = {}
    
    result = {}
    if not isinstance(target_schema, list): return result

    for item_schema in target_schema:
        if not isinstance(item_schema, ParameterSchema): continue
        item_name = item_schema.name
        if not item_name: continue

        # Pass the corresponding part of the real_data for processing.
        current_real_data = real_data.get(item_name)
        result[item_name] = await _process_schema_node(item_schema, context, current_real_data)
        
    return result

async def _process_schema_node(item_schema: Union[ParameterSchema, SchemaBlueprint], context: Dict[str, Any], real_data_for_item: Any):
    """
    Recursively processes a single properties node to determine its final value.
    This is the core recursive worker.
    """
    item_type = item_schema.type
    # --- Step 1: Determine the raw value based on priority ---
    return_value = None
    priority_source = "none" # For debugging

    # Priority 1: Use real_data if it exists.
    if real_data_for_item is not None:
        return_value = real_data_for_item
        priority_source = "real_data"
    
    # Priority 2: Evaluate 'value' definition (ref, literal, etc.)
    elif isinstance(item_schema, ParameterSchema) and item_schema.value:
        value_type = item_schema.value.type
        content = item_schema.value.content
        priority_source = value_type

        if value_type == 'literal':
            return_value = content
        elif value_type == 'ref':
            # This part is for workflow context, but we implement it for future-proofing.
            # In a stateless plugin call, `context` will be empty.
            block_id = content.get('blockID')
            path = content.get('path')
            if block_id and path and context:
                source_data = context.get(block_id)
                if isinstance(source_data, Streamable):
                    source_data = await source_data.get_result()
                if source_data and path:
                    return_value = await get_value_by_path(source_data, path)
        # elif value_type == 'expr':
            # Expression evaluation logic would go here
    
    # Priority 3: Use the properties's 'default' value.
    if return_value is None and 'default' in item_schema.model_fields_set:
        return_value = item_schema.default
        priority_source = "default"

    # --- Step 2: Shape the return_value according to the properties's type ---
    if item_type == 'object':
        # Ensure return_value is a dict for processing; otherwise, start with an empty dict.
        source_obj = convert_value_by_type(return_value, item_type)
        sub_schemas = item_schema.properties or []
        
        # Always return a dictionary. If no sub-schemas, return the source object (or empty).
        if not sub_schemas:
            return source_obj
        
        # Recursively call the main function to process sub-schemas.
        # This ensures consistent filling and shaping logic at all levels.
        return await schemas2obj(sub_schemas, context, source_obj)

    elif item_type == 'array':
        # Ensure return_value is a list; otherwise, start with an empty list.
        source_list = convert_value_by_type(return_value, item_type)
        items_blueprint = item_schema.items
        
        # If no item properties is defined, we cannot shape the items. Return the list as is.
        if not items_blueprint or not isinstance(items_blueprint, SchemaBlueprint):
            return [] # source_list

        # **关键修正**: 如果 source_list 为空，但 items_blueprint 存在，
        # 这意味着我们需要“从零构建”一个包含示例项的列表。
        if not source_list:
            # 递归调用自身来构建一个默认的列表项
            default_item = await _process_schema_node(items_blueprint, context, None)
            return [default_item]
            
        # 遍历继承的列表，对每一项应用 sub_schema 进行塑形
        shaped_list = []
        for item_data in source_list:
            # 将列表中的项作为 real_data 传递给下一层递归进行处理
            shaped_item = await _process_schema_node(items_blueprint, context, item_data)
            shaped_list.append(shaped_item)
        return shaped_list

    else: # --- Step 3: Handle primitive types and final fallbacks ---
        # If after all priority checks, return_value is still None, apply final fallback.
        if return_value is None:
            return get_default_value_by_type(item_type)
        
    # Here you could add type casting for robustness (e.g., int(return_value))
    return convert_value_by_type(return_value, item_type)
```

# ========== FILE: src/app/engine/utils/stream/__init__.py ==========
```python
from .base import Streamable
from .broadcaster import StreamBroadcaster
```

# ========== FILE: src/app/engine/utils/stream/base.py ==========
```python
from abc import ABC, abstractmethod
from typing import Any, AsyncGenerator

class Streamable(ABC):
    """流式接口定义"""
    
    @abstractmethod
    async def get_result(self) -> Any:
        pass
        
    @abstractmethod
    async def cancel(self):
        pass

    @abstractmethod
    def subscribe(self) -> AsyncGenerator[Any, None]:
        pass
```

# ========== FILE: src/app/engine/utils/stream/broadcaster.py ==========
```python
import asyncio
from typing import Optional, List, Dict, Any, AsyncGenerator
from .base import Streamable

class StreamBroadcaster(Streamable):
    """
    通用异步流广播器。
    允许多个消费者订阅同一个数据流，并在流结束时触发回调。
    """
    def __init__(
        self, 
        id: str
    ):
        self._id = id
        self._task = None
        self._queues = []
        self._history = []
        self._history_lock = asyncio.Lock()
        self._sentinel = object()
        self._stopped = False

    async def _task_wrapper(self, task_coro):
        """包装流式任务，确保完成后发出信号。"""
        try:
            # 执行实际的流式处理协程
            return await task_coro
        except Exception as e:
            print(f"[StreamBroadcaster] Error in task for {self._id}: {e}")
            await self.broadcast({"type": "error", "data": str(e)})
            raise e
        finally:
            # 向所有监听者发送停止信号
            async with self._history_lock: # 确保在发送Stop前不再有新数据进入
                self._stopped = True
                for q in self._queues:
                    await q.put(self._sentinel)

    def create_task(self, task_coro) -> asyncio.Task:
        """启动后台流式任务"""
        self._task = asyncio.create_task(self._task_wrapper(task_coro))
        return self._task

    def get_task(self) -> Optional[asyncio.Task]:
        return self._task

    async def get_result(self) -> Any:
        """
        直接利用 Task 作为 Future 的特性。
        """
        if not self._task:
            raise RuntimeError("Task not started")
        # 直接等待 Task，它处理了 Result 返回和 Exception 抛出
        return await self._task

    async def cancel(self):
        """允许外部取消内部任务"""
        if self._task and not self._task.done():
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass

    async def broadcast(self, chunk: Any):
        """广播数据块。"""
        async with self._history_lock:
            self._history.append(chunk)
            for q in self._queues:
                await q.put(chunk)

    def subscribe(self) -> AsyncGenerator[Any, None]:
        """获取订阅生成器。"""
        q = asyncio.Queue()
        
        async def gen():
            # 发送历史数据
            async with self._history_lock:
                for item in self._history:
                    await q.put(item)
                
                # 如果已经结束，直接发停止信号
                if self._stopped:
                    await q.put(self._sentinel)
                else:
                    self._queues.append(q)

            try:
                while True:
                    value = await q.get()
                    if value is self._sentinel:
                        break
                    yield value
                    q.task_done()
            finally:
                # 清理
                if q in self._queues:
                    self._queues.remove(q)
        
        return gen()
```

# ========== FILE: src/app/engine/utils/tokenizer/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/utils/tokenizer/base.py ==========
```python
# src/app/engine/utils/tokenizer/base.py

from typing import List, Protocol, Union

class BaseTokenizer(Protocol):
    """
    Tokenizer 的统一协议接口。
    """
    def encode(self, text: str) -> List[int]:
        """将文本转换为 Token ID 列表"""
        ...

    def decode(self, tokens: List[int]) -> str:
        """将 Token ID 列表转换回文本"""
        ...

    def count(self, text: str) -> int:
        """快速计算文本的 Token 数量"""
        ...
```

# ========== FILE: src/app/engine/utils/tokenizer/bundled.py ==========
```python
# src/app/engine/utils/tokenizer/bundled.py

import logging
from typing import List, Optional
try:
    import tiktoken
except ImportError:
    tiktoken = None

from .base import BaseTokenizer

logger = logging.getLogger(__name__)

class TiktokenTokenizer:
    """
    基于 OpenAI tiktoken 的实现。
    支持 model 自动推断 encoding。
    """
    def __init__(self, model_name: str = "gpt-4"):
        if not tiktoken:
            raise ImportError("tiktoken is not installed.")
        
        try:
            self.encoding = tiktoken.encoding_for_model(model_name)
        except KeyError:
            # 对于未知模型，默认使用 GPT-4 的 cl100k_base，这是目前最通用的 BPE
            logger.warning(f"Model '{model_name}' not found in tiktoken. Falling back to 'cl100k_base'.")
            self.encoding = tiktoken.get_encoding("cl100k_base")

    def encode(self, text: str) -> List[int]:
        return self.encoding.encode(text)

    def decode(self, tokens: List[int]) -> str:
        return self.encoding.decode(tokens)

    def count(self, text: str) -> int:
        if not text:
            return 0
        return len(self.encode(text))

class CharacterRatioTokenizer:
    """
    [兜底策略] 基于字符比率的估算器。
    用于没有本地 Tokenizer 库的模型 (如 Gemini, Claude 的本地库比较重或不存在)。
    """
    def __init__(self, ratio: float = 0.3):
        # 经验值：中文约 0.6-0.8，英文约 0.25-0.3。
        # 为了计费安全（宁少勿多防止超扣），或者为了上下文安全（宁多勿少防止超长），策略不同。
        # 这里取一个折中值 0.5 (2 chars ≈ 1 token)
        self.ratio = ratio

    def encode(self, text: str) -> List[int]:
        # 伪实现：返回虚构的 token ids
        return [0] * self.count(text)

    def decode(self, tokens: List[int]) -> str:
        raise NotImplementedError("Ratio tokenizer cannot decode.")

    def count(self, text: str) -> int:
        if not text:
            return 0
        return int(len(text) * self.ratio) + 1
```

# ========== FILE: src/app/engine/utils/tokenizer/manager.py ==========
```python
# src/app/engine/utils/tokenizer/manager.py

import logging
from typing import Dict, Type, Callable, Optional, Union
from .base import BaseTokenizer
from .bundled import TiktokenTokenizer, CharacterRatioTokenizer

logger = logging.getLogger(__name__)

# 定义构造函数类型：接收 model_name，返回实例
TokenizerFactory = Callable[[str], BaseTokenizer]

class TokenizerManager:
    """
    全局 Tokenizer 管理器。
    单例模式，负责路由和缓存。
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(TokenizerManager, cls).__new__(cls)
            cls._instance._initialize()
        return cls._instance

    def _initialize(self):
        self._cache: Dict[str, BaseTokenizer] = {}
        self._factories: Dict[str, TokenizerFactory] = {}
        
        # --- 1. 注册默认策略 ---
        
        # OpenAI Family
        self.register_factory("openai", lambda m: TiktokenTokenizer(m))
        self.register_factory("azure", lambda m: TiktokenTokenizer(m))
        
        # DeepSeek / Qwen (通常兼容 cl100k_base)
        self.register_factory("deepseek", lambda m: TiktokenTokenizer("gpt-4"))
        self.register_factory("qwen", lambda m: TiktokenTokenizer("gpt-4"))
        
        # Google Gemini / Anthropic Claude
        # 如果没有安装特定库，使用保守估算 (1 token ≈ 2.5 chars for mixed content)
        self.register_factory("google", lambda m: CharacterRatioTokenizer(ratio=0.4))
        self.register_factory("anthropic", lambda m: CharacterRatioTokenizer(ratio=0.4))
        
        # Default Fallback
        self._default_factory = lambda m: TiktokenTokenizer("gpt-4")

    def register_factory(self, provider: str, factory: TokenizerFactory):
        """
        允许上层注册新的 Tokenizer 实现。
        provider: 厂商标识 (openai, google, zhipu...)
        """
        self._factories[provider.lower()] = factory

    def get_tokenizer(self, provider: str, model: str) -> BaseTokenizer:
        """
        获取 Tokenizer 实例。优先匹配 Provider，其次尝试推断。
        """
        cache_key = f"{provider}:{model}"
        if cache_key in self._cache:
            return self._cache[cache_key]

        factory = None
        
        # 1. 尝试通过 Provider 路由
        if provider and provider.lower() in self._factories:
            factory = self._factories[provider.lower()]
        
        # 3. 兜底
        if not factory:
            logger.debug(f"No specific tokenizer found for {provider}/{model}, using default tiktoken.")
            factory = self._default_factory

        try:
            tokenizer = factory(model)
        except Exception as e:
            logger.error(f"Failed to instantiate tokenizer for {model}: {e}. Using CharRatio fallback.")
            tokenizer = CharacterRatioTokenizer()

        self._cache[cache_key] = tokenizer
        return tokenizer

    def count_tokens(self, text: str, provider: str, model: str) -> int:
        """便捷静态方法"""
        tokenizer = self.get_tokenizer(provider, model)
        return tokenizer.count(text)

# 全局单例
tokenizer_manager = TokenizerManager()
```

# ========== FILE: src/app/engine/vector/__init__.py ==========
```python
# engine/vector/__init__.py

from .base import VectorEngineService
```

# ========== FILE: src/app/engine/vector/base.py ==========
```python
# engine/vector/base.py
from abc import ABC, abstractmethod
from typing import Literal, List, Dict, Any, NamedTuple

class VectorEngineConfig(NamedTuple):
    """
    一个标准的、可序列化的模型，用于定义任何向量引擎的连接配置。
    """
    engine_type: Literal["milvus", "qdrant"] # 未来可扩展
    host: str
    port: int
    alias: str # 此配置的唯一别名，如 'default_milvus', 'high_perf_qdrant'

class VectorChunk(NamedTuple):
    id: str
    vector: List[float]
    payload: Dict[str, Any] # Will store content and other metadata

class SearchResult(NamedTuple):
    id: str
    score: float
    payload: Dict[str, Any]

class VectorEngineError(Exception):
    """Base exception for all engine errors"""
    pass
    
class VectorEngineService(ABC):
    """
    The abstract interface for all vector database operations.
    It is completely stateless and works with native Python types.
    """
    @abstractmethod
    async def create_collection(self, name: str, vector_size: int):
        raise NotImplementedError

    @abstractmethod
    async def delete_collection(self, name: str):
        raise NotImplementedError

    @abstractmethod
    async def list_collections(self) -> List[str]:
        """List all collection names in the vector database."""
        raise NotImplementedError
        
    @abstractmethod
    async def insert(self, collection_name: str, chunks: List[VectorChunk]):
        raise NotImplementedError

    @abstractmethod
    async def upsert(self, collection_name: str, chunks: List[VectorChunk]):
        raise NotImplementedError

    @abstractmethod
    async def delete(self, collection_name: str, pks: List[str], filter_expr: str) -> int:
        raise NotImplementedError

    @abstractmethod
    async def query(self, collection_name: str, pks: List[str], filter_expr: str, output_fields: List[str]) -> List[VectorChunk]:
        raise NotImplementedError

    @abstractmethod
    async def search(
        self,
        collection_name: str,
        query_vector: List[float],
        top_k: int
    ) -> List[SearchResult]:
        raise NotImplementedError
```

# ========== FILE: src/app/engine/vector/main.py ==========
```python
# engine/vector/main.py
import asyncio
from typing import Dict, List
from .base import VectorEngineService, VectorEngineConfig, VectorEngineError
from .milvus_engine import MilvusEngine
# from .qdrant_engine import QdrantEngine # For future extension

class VectorEngineManager:
    """
    [核心管理器]
    管理所有向量引擎的配置和生命周期。
    - 在应用启动时初始化。
    - 懒加载并缓存底层客户端连接。
    - 按需提供具体的、请求作用域的 VectorEngineService 实例。
    """
    def __init__(self, configs: List[VectorEngineConfig]):
        self._configs: Dict[str, VectorEngineConfig] = {c.alias: c for c in configs}
        self._clients: Dict[str, any] = {} # 缓存已连接的底层客户端 (e.g., MilvusClient)
        self._client_locks: Dict[str, asyncio.Lock] = {c.alias: asyncio.Lock() for c in configs}

    async def startup(self):
        """
        [生命周期] 可以在启动时预连接默认客户端，或保持完全懒加载。
        为简单起见，我们保持懒加载，此方法目前为空。
        """
        print(f"VectorEngineManager initialized with {len(self._configs)} configurations.")

    async def shutdown(self):
        """[生命周期] 安全关闭所有已建立的客户端连接。"""
        print("Shutting down VectorEngineManager, closing all clients...")
        for alias, client in self._clients.items():
            config = self._configs.get(alias)
            if not config:
                raise VectorEngineError(f"Configuration with alias '{alias}' not found.")
            try:
                if config.engine_type == 'milvus':
                    client.close()
                print(f"Client for alias '{alias}' closed.")
            except Exception as e:
                print(f"Error closing client for alias '{alias}': {e}")
        self._clients.clear()

    async def _get_client(self, alias: str) -> any:
        """[懒加载核心] 按需创建并缓存底层客户端连接。"""
        if alias not in self._configs:
            raise VectorEngineError(f"Configuration with alias '{alias}' not found.")
        
        # 线程/协程安全地检查和创建客户端
        async with self._client_locks[alias]:
            if alias not in self._clients:
                print(f"Client for '{alias}' not found in cache. Creating new connection...")
                config = self._configs[alias]
                
                if config.engine_type == 'milvus':
                    from pymilvus import MilvusClient
                    uri = f"http://{config.host}:{config.port}"
                    self._clients[alias] = MilvusClient(uri=uri, timeout=5)
                # elif config.engine_type == 'qdrant':
                #     from qdrant_client import QdrantClient
                #     self._clients[alias] = QdrantClient(host=config.host, port=config.port)
                else:
                    raise NotImplementedError(f"Engine type '{config.engine_type}' is not supported.")
                print(f"Successfully created and cached client for '{alias}'.")

        return self._clients[alias]

    async def get_engine(self, alias: str) -> VectorEngineService:
        """
        [工厂方法]
        获取一个配置好的、可用的 VectorEngineService 实例。
        这是应用层（依赖项、Worker）应该调用的唯一方法。
        """
        config = self._configs.get(alias)
        if not config:
            raise VectorEngineError(f"Configuration with alias '{alias}' not found.")
            
        client = await self._get_client(alias)
        
        if config.engine_type == 'milvus':
            return MilvusEngine(client=client)
        # elif config.engine_type == 'qdrant':
        #     return QdrantEngine(client=client)
        else:
            raise NotImplementedError(f"Engine type '{config.engine_type}' is not supported.")
```

# ========== FILE: src/app/engine/vector/milvus_engine.py ==========
```python
# engine/vector/milvus_engine.py

import logging
import re
import json
import asyncio
from typing import List, Dict, Any

from pymilvus import (
    MilvusClient,
    CollectionSchema,
    FieldSchema,
    DataType,
    exceptions,
    utility
)

from .base import VectorEngineService, VectorChunk, SearchResult, VectorEngineError

# 使用标准日志记录器
logger = logging.getLogger(__name__)

# 定义特定于此引擎的异常
class MilvusEngineError(VectorEngineError):
    """Base exception for all Milvus engine errors, abstracting pymilvus specifics."""
    pass

class MilvusEngine(VectorEngineService):
    """
    A stateless, robust, and production-ready implementation of VectorEngineService using Milvus.

    This class is designed to be a lightweight, request-scoped service that operates on a
    globally managed, long-lived MilvusClient instance. It embodies the following principles:
    - **Stateless**: It does not hold any in-memory state about collections (e.g., load status).
    - **Dependency Injected**: It receives a connected MilvusClient instance upon initialization.
    - **Error Resilient**: Implements a retry mechanism for transient "collection not loaded" errors.
    - **Schema-Driven**: Uses a consistent, predefined schema for all managed collections.
    """
    PK_FIELD = "pk"
    VECTOR_FIELD = "vector"
    PAYLOAD_FIELD = "payload"
    METRIC_TYPE = "IP"

    def __init__(self, client: MilvusClient):
        """
        Initializes the engine with a pre-configured MilvusClient.

        Args:
            client: A connected instance of pymilvus.MilvusClient.
        """
        self.client = client

    def _validate_collection_name(self, name: str):
        """
        Validates the collection name against Milvus naming conventions to prevent errors.
        """
        if not re.match(r"^[a-zA-Z_][a-zA-Z0-9_]{0,254}$", name):
            raise ValueError(f"Invalid collection name: '{name}'. Name must start with a letter or underscore, "
                             "followed by letters, numbers, or underscores, and be at most 255 characters long.")

    def _prepare_data_for_milvus(self, chunks: List[VectorChunk]) -> List[Dict[str, Any]]:
        """[新增] 内部辅助函数，用于数据转换，避免代码重复。"""
        return [
            {
                self.PK_FIELD: c.id,
                self.VECTOR_FIELD: c.vector,
                self.PAYLOAD_FIELD: c.payload
            }
            for c in chunks
        ]

    def _build_filter_expr(self, pks: List[str] = None, filter_expr: str = None) -> str:
        """[内部辅助] 安全地构建最终的 filter 表达式。"""
        if pks and filter_expr:
            raise ValueError("Cannot provide both 'pks' and 'filter_expr'.")
        
        if pks:
            if not all(isinstance(pk, (str, int)) for pk in pks):
                raise TypeError("All primary keys in 'pks' must be strings or integers.")
            # 正确格式化 for 'in' operator
            formatted_pks = json.dumps(pks)
            return f'{self.PK_FIELD} in {formatted_pks}'
        
        if filter_expr:
            # [未来] 在这里可以添加对 filter_expr 的安全校验
            return filter_expr
            
        raise ValueError("Must provide either 'pks' or 'filter_expr'.")

    async def create_collection(self, name: str, vector_size: int):
        """
        Creates a new collection in Milvus if it doesn't exist, including schema, index, and loading.
        This operation is idempotent.
        """
        self._validate_collection_name(name)
        try:
            if self.client.has_collection(collection_name=name):
                logger.info(f"Collection '{name}' already exists. Ensuring it is loaded.")
                # Even if it exists, a load command is idempotent and ensures it's ready.
                self.client.load_collection(collection_name=name)
                return

            logger.info(f"Collection '{name}' does not exist. Creating...")
            
            # Define a consistent schema for all our collections
            fields = [
                FieldSchema(name=self.PK_FIELD, dtype=DataType.VARCHAR, is_primary=True, max_length=64,
                            description="Unique identifier for the vector chunk"),
                FieldSchema(name=self.VECTOR_FIELD, dtype=DataType.FLOAT_VECTOR, dim=vector_size,
                            description="The vector embedding"),
                FieldSchema(name=self.PAYLOAD_FIELD, dtype=DataType.JSON,
                            description="Business-specific metadata")
            ]
            schema = CollectionSchema(fields=fields, description=f"PrismaSpace managed collection: {name}")
            
            self.client.create_collection(collection_name=name, schema=schema)
            logger.info(f"Collection '{name}' created with schema. Proceeding to create index...")

            # Prepare and create the index
            index_params = self.client.prepare_index_params()
            index_params.add_index(
                field_name=self.VECTOR_FIELD,
                index_type="IVF_FLAT",  # A balanced choice for general purpose use
                metric_type=self.METRIC_TYPE,
                params={"nlist": 1024} # A common starting point for nlist
            )
            self.client.create_index(collection_name=name, index_params=index_params)
            logger.info(f"Index created for collection '{name}'. Loading collection into memory...")

            # Load the collection into memory to make it searchable
            self.client.load_collection(collection_name=name)
            logger.info(f"Collection '{name}' created, indexed, and loaded successfully.")

        except exceptions.MilvusException as e:
            logger.error(f"A Milvus error occurred while creating collection '{name}': {e}")
            raise VectorEngineError(f"Failed to create collection '{name}': {e}")
        except Exception as e:
            logger.error(f"An unexpected error occurred during collection creation '{name}': {e}")
            raise VectorEngineError(f"An unexpected error occurred: {e}")

    async def delete_collection(self, name: str):
        """
        Drops a collection from Milvus. This operation is idempotent.
        It's good practice to release the collection from memory before dropping.
        """
        self._validate_collection_name(name)
        try:
            if self.client.has_collection(collection_name=name):
                # Release is not strictly required before drop, but it's a clean pattern
                self.client.release_collection(collection_name=name)
                self.client.drop_collection(collection_name=name)
                logger.info(f"Collection '{name}' released and dropped successfully.")
            else:
                logger.info(f"Collection '{name}' does not exist. Skipping deletion.")
        except exceptions.MilvusException as e:
            logger.error(f"A Milvus error occurred while dropping collection '{name}': {e}")
            raise VectorEngineError(f"Failed to drop collection '{name}': {e}")

    async def list_collections(self) -> List[str]:
        try:
            return self.client.list_collections()
        except Exception as e:
            logger.error(f"Failed to list collections: {e}")
            raise VectorEngineError(f"List collections failed: {e}")
            
    async def insert(self, collection_name: str, chunks: List[VectorChunk]) -> int:
        """For bulk ingestion of new data. More performant for large scale."""
        if not chunks:
            return 0
        self._validate_collection_name(collection_name)
        try:
            data_to_insert = self._prepare_data_for_milvus(chunks)
            res = self.client.insert(collection_name=collection_name, data=data_to_insert)
            logger.info(f"Inserted {res.get('insert_count')} chunks into '{collection_name}'.")
            return res.get('insert_count')
        except exceptions.MilvusException as e:
            # 特别是主键冲突的异常
            logger.error(f"A Milvus error occurred while inserting chunks into '{collection_name}': {e}")
            raise VectorEngineError(f"Failed to insert chunks into '{collection_name}': {e}")

    async def upsert(self, collection_name: str, chunks: List[VectorChunk]) -> int:
        """For updating existing data or inserting small amounts of data."""
        if not chunks:
            return 0
        self._validate_collection_name(collection_name)
        try:
            data_to_upsert = self._prepare_data_for_milvus(chunks)
            res = self.client.upsert(collection_name=collection_name, data=data_to_upsert)
            logger.info(f"Upserted {res.get('upsert_count')} chunks into '{collection_name}'.")
            return res.get('upsert_count')
        except exceptions.MilvusException as e:
            logger.error(f"A Milvus error occurred while upserting chunks into '{collection_name}': {e}")
            raise VectorEngineError(f"Failed to upsert chunks into '{collection_name}': {e}")

    async def delete(self, collection_name: str, pks: List[str] = None, filter_expr: str = None) -> int:
        # [NEW] Implementation from above
        self._validate_collection_name(collection_name)
        expr = self._build_filter_expr(pks=pks, filter_expr=filter_expr)
        try:
            res = self.client.delete(collection_name=collection_name, filter=expr)
            logger.info(f"Delete on '{collection_name}' successful.")
            return res.get('delete_count')
        except exceptions.MilvusException as e:
            raise VectorEngineError(f"Failed to delete from '{collection_name}': {e}")

    async def query(self, collection_name: str, pks: List[str] = None, filter_expr: str = None, output_fields: List[str] = None) -> List[VectorChunk]:
        # [NEW] Implementation from above
        self._validate_collection_name(collection_name)
        expr = self._build_filter_expr(pks=pks, filter_expr=filter_expr)
        final_output_fields = output_fields or [self.PK_FIELD, self.VECTOR_FIELD, self.PAYLOAD_FIELD]
        try:
            results = self.client.query(collection_name=collection_name, filter=expr, output_fields=final_output_fields)
            return [
                VectorChunk(
                    id=res.get(self.PK_FIELD),
                    vector=res.get(self.VECTOR_FIELD),
                    payload=res.get(self.PAYLOAD_FIELD)
                ) for res in results
            ]
        except exceptions.MilvusException as e:
            raise VectorEngineError(f"Failed to query chunks in '{collection_name}': {e}")
            
    async def search(
        self,
        collection_name: str,
        query_vector: List[float],
        top_k: int,
        filter_expr: str = None,
        max_retries: int = 3
    ) -> List[SearchResult]:
        """
        Performs a vector similarity search with optional metadata filtering.
        Includes a retry mechanism for "collection not loaded" errors.
        """
        self._validate_collection_name(collection_name)
        
        for attempt in range(max_retries + 1):
            try:
                # The search parameters are now part of the search call itself
                search_params = {
                    "metric_type": self.METRIC_TYPE, # Assuming IP, could be dynamically fetched if needed
                    "params": {"nprobe": 10},
                }

                results = self.client.search(
                    collection_name=collection_name,
                    data=[query_vector],
                    anns_field=self.VECTOR_FIELD,
                    limit=top_k,
                    filter=filter_expr,
                    search_params=search_params,
                    # Bounded consistency is a good default for RAG, ensuring reads see recent writes.
                    consistency_level="Bounded",
                    output_fields=[self.PK_FIELD, self.PAYLOAD_FIELD]
                )
                
                # The new MilvusClient returns a list of lists of Hit objects
                hits = results[0]
                search_results = []
                for hit in hits:
                    # For IP metric, distance is the inner product (higher is better),
                    # so we can use it directly as the score.
                    # For L2, hit.distance is Euclidean distance (lower is better).
                    # We will assume IP and directly use the score.
                    # A more advanced version could check metric_type and convert score if L2.
                    search_results.append(SearchResult(
                        id=hit.entity.get(self.PK_FIELD),
                        score=hit.distance,
                        payload=hit.entity.get(self.PAYLOAD_FIELD)
                    ))
                return search_results

            except exceptions.MilvusException as e:
                # This specific error message indicates the collection is in the cluster but not in memory
                if "collection not loaded" in str(e):
                    if attempt < max_retries:
                        logger.warning(f"Attempt {attempt + 1}: Collection '{collection_name}' not loaded. "
                                       f"Attempting to load and retry search...")
                        try:
                            self.client.load_collection(collection_name=collection_name)
                            await asyncio.sleep(0.2) # Small delay to allow loading to propagate
                            continue # Retry the search
                        except exceptions.MilvusException as load_e:
                            logger.error(f"Failed to explicitly load collection '{collection_name}' during retry: {load_e}")
                            # If loading fails, there's no point in retrying further
                            raise VectorEngineError(f"Failed to load and search collection '{collection_name}': {load_e}")
                
                # For all other Milvus errors, or if retries are exhausted, raise the wrapped error
                logger.error(f"A Milvus error occurred while searching in '{collection_name}' with expr='{filter_expr}': {e}")
                raise VectorEngineError(f"Failed to search in collection '{collection_name}': {e}")
        
        # This line should theoretically not be reached, but as a fallback:
        raise VectorEngineError(f"Search in '{collection_name}' failed after {max_retries + 1} attempts.")
```

# ========== FILE: src/app/engine/workflow/__init__.py ==========
```python
from .main import WorkflowEngineService
from .definitions import *
from .graph import WorkflowGraph
from .registry import NodeExecutor, NodeExecutionResult, WorkflowRuntimeContext, register_node
from .context import NodeState
from .orchestrator import WorkflowCallbacks, WorkflowOrchestrator
```

# ========== FILE: src/app/engine/workflow/context.py ==========
```python
from typing import Dict, Any, Literal, Optional
from pydantic import BaseModel, Field
from .definitions import NodeStatus, NodeResultData

class NodeState(BaseModel):
    node_id: str
    status: NodeStatus = "PENDING"
    input: Dict[str, Any] = Field(default_factory=dict, description="最终运行时输入")
    result: NodeResultData = Field(default_factory=NodeResultData, description="最终运行时输出")
    activated_port: str = "0"
    executed_time: float = 0.0

class WorkflowContext:
    def __init__(self, payload: Dict[str, Any]):
        self._payload = payload
        self._variables: Dict[str, Any] = {}
        self._node_states: Dict[str, NodeState] = {}
        self._version: int = 0 

    @property
    def payload(self) -> Dict[str, Any]:
        return self._payload

    @property
    def variables(self) -> Dict[str, Any]:
        return self._variables
    
    @property
    def version(self) -> int:
        return self._version

    def init_node_state(self, node_id: str) -> NodeState:
        self._node_states[node_id] = NodeState(node_id=node_id)
        return self._node_states[node_id]

    def get_node_state(self, node_id: str) -> NodeState:
        return self._node_states[node_id]

    def update_node_state(self, node_id: str, **kwargs) -> NodeState:
        state = self._node_states[node_id]
        updated = state.model_copy(update=kwargs)
        self._node_states[node_id] = updated
        return self._node_states[node_id]

    def set_variable(self, node_id: str, value: Any) -> Any:
        self._variables[node_id] = value
        self._version += 1
        return self._variables[node_id]
```

# ========== FILE: src/app/engine/workflow/definitions.py ==========
```python
from __future__ import annotations
from enum import Enum
from typing import List, Dict, Any, Optional, Union, Literal, NamedTuple
from pydantic import BaseModel, Field, ConfigDict

from ..schemas.parameter_schema import ParameterSchema, ParameterValue
from ..schemas.form_schema import FormProperty

# ============================================================================
# 1. 权威状态定义 (Authoritative Status Definitions)
# ============================================================================

NodeStatus = Literal["PENDING", "RUNNING", "COMPLETED", "SKIPPED", "STREAMTASK", "STREAMSTART", "STREAMING", "STREAMEND", "FAILED"]

class StreamEvent(BaseModel):
    node_id: str
    status: NodeStatus = "STREAMSTART"
    content: Optional[str] = None

class ErrorBody(BaseModel):
    """标准错误信息结构"""
    message: str = Field(..., description="错误消息")
    type: str = Field(..., description="错误类型")
    data: Optional[Any] = Field(None, description="容错降级数据")

class RuntimeStatus(BaseModel):
    """节点运行时状态，用于容错和调试"""
    isSuccess: bool = Field(..., description="是否执行成功")
    errorBody: Optional[ErrorBody] = Field(None, description="错误详情")

class NodeResultData(BaseModel):
    """
    [权威定义] 节点执行的业务数据结构。
    这是引擎上下文中变量的实际形态。
    """
    output: Dict[str, Any] = Field(default_factory=dict, description="节点的结构化输出，变量树的根")
    content: Optional[str] = Field(None, description="节点的文本内容 (用于 Text 类型输出)")
    error_msg: Optional[str] = None

class NodeExecutionResult(BaseModel):
    """
    节点执行结果容器。
    data: 非流式时为 NodeResultData；流式时为 StreamBroadcaster。
    """
    input: Dict[str, Any] = Field(default_factory=dict, description="最终运行时输入")
    data: Union[NodeResultData, Any] 
    activated_port: str = "0"
    
# ============================================================================
# 2. 策略与控制流定义 (Policy & Control Flow)
#    这些是引擎核心调度逻辑(Orchestrator)直接依赖的数据结构
# ============================================================================

class ExecutionPolicy(BaseModel):
    """
    节点执行策略，严格对齐原型 execute_node 中的逻辑。
    """
    switch: bool = Field(default=False, description="是否开启策略")
    timeoutMs: int = Field(default=180000, description="超时时间(毫秒)")
    retryTimes: int = Field(default=0, description="重试次数")
    # 1=中断, 2=返回固定内容, 3=走异常分支(error端口)
    processType: int = Field(default=1, description="失败后的处理策略") 
    dataOnErr: Optional[str] = Field(None, description="当 processType=2 时返回的默认值")

class BranchLogic(str, Enum):
    """分支逻辑关系"""
    AND = "&"
    OR = "|"

class BranchCondition(BaseModel):
    """
    分支单条条件。
    原型逻辑：left/right 均被视为具备 value 结构的参数对象。
    """
    operator: int = Field(..., description="操作符ID (1-10)")
    # 复用 ParameterSchema，因为原型中通过 getBlockRefValue 解析 left['value']['content']
    # 这意味着 left/right 本质上就是参数定义
    left: ParameterSchema 
    right: ParameterSchema

class BranchGroup(BaseModel):
    """分支组"""
    id: Optional[str] = None
    logic: BranchLogic = Field(default=BranchLogic.AND)
    conditions: List[BranchCondition] = []

# ============================================================================
# 3. 节点配置容器 (Node Configuration)
#    采用“核心严格 + 扩展开放”的策略
# ============================================================================

class BaseNodeConfig(BaseModel):
    """
    节点 data.config 的通用基类，注册节点时继承它。
    """
    # --- 核心调度通用 ---
    executionPolicy: Optional[ExecutionPolicy] = None
    stream: bool = Field(default=False, description="是否作为流式生产者")
    returnType: Optional[str] = Field(None, description="返回类型，如 'Object' 或 'Text'")
    content: Optional[str] = Field(None, description="End/Output 节点的模板内容")

    # --- 默认宽松模式 ---
    model_config = ConfigDict(extra="allow")

# ============================================================================
# 4. 节点与边 (Nodes & Edges)
# ============================================================================

class WorkflowEdge(BaseModel):
    """工作流连线定义"""
    id: Optional[str] = None
    sourceNodeID: str
    targetNodeID: str
    sourcePortID: str
    targetPortID: str

class NodeData(BaseModel):
    """
    这里是引擎与业务数据的交汇点。
    """
    registryId: str = Field(..., min_length=1, max_length=50, description="开发者定义的全局唯一标识，用于锚定引擎中注册的节点函数。如 'Start', 'LLM', 'Agent'")
    name: str = Field("未命名", min_length=1, max_length=100, description="节点名称")
    description: str = Field("", description="节点描述")
    # 核心配置
    config: BaseNodeConfig = Field(default_factory=BaseNodeConfig)
    
    # 复用引擎统一的 ParameterSchema
    inputs: List[ParameterSchema] = Field(default_factory=list)
    outputs: List[ParameterSchema] = Field(default_factory=list)
    
    # --- Loop 节点特有：嵌套子图 ---
    # 使用 ForwardRef 解决 WorkflowNode 的递归引用
    blocks: Optional[List['WorkflowNode']] = Field(None, description="Loop节点的子节点列表")
    edges: Optional[List[WorkflowEdge]] = Field(None, description="Loop节点的子边列表")
    model_config = ConfigDict(extra="forbid")

class WorkflowNode(BaseModel):
    """工作流节点定义"""
    id: str = Field(None, description="自动生成。只保证当前工作流内唯一，用于审计等")
    data: NodeData = Field(..., description="适配前端画布框架，核心运行时数据")
    position: Optional[Dict[str, float]] = None # 坐标信息，引擎透传

# ============================================================================
# 5. 工作流整体结构 (Workflow Graph)
# ============================================================================

class WorkflowGraphDef(BaseModel):
    """
    对应 workflow.data JSON 结构。
    这是引擎执行的静态蓝图。
    """
    nodes: List[WorkflowNode] = Field(default_factory=list)
    edges: List[WorkflowEdge] = Field(default_factory=list)
    viewport: Optional[Dict[str, Any]] = None

# 手动更新前向引用，确保 WorkflowNode 中的 blocks 字段能正确解析 WorkflowNode 类型
NodeData.model_rebuild()
WorkflowNode.model_rebuild()

# ==============================================================================
# 6. 权威节点分类
# ==============================================================================
class NodeCategory(str, Enum):
    COMMON = "common"       # 通用 (Start, End)
    LOGIC = "logic"         # 逻辑 (Loop, Branch)
    MODEL = "model"         # 模型 (LLM)
    TOOL = "tool"           # 工具 (Api, WebBrowser)
    AGENT = "agent"         # 智能体 (Sub-Agent)
    DATA = "data"           # 数据操作 (Database, Knowledge)
    CUSTOM = "custom"       # 自定义扩展

# ==============================================================================
# 7. 节点模板 (Node Template) - 核心载体
# ==============================================================================
class NodeTemplate(BaseModel):
    """
    [权威定义] 节点模版。
    包含了节点的静态 UI 定义，应用层可持久化存储。
    """
    # --- UI 展示元数据 ---
    category: NodeCategory = Field(..., description="节点分类")
    icon: str = Field(..., description="图标标识")
    display_order: int = Field(0, description="排序权重")
    
    # --- 核心预设数据 (The Payload) ---
    data: NodeData = Field(..., description="预设节点数据")

    # --- 编辑器元数据 ---
    # 定义了如何渲染 node_data.config 的表单
    forms: List[FormProperty] = Field(default_factory=list, description="节点的配置表单定义")
    
    is_active: bool = Field(True)
    
    @property
    def registry_id(self) -> str:
        return self.data.registryId

    model_config = ConfigDict(arbitrary_types_allowed=True)
```

# ========== FILE: src/app/engine/workflow/example/__init__.py ==========
```python

```

# ========== FILE: src/app/engine/workflow/example/complex_test.py ==========
```python
import asyncio
import json
from typing import Dict, Any, List

# 引入之前的定义
from ..main import WorkflowEngineService
from .mocks import TestCallbacks, MockLLMNode, FailNode, UnstableWorkerNode
from ...utils.parameter_schema_utils import schemas2obj

async def run_complex_system_test():
    print("\n" + "="*80)
    print("=== FINAL EXAM: Complex System Test (Loop + Fault Tolerance + Branch + Stream) ===")
    print("Scenario: Analyze a list of products. One product causes a crash, but the system should recover.")
    print("="*80 + "\n")

    # -------------------------------------------------------------------------
    # 数据流设计：
    # 1. Start: 输入产品列表 (包含一个 "Buggy Product" 触发故障)
    # 2. Loop (Analyzer): 
    #    - Step A (UnstableWorker): 尝试预处理。遇到 Buggy 会失败，策略是重试1次后降级。
    #    - Step B (MockLLM - JSON): 对预处理结果进行评分。
    #    - Aggregation: 收集所有评分结果。
    # 3. Output: 打印中间聚合结果。
    # 4. Branch: 检查是否需要生成详细报告 (generate_report=True)。
    # 5. Final LLM (Markdown): 如果需要报告，将聚合结果生成 Markdown 摘要。
    # 6. End: 流式输出最终报告。
    # -------------------------------------------------------------------------

    workflow = {
        "nodes": [
            # --- 1. Start Node ---
            {
                "id": "start",
                "data": {
                    "registryId": "Start",
                    "name": "User Input",
                    "outputs": [
                        # 复杂数组定义：Array<String>
                        {
                            "name": "products",
                            "type": "array",
                            "items": {"type": "string"},
                        },
                        # 控制开关
                        {"name": "generate_report", "type": "boolean"}
                    ]
                }
            },

            # --- 2. Loop Node (The Engine Room) ---
            {
                "id": "analyzer_loop",
                "data": {
                    "registryId": "Loop",
                    "name": "Batch Analysis",
                    "config": {
                        "loopType": "list",
                        "loopList": {
                            "name": "list_ref",
                            "type": "array",
                            "items": {"type": "string"},
                            "value": {"type": "ref", "content": {"blockID": "start", "path": "products"}}
                        },
                        "loopCount": None
                    },
                    "inputs": [], # 不需要额外外部变量，使用 loopList 的 item
                    
                    # 定义循环对外输出：聚合内部 LLM 的结果
                    "outputs": [
                        {
                            "name": "batch_results",
                            "type": "array",
                            "items": { 
                                "type": "object", 
                                "properties": [
                                    {"name": "product_name", "type": "string"},
                                    {"name": "score", "type": "string"}
                                ]
                            },
                            "value": {
                                "type": "ref",
                                "content": {
                                    "source": "loop-block-output",
                                    "blockID": "inner_llm", # 聚合内部 inner_llm 的输出
                                    "path": "analysis_json" # 聚合其 output.analysis_json 字段
                                }
                            }
                        }
                    ],
                    
                    # --- Loop 内部子图 ---
                    "blocks": [
                        # Sub-Node A: 不稳定的预处理器
                        {
                            "id": "worker",
                            "data": {
                                "registryId": "UnstableWorker", 
                                "name": "Unstable Preprocessor",
                                "config": {
                                    # [核心测试点] 容错策略
                                    "executionPolicy": {
                                        "switch": True,
                                        "retryTimes": 1,       # 失败重试1次
                                        "processType": 2,      # 2 = 降级 (并不中断循环)
                                        "dataOnErr": "fallback_val" # 引擎暂未自动映射到output，但会标记为完成
                                    }
                                },
                                "inputs": [
                                    {"name": "item", "type": "string", "value": {"type": "ref", "content": {"blockID": "analyzer_loop", "path": "item"}}}
                                ],
                                "outputs": [{"name": "processed_item", "type": "string"}]
                            }
                        },
                        # Sub-Node B: 内部评分 LLM
                        {
                            "id": "inner_llm",
                            "data": {
                                "registryId": "MockLLM",
                                "name": "Score Rater",
                                "config": {"model": "gpt-4", "system_prompt": "", "response_format": "json"}, # 要求 JSON 结构化输出
                                "inputs": [
                                    # 引用上一个节点。如果上一个节点失败降级，这里引用可能拿到 None，
                                    # 或者我们可以引用 runtimeStatus 做更高级处理。
                                    # 这里简单引用 output，MockLLM 会处理 None 为 "parsed..."
                                    {"name": "input_query", "type": "string", "value": {"type": "ref", "content": {"blockID": "worker", "path": "processed_item"}}}
                                ],
                                "outputs": [
                                    # MockLLM 在 JSON 模式下会流式生成这个字段
                                    {"name": "analysis_json", "type": "object"} 
                                ]
                            }
                        }
                    ],
                    # 子图连线
                    "edges": [
                        {"sourceNodeID": "analyzer_loop", "targetNodeID": "worker", "sourcePortID": "loop-function-inline-output", "targetPortID": "0"},
                        {"sourceNodeID": "worker", "targetNodeID": "inner_llm", "sourcePortID": "0", "targetPortID": "0"},
                        {"sourceNodeID": "inner_llm", "targetNodeID": "analyzer_loop", "sourcePortID": "0", "targetPortID": "loop-function-inline-input"}
                    ]
                }
            },

            # --- 3. Output Node (Debugging) ---
            {
                "id": "debug_output",
                "data": {
                    "registryId": "Output",
                    "name": "Debug Log",
                    "outputs": [{"name": "raw_data", "type": "array", "items": {"type":"object"}, "value": {"type": "ref", "content": {"blockID": "analyzer_loop", "path": "batch_results"}}}]
                }
            },

            # --- 4. Branch Node ---
            {
                "id": "gatekeeper",
                "data": {
                    "registryId": "Branch",
                    "name": "Check Mode",
                    "config": {
                        "branchs": [
                            {
                                "id": "yes_report",
                                "conditions": [
                                    # 检查 start.generate_report == True
                                    {
                                        "operator": 1, # Equal
                                        "left": {"name": "L", "type": "boolean", "value": {"type": "ref", "content": {"blockID": "start", "path": "generate_report"}}},
                                        "right": {"name": "R", "type": "boolean", "value": {"type": "literal", "content": True}}
                                    }
                                ]
                            }
                        ]
                    }
                }
            },

            # --- 5. Final Summary LLM (Stream Producer) ---
            {
                "id": "final_writer",
                "data": {
                    "registryId": "MockLLM",
                    "name": "Report Writer",
                    "config": {"model": "gpt-4", "response_format": "markdown"}, # Markdown 流式模式
                    "inputs": [
                        # 将数组转为字符串给 LLM
                        {"name": "input_query", "type": "string", "value": {"type": "ref", "content": {"blockID": "analyzer_loop", "path": "batch_results"}}}
                    ],
                    "outputs": [
                        {"name": "report_md", "type": "string"}
                    ]
                }
            },

            # --- 6. End Node ---
            {
                "id": "end",
                "data": {
                    "registryId": "End",
                    "name": "End Stream",
                    "config": {
                        "returnType": "Text",
                        "stream": True, # [核心] 开启流式响应
                        "content": "# Final Report\n{{report_content}}"
                    },
                    "inputs": [
                        {"name": "report_content", "type": "string", "value": {"type": "ref", "content": {"blockID": "final_writer", "path": "report_md"}}}
                    ]
                }
            }
        ],
        "edges": [
            {"sourceNodeID": "start", "targetNodeID": "analyzer_loop", "sourcePortID": "0", "targetPortID": "0"},
            {"sourceNodeID": "analyzer_loop", "targetNodeID": "debug_output", "sourcePortID": "0", "targetPortID": "0"},
            {"sourceNodeID": "debug_output", "targetNodeID": "gatekeeper", "sourcePortID": "0", "targetPortID": "0"},
            # 分支逻辑：端口 "0" (第一个分支) 激活 -> Final Writer
            {"sourceNodeID": "gatekeeper", "targetNodeID": "final_writer", "sourcePortID": "0", "targetPortID": "0"},
            {"sourceNodeID": "final_writer", "targetNodeID": "end", "sourcePortID": "0", "targetPortID": "0"}
            # 注意：如果分支不匹配，流程会自然结束，或者我们可以连一个 "Else" 节点。
            # 这里我们保证测试数据 inputs 会激活该分支。
        ]
    }

    # 准备测试数据
    # "Buggy Product" 将触发 UnstableWorker 的重试和降级逻辑
    params = {
        "products": ["MacBook Pro", "Buggy Product", "iPhone 15"],
        "generate_report": True 
    }

    engine = WorkflowEngineService()

    try:
        await engine.run(workflow, payload=params, callbacks=TestCallbacks())
    except Exception as e:
        import traceback
        traceback.print_exc()
        print(f"Test Failed with error: {e}")
        raise e

# 运行入口
if __name__ == "__main__":
    asyncio.run(run_complex_system_test())
```

# ========== FILE: src/app/engine/workflow/example/mocks.py ==========
```python
import asyncio
import json
from typing import Dict, Any, List
from pydantic import Field, ConfigDict
from ..definitions import WorkflowGraphDef, WorkflowNode, NodeData, BaseNodeConfig, NodeExecutionResult, NodeResultData, NodeTemplate, NodeCategory, StreamEvent
from ..context import NodeState
from ..registry import register_node, BaseNode
from ...utils.parameter_schema_utils import schemas2obj
from ...utils.stream import StreamBroadcaster
from ...schemas.parameter_schema import ParameterSchema 
from ...schemas.form_schema import FormProperty

# --- 权威定义该节点的配置结构 ---
class LLMNodeConfig(BaseNodeConfig):
    model: str = Field(..., description="模型名称，如 gpt-4")
    system_prompt: str = Field(default="")
    input_query: str = Field("", description="用户输入")
    temperature: float = Field(0.7, ge=0.0, le=2.0)
    top_p: float = Field(default=1.0)
    response_format: str = Field(default="text")
    model_config = ConfigDict(extra="forbid")

MockLLM_TEMPLATE = NodeTemplate(
    category=NodeCategory.MODEL,
    icon="cpu",
    # 核心预设数据 (NodeData)
    data=NodeData(
        registryId="MockLLM",
        name="大语言模型",
        description="调用系统集成的 LLM 模型进行文本生成。",
        # 预设参数
        inputs=[],
        outputs=[ParameterSchema(name="text", type="string", label="生成结果")],
        # 预设配置 (使用 Config 类的默认值)
        config=LLMNodeConfig(model="gpt-4o")
    ),
    
    # UI 表单定义
    forms=[
        FormProperty(
            label="模型名称",
            type="form",
            form_type="model_selector",
            output_key="config.model",
            props={"type": "llm"},
            show_expr=True
        ),
        FormProperty(
            label="系统提示词",
            type="form",
            form_type="textarea",
            output_key="config.system_prompt",
            show_expr=True
        ),
        FormProperty(
            label="随机性",
            type="form",
            form_type="slider",
            output_key="config.temperature",
            props={"min": 0, "max": 1, "step": 0.1},
            show_expr=True
        )
    ]
)

@register_node(template=MockLLM_TEMPLATE)
class MockLLMNode(BaseNode):
    """
    生产级 LLM 节点参考实现。
    支持 Text, Markdown, JSON 三种模式的流式生成与结构化输出。
    """
    async def execute(self) -> NodeExecutionResult:
        # 1. 解析配置与输入
        node_input = await schemas2obj(self.node.data.inputs, self.context.variables)
        prompt = self.node.data.config.system_prompt
        input_query = self.node.data.config.input_query
        
        # 获取输出模式，默认为 text
        # 这里的 config 结构对应前端的配置项
        # 假设 config 结构: { "response_format": "json" | "text" | "markdown" }
        response_format = getattr(self.node.data.config, "response_format", "text") 

        outputs_schema = self.node.data.outputs or []
        
        # 2. 准备流广播器
        broadcaster = None
        if self.is_stream_producer:
            broadcaster = StreamBroadcaster(self.node.id)

        # 3. 定义生成逻辑 (根据模式策略)
        
        async def _generate_text_or_markdown() -> Dict[str, Any]:
            """文本/Markdown 模式：单字段流式"""
            if not outputs_schema:
                primary_key = "text" # Fallback
            else:
                primary_key = outputs_schema[0].name
            
            # 模拟 Markdown 内容
            is_md = response_format == "markdown"
            content = f"## Analysis\n**Echo:** {input_query[::-1]}" if is_md else f"Echo: {input_query[::-1]}"
            
            full_content = ""
            for char in content:
                await asyncio.sleep(0.05)
                if broadcaster:
                    # 广播结构：{ primary_key: char }
                    await broadcaster.broadcast({primary_key: char})
                full_content += char
            
            # 最终结果需要包含其他非主要字段的默认值(如果有)
            base_output = await schemas2obj(outputs_schema, self.context.variables)
            base_output[primary_key] = full_content
            return base_output

        async def _generate_json() -> Dict[str, Any]:
            """JSON 模式：多字段结构化流式"""
            # 模拟 LLM 根据 input_query 生成了对应的 JSON 数据
            # 这里我们要根据 outputs 定义的字段来生成 Mock 数据
            final_data = {}
            
            # 1. 初始化结构
            base_output = await schemas2obj(outputs_schema, self.context.variables)
            final_data.update(base_output)

            # 2. 模拟逐个字段生成 (或者交替生成)
            # 假设我们只对 String 类型的字段进行流式模拟
            target_fields = [f for f in outputs_schema if f.type == 'string']
            
            if not target_fields:
                # 如果没有字符串字段，直接返回结果，不流式广播
                return final_data

            # 模拟数据源
            mock_values = {
                f.name: f"Parsed value for {f.name} based on '{input_query}'" 
                for f in target_fields
            }

            # 模拟流式过程：我们简单地轮流发送每个字段的一个字符
            # 这模拟了 LLM 在生成 JSON 字符串时的增量解析效果
            max_len = max(len(v) for v in mock_values.values())
            
            for i in range(max_len):
                await asyncio.sleep(0.05)
                chunk_delta = {}
                
                for field in target_fields:
                    val = mock_values[field.name]
                    if i < len(val):
                        char = val[i]
                        chunk_delta[field.name] = char
                        # 更新最终结果
                        current_val = final_data.get(field.name, "")
                        final_data[field.name] = current_val + char
                
                if broadcaster and chunk_delta:
                    # 广播包含多个字段 Delta 的字典
                    await broadcaster.broadcast(chunk_delta)

            return final_data

        # 4. 调度任务
        generator_func = _generate_json if response_format == "json" else _generate_text_or_markdown
        
        if broadcaster:
            task = broadcaster.create_task(generator_func())
            return NodeExecutionResult(input=node_input, data=broadcaster)
        else:
            # 非流式直接运行并返回
            output = await generator_func()
            return NodeExecutionResult(input=node_input, data=NodeResultData(output=output))

# 2. Fail Node (用于测试容错)
FAIL_TEMPLATE = NodeTemplate(
    category=NodeCategory.COMMON,
    icon="play",
    data=NodeData(
        registryId="FailNode",
        name="FailNode",
        description="FailNode",
        inputs=[],
        outputs=[]
    ),
    forms=[]
)

@register_node(template=FAIL_TEMPLATE)
class FailNode(BaseNode):
    async def execute(self) -> NodeExecutionResult:
        # 总是抛出异常
        raise ValueError("Intentional Failure for Testing")

UNSTABLEWORKER_TEMPLATE = NodeTemplate(
    category=NodeCategory.COMMON,
    icon="play",
    data=NodeData(
        registryId="UnstableWorker",
        name="UnstableWorker",
        description="UnstableWorker",
        inputs=[],
        outputs=[]
    ),
    forms=[]
)

@register_node(template=UNSTABLEWORKER_TEMPLATE)
class UnstableWorkerNode(BaseNode):
    """
    模拟一个不稳定的处理节点。
    如果输入包含 'Buggy'，则抛出异常，用于测试 executionPolicy。
    如果成功，返回结构化数据。
    """
    async def execute(self) -> NodeExecutionResult:
        node_input = await schemas2obj(self.node.data.inputs, self.context.variables)
        raw_item = node_input.get("item", "")
        
        # 模拟故障：遇到 "Buggy" 必挂
        if "Buggy" in str(raw_item):
            print(f"    [UnstableWorker] Encountered 'Buggy' item. Simulating Crash...")
            raise ValueError(f"Simulated API Failure for item: {raw_item}")
            
        # 正常处理
        return NodeExecutionResult(input=node_input, data=NodeResultData(output={
            "processed_item": f"SAFE_{raw_item}",
            "status": "ok"
        }))

# 3. Callbacks (用于观察执行过程)
class TestCallbacks:
    async def on_execution_start(self, workflow_def: WorkflowGraphDef) -> None:
        print(f"\n>>> Workflow Started")

    async def on_node_start(self, state: NodeState) -> None:
        print(f"[Node Start] {state.node_id}")

    async def on_node_finish(self, state: NodeState) -> None:
        # result 包含 node_id, status, result 等
        print(f"[Node Finish] {state.node_id} Status: {state.status} Result: {state.model_dump_json()}")

    async def on_stream_chunk(self, event: StreamEvent) -> None:
        # End 节点产生的最终流
        print(f"[Stream] {event.content}", end="", flush=True)

    async def on_node_skipped(self, state: NodeState) -> None:
        print(f"[Node Skipped] {state.node_id}")
        
    async def on_node_error(self, state: NodeState) -> None:
        print(f"[Node Error] {state.node_id}: {state.result.error_msg}")

    async def on_execution_end(self, result: NodeResultData) -> None:
        print(f"\n>>> Workflow Ended. Result: {result.model_dump_json()}\n")
    
    async def on_event(self, type: str, data: Any) -> None:
        pass # Ignore others
```

# ========== FILE: src/app/engine/workflow/example/run_suite.py ==========
```python
import asyncio
import json
from ..main import WorkflowEngineService
from .mocks import TestCallbacks, MockLLMNode, FailNode

async def run_basic_flow():
    print("=== Test 1: Basic Linear Flow (Start -> Output -> End) ===")
    # 修正点：模板变量 {{name}} 对应 inputs 中的 name
    
    workflow = {
        "nodes": [
            {
                "id": "start",
                "data": {
                    "registryId": "Start",
                    "name": "Start",
                    "outputs": [{"name": "name", "type": "string"}]
                }
            },
            {
                "id": "end",
                "data": {
                    "registryId": "End",
                    "name": "End",
                    "config": {
                        "returnType": "Text",
                        "content": "Hello {{name}}!" # [Fix] 使用本地 input 变量名
                    },
                    "inputs": [
                        {
                            "name": "name", 
                            "type": "string", 
                            "value": {"type": "ref", "content": {"blockID": "start", "path": "name"}} 
                            # Start 节点通常比较特殊，它的输出直接挂在根路径或由 payload 决定。
                            # 假设 payload={"name": "Prisma"}，Start output={"name": "Prisma"}
                        }
                    ]
                }
            }
        ],
        "edges": [
            {"sourceNodeID": "start", "targetNodeID": "end", "sourcePortID": "0", "targetPortID": "0"}
        ]
    }
    
    engine = WorkflowEngineService()
    await engine.run(workflow, payload={"name": "PrismaSpace"}, callbacks=TestCallbacks())


async def run_streaming_flow():
    print("=== Test 2: Streaming Flow (Start -> MockLLM -> End) ===")
    # 修正点1：模板变量 {{text}} 对应 inputs 中的 text
    # 修正点2：引用路径改为 output.text，因为 NodeExecutionResult 默认包裹在 output 中
    
    workflow = {
        "nodes": [
            {
                "id": "start",
                "data": {
                    "registryId": "Start",
                    "name": "Start",
                    "outputs": [{"name": "topic", "type": "string"}]
                }
            },
            {
                "id": "llm",
                "data": {
                    "registryId": "MockLLM",
                    "name": "LLM Generator",
                    "config": {
                        "model": "gpt-4",
                    },
                    "inputs": [
                        {"name": "input_query", "type": "string", "value": {"type": "ref", "content": {"blockID": "start", "path": "topic"}}}
                    ],
                    "outputs": [
                        {"name": "text", "type": "string"}
                    ]
                }
            },
            {
                "id": "end",
                "data": {
                    "registryId": "End",
                    "name": "End",
                    "config": {
                        "returnType": "Text",
                        "stream": True,
                        "content": "Result: {{text}}" # [Fix] 本地变量名
                    },
                    "inputs": [
                        # [Fix] path="output.text" 对应 MockLLM 返回的 {"output": {"text": ...}}
                        {"name": "text", "type": "string", "value": {"type": "ref", "content": {"blockID": "llm", "path": "text"}}}
                    ]
                }
            }
        ],
        "edges": [
            {"sourceNodeID": "start", "targetNodeID": "llm", "sourcePortID": "0", "targetPortID": "0"},
            {"sourceNodeID": "llm", "targetNodeID": "end", "sourcePortID": "0", "targetPortID": "0"}
        ]
    }
    
    engine = WorkflowEngineService()
    
    await engine.run(workflow, payload={"topic": "12345"}, callbacks=TestCallbacks())


async def run_markdown_flow():
    print("\n=== Test 3: Markdown Output Flow ===")
    # Start -> MockLLM (Markdown Mode) -> End
    
    workflow = {
        "nodes": [
            {"id": "start", "data": {"registryId": "Start", "name": "Start", "outputs": [{"name": "input", "type": "string"}]}},
            {
                "id": "llm",
                "data": {
                    "registryId": "MockLLM",
                    "name": "LLM MD",
                    "config": {
                        "model": "gpt-4",
                        "response_format": "markdown", # [Config] 启用 Markdown
                    },
                    "inputs": [
                        {"name": "input_query", "type": "string", "value": {"type": "ref", "content": {"blockID": "start", "path": "input"}}}
                    ],
                    "outputs": [
                        {"name": "md_content", "type": "string"} # 自定义变量名
                    ]
                }
            },
            {
                "id": "end",
                "data": {
                    "registryId": "End",
                    "name": "End",
                    "config": {
                        "returnType": "Text",
                        "stream": True,
                        "content": "Rendered:\n{{content}}"
                    },
                    "inputs": [
                        {"name": "content", "type": "string", "value": {"type": "ref", "content": {"blockID": "llm", "path": "md_content"}}}
                    ]
                }
            }
        ],
        "edges": [
            {"sourceNodeID": "start", "targetNodeID": "llm", "sourcePortID": "0", "targetPortID": "0"},
            {"sourceNodeID": "llm", "targetNodeID": "end", "sourcePortID": "0", "targetPortID": "0"}
        ]
    }
    
    engine = WorkflowEngineService()
    await engine.run(workflow, payload={"input": "CBA"}, callbacks=TestCallbacks())


async def run_json_flow():
    print("\n=== Test 4: JSON Mode Flow (Structured Output) ===")
    # Start -> MockLLM (JSON Mode with 2 fields) -> End
    # 验证 MockLLM 能同时流式传输两个字段：reasoning 和 answer
    
    workflow = {
        "nodes": [
            {"id": "start", "data": {"registryId": "Start", "name": "Start", "outputs": [{"name": "q", "type": "string"}]}},
            {
                "id": "llm",
                "data": {
                    "registryId": "MockLLM",
                    "name": "LLM JSON",
                    "config": {
                        "model": "gpt-4",
                        "response_format": "json", # [Config] 启用 JSON 模式
                    },
                    "inputs": [
                        {"name": "input_query", "type": "string", "value": {"type": "ref", "content": {"blockID": "start", "path": "q"}}}
                    ],
                    "outputs": [
                        # 定义两个输出字段
                        {"name": "reasoning", "type": "string"},
                        {"name": "answer", "type": "string"}
                    ]
                }
            },
            {
                "id": "end",
                "data": {
                    "registryId": "End",
                    "name": "End",
                    "config": {
                        "returnType": "Text",
                        "stream": True,
                        # 同时引用两个字段，验证流式拼接能力
                        "content": "Think: {{reason}}\nResult: {{ans}}"
                    },
                    "inputs": [
                        {"name": "reason", "type": "string", "value": {"type": "ref", "content": {"blockID": "llm", "path": "reasoning"}}},
                        {"name": "ans", "type": "string", "value": {"type": "ref", "content": {"blockID": "llm", "path": "answer"}}}
                    ]
                }
            }
        ],
        "edges": [
            {"sourceNodeID": "start", "targetNodeID": "llm", "sourcePortID": "0", "targetPortID": "0"},
            {"sourceNodeID": "llm", "targetNodeID": "end", "sourcePortID": "0", "targetPortID": "0"}
        ]
    }
    
    engine = WorkflowEngineService()
    await engine.run(workflow, payload={"q": "Why?"}, callbacks=TestCallbacks())

async def run_fault_tolerance_flow():
    print("=== Test 5: Fault Tolerance (Retry & Fallback) ===")
    
    workflow = {
        "nodes": [
            {
                "id": "start", 
                "data": {"registryId": "Start", "name": "Start", "outputs": []}
            },
            {
                "id": "risky",
                "data": {
                    "registryId": "FailNode",
                    "name": "Risky Node",
                    "config": {
                        "executionPolicy": {
                            "switch": True,
                            "retryTimes": 2,
                            "processType": 2, # Fallback
                            "dataOnErr": "SafeValue"
                        }
                    },
                    "outputs": [{"name": "val", "type": "string"}]
                }
            },
            {
                "id": "end",
                "data": {
                    "registryId": "End",
                    "name": "End",
                    "config": {
                        "returnType": "Text", 
                        # [验证点] 引用 runtimeStatus 中的降级数据
                        "content": "ErrorData: {{errData}}"
                    }, 
                    "inputs": [
                         {
                             "name": "errData", 
                             "type": "string", 
                             # 引用路径：output -> runtimeStatus -> errorBody -> data
                             "value": {"type": "ref", "content": {"blockID": "risky", "path": "runtimeStatus.errorBody.data"}}
                         }
                    ]
                }
            }
        ],
        "edges": [
            {"sourceNodeID": "start", "targetNodeID": "risky", "sourcePortID": "0", "targetPortID": "0"},
            {"sourceNodeID": "risky", "targetNodeID": "end", "sourcePortID": "0", "targetPortID": "0"}
        ]
    }
    
    engine = WorkflowEngineService()
    await engine.run(workflow, payload={}, callbacks=TestCallbacks())


async def run_invalid_flow():
    print("=== Test 6: Invalid Workflow (Cycle) ===")
    workflow = {
        "nodes": [
            {"id": "A", "data": {"registryId": "Start", "name": "A"}},
            {"id": "B", "data": {"registryId": "End", "name": "B"}}
        ],
        "edges": [
            {"sourceNodeID": "A", "targetNodeID": "B", "sourcePortID": "0", "targetPortID": "0"},
            {"sourceNodeID": "B", "targetNodeID": "A", "sourcePortID": "0", "targetPortID": "0"}
        ]
    }
    
    engine = WorkflowEngineService()
    try:
        await engine.run(workflow, payload={})
    except ValueError as e:
        print(f"Caught expected error: {e}")

async def run_complex_flow():
    print("\n=== Test 7: Complex Flow (Loop + LLM Aggregation) ===")
    
    workflow = {
        "nodes": [
            # 1. Start
            {
                "id": "start",
                "data": {
                    "registryId": "Start",
                    "name": "Start",
                    "outputs": [{"name": "list", "type": "array", "items": {
                                "type": "string",
                            }, "value": {"type": "literal", "content": ["short", "long_text"]}}]
                }
            },
            # 2. Loop
            {
                "id": "loop",
                "data": {
                    "registryId": "Loop",
                    "name": "Loop",
                    "config": {
                        "loopType": "list",
                        "loopList": {
                            "name": "loopList",
                            "type": "array",
                            "items": {
                                "type": "string",
                            },
                            "value": {
                                "type": "ref", 
                                "content": {"blockID": "start", "path": "list"}
                            }
                        }
                    },
                    "inputs": [],
                    "outputs": [
                        {
                            "name": "results",
                            "type": "array",
                            "items": {
                                "type": "string",
                            },
                            "value": {
                                "type": "ref",
                                "content": {
                                    "source": "loop-block-output", 
                                    "blockID": "llm",  # 指向子节点
                                    "path": "res"      # 子节点输出路径
                                }
                            }
                        }
                    ],
                    "blocks": [
                        {
                            "id": "llm",
                            "data": {
                                "registryId": "MockLLM",
                                "name": "LLM",
                                "config": {
                                    "model": "gpt-4",
                                },
                                "inputs": [{"name": "input_query", "type": "string", "value": {"type": "ref", "content": {"blockID": "loop", "path": "item"}}}],
                                "outputs": [{"name": "res", "type": "string"}]
                            }
                        }
                    ],
                    "edges": [
                        # 内部连线主要用于控制流，数据流走 Context 引用
                        {"sourceNodeID": "loop", "targetNodeID": "llm", "sourcePortID": "loop-function-inline-output", "targetPortID": "0"},
                        {"sourceNodeID": "llm", "targetNodeID": "loop", "sourcePortID": "0", "targetPortID": "loop-function-inline-input"}
                    ]
                }
            },
            # 3. End
            {
                "id": "end",
                "data": {
                    "registryId": "End",
                    "name": "End",
                    "config": {"returnType": "Object"},
                    "inputs": [{"name": "final", "type": "array", "items": {
                                "type": "string",
                            }, "value": {"type": "ref", "content": {"blockID": "loop", "path": "results"}}}]
                }
            }
        ],
        "edges": [
            {"sourceNodeID": "start", "targetNodeID": "loop", "sourcePortID": "0", "targetPortID": "0"},
            {"sourceNodeID": "loop", "targetNodeID": "end", "sourcePortID": "0", "targetPortID": "0"}
        ]
    }

    engine = WorkflowEngineService()
    await engine.run(workflow, payload={}, callbacks=TestCallbacks())

async def main():
    await run_basic_flow()
    await run_streaming_flow()
    await run_markdown_flow()
    await run_json_flow()
    await run_fault_tolerance_flow()
    await run_invalid_flow()
    await run_complex_flow()
    
if __name__ == "__main__":
    asyncio.run(main())
```

# ========== FILE: src/app/engine/workflow/graph.py ==========
```python
import networkx as nx
from typing import Dict, List, Tuple, Set, Optional
from collections import defaultdict
from .definitions import WorkflowGraphDef, WorkflowNode

class WorkflowGraph:
    """
    工作流图结构的静态分析容器。
    完全复刻 WorkflowValidator 的结构分析能力。
    """
    def __init__(self, graph_def: WorkflowGraphDef):
        self._def = graph_def
        self._nodes_map: Dict[str, WorkflowNode] = {n.id: n for n in graph_def.nodes}
        self._start_node: Optional[WorkflowNode] = None
        self._end_node: Optional[WorkflowNode] = None
        self._nx_graph = nx.DiGraph()
        
        # 索引: (source_node_id, source_port_id) -> List[target_node_id]
        self._edge_index: Dict[Tuple[str, str], List[str]] = defaultdict(list)
        
        self._build_and_validate()

    @property
    def start_node(self) -> WorkflowNode:
        if self._start_node is None:
            raise ValueError("Start node not initialized.")
        return self._start_node

    @property
    def start_node_id(self) -> str:
        return self.start_node.id

    @property
    def end_node(self) -> WorkflowNode:
        if self._end_node is None:
            raise ValueError("End node not initialized.")
        return self._end_node

    @property
    def end_node_id(self) -> str:
        return self.end_node.id

    @property
    def all_nodes(self) -> List[WorkflowNode]:
        return self._def.nodes

    def get_node(self, node_id: str) -> WorkflowNode:
        if node_id not in self._nodes_map:
            raise KeyError(f"Node {node_id} not found in graph.")
        return self._nodes_map[node_id]

    def get_successors(self, node_id: str) -> List[str]:
        if node_id not in self._nx_graph: return []
        return list(self._nx_graph.successors(node_id))

    def get_predecessors(self, node_id: str) -> List[str]:
        if node_id not in self._nx_graph: return []
        return list(self._nx_graph.predecessors(node_id))

    def get_targets_from_port(self, node_id: str, port_id: str) -> List[str]:
        return self._edge_index.get((node_id, port_id), [])

    def _build_and_validate(self):
        # 1. 构建图
        start_nodes = []
        end_nodes = []

        for node in self._def.nodes:
            self._nx_graph.add_node(node.id)
            if node.data.registryId == 'Start':
                start_nodes.append(node)
            elif node.data.registryId == 'End':
                end_nodes.append(node)

        # 校验：必须有且仅有一个 Start 节点
        if len(start_nodes) != 1:
            raise ValueError(f"Workflow must have exactly one 'Start' node. Found {len(start_nodes)}.")
        self._start_node = start_nodes[0]

        # 校验：必须有且仅有一个 End 节点
        if len(end_nodes) != 1:
            raise ValueError(f"Workflow must have exactly one 'End' node. Found {len(end_nodes)}.")
        self._end_node = end_nodes[0]

        # --- 2. 构建边 ---
        for edge in self._def.edges:
            self._nx_graph.add_edge(edge.sourceNodeID, edge.targetNodeID)
            self._edge_index[(edge.sourceNodeID, edge.sourcePortID)].append(edge.targetNodeID)

        # --- 3. 结构连通性检查 ---
        if not nx.is_directed_acyclic_graph(self._nx_graph):
            raise ValueError("Workflow contains cycles (DAG violation).")
            
        # 检查孤立节点
        if len(self._nodes_map) > 0:
            start_id = self.start_node_id
            descendants = nx.descendants(self._nx_graph, start_id)
            descendants.add(start_id)
            all_ids = set(self._nodes_map.keys())
            unreachable = all_ids - descendants
            if unreachable:
                raise ValueError(f"Unreachable nodes found: {unreachable}")
```

# ========== FILE: src/app/engine/workflow/interceptor.py ==========
```python
from typing import Protocol, Callable, Awaitable, Any, List
from .definitions import WorkflowNode
from .context import WorkflowContext, NodeState

# 定义 Next 函数的签名：一个不接受参数、返回 NodeState 的异步函数
NextCall = Callable[[], Awaitable[NodeState]]

class NodeExecutionInterceptor(Protocol):
    """
    [纯粹引擎协议] 节点执行拦截器。
    
    设计原则：
    1. 纯粹性：不依赖 App 层模型（如 User, Trace）。
    2. 包裹性：控制执行的前（Pre）、中（Exec）、后（Post）以及异常（Exception）。
    """
    async def intercept(
        self, 
        node: WorkflowNode, 
        context: WorkflowContext, 
        next_call: NextCall
    ) -> NodeState:
        """
        拦截逻辑。必须在逻辑中调用 await next_call() 来继续执行链。
        """
        ...
```

# ========== FILE: src/app/engine/workflow/main.py ==========
```python
from typing import Dict, Any, Type, Optional

from .definitions import WorkflowGraphDef, NodeResultData
from .orchestrator import WorkflowCallbacks, WorkflowOrchestrator
from .registry import default_node_registry, NodeExecutor
from . import nodes # 导入这些模块以触发 @register_node 装饰器自动注册

class WorkflowEngineService:
    """
    工作流引擎服务门面。
    负责初始化环境、解析定义并启动调度器。
    """

    async def run(
        self, 
        workflow_def: Dict[str, Any], 
        payload: Dict[str, Any] = None,
        callbacks: WorkflowCallbacks = None,
        external_context: Any = None
    ) -> NodeResultData:
        """
        执行工作流。
        :param workflow_def: 工作流定义的原始字典 (JSON)
        :param payload: 全局入参
        :param callbacks: 事件回调处理器
        :return: 最终执行结果
        """
        # 1. 解析并校验图结构
        try:
            graph_def = WorkflowGraphDef.model_validate(workflow_def)
        except Exception as e:
            raise ValueError(f"Invalid workflow definition: {e}")

        # 2. 创建调度器
        # Orchestrator 是纯粹的逻辑核心，每次执行都是独立的
        orchestrator = WorkflowOrchestrator(
            workflow_def=graph_def,
            payload=payload or {},
            callbacks=callbacks,
            external_context=external_context
        )

        # 3. 启动执行
        try:
            result = await orchestrator.execute()
            return result
        except Exception as e:
            # 这里的异常通常是引擎内部未捕获的严重错误
            # 节点级的错误通常会被 Orchestrator 捕获并记录在状态中
            if callbacks:
                await callbacks.on_event("system_error", str(e))
            raise e
```

# ========== FILE: src/app/engine/workflow/nodes/__init__.py ==========
```python
from . import control, loop
```

# ========== FILE: src/app/engine/workflow/nodes/control.py ==========
```python
import re
import asyncio
from typing import Dict, Any, AsyncGenerator, List
from ...utils.parameter_schema_utils import schemas2obj
from ...utils.data_parser import get_value_by_path, get_value_by_expr_template
from ...utils.stream import StreamBroadcaster
from ..registry import register_node, BaseNode, WorkflowRuntimeContext
from ..definitions import WorkflowNode, NodeResultData, ParameterSchema, NodeExecutionResult, StreamEvent
from .template import START_TEMPLATE, END_TEMPLATE, OUTPUT_TEMPLATE, BRANCH_TEMPLATE

# ============================================================================
# 1. Start Node
# ============================================================================
@register_node(template=START_TEMPLATE)
class StartNode(BaseNode):
    """
    工作流开始节点。
    逻辑复刻：直接将全局 payload 映射到输出。
    """
    async def execute(self) -> NodeExecutionResult:
        # 这里的 ready_params 就是工作流的全局入参 (self.context.payload)
        # schemas2obj 会根据 outputs 定义的结构，从 payload 中提取数据
        node_input = await schemas2obj(
            target_schema=self.node.data.outputs, 
            context=self.context.variables, 
            real_data=self.context.payload
        )
        
        return NodeExecutionResult(input=node_input, data=NodeResultData(output=node_input))

# ============================================================================
# 2. Output Node
# ============================================================================
@register_node(template=OUTPUT_TEMPLATE)
class OutputNode(BaseNode):
    """
    中间输出节点。用于在工作流执行过程中输出调试信息或中间结果。
    """
    async def execute(self) -> NodeExecutionResult:
        return NodeExecutionResult(data=NodeResultData(output={}))

# ============================================================================
# 3. End Node (核心流式逻辑)
# ============================================================================
@register_node(template=END_TEMPLATE)
class EndNode(BaseNode):
    """
    工作流结束节点。
    核心职责：生成最终响应。如果配置了流式输出，需要处理流的拼接和透传。
    """

    async def _stream_content_handler(self, template: str) -> AsyncGenerator[str, None]:
        parts = re.split(r'(\{\{[^}]+\}\})', template)
        
        # 缓存解析后的参数，避免重复计算
        cached_params = {}
        last_seen_version = -1

        for part in parts:
            if not part: continue

            match = re.match(r'\{\{([^}]+)\}\}', part)
            if not match:
                yield part
                continue

            variable_path = match.group(1).strip()
            ref_details = self.context.get_ref_details(self.node.id, variable_path)
            block_id = ref_details.get('blockID') if ref_details else None

            source_data = None
            if block_id:
                source_data = self.context.variables.get(block_id)

            # [Case 1] 源是流广播器 -> 走流式快速通道
            if isinstance(source_data, StreamBroadcaster):
                path_in_chunk = ref_details.get('path', '')
                stream_generator = source_data.subscribe()
                async for chunk in stream_generator:
                    value = chunk if isinstance(chunk, str) else await get_value_by_path(chunk, path_in_chunk)
                    if value is not None:
                        yield str(value)
            
            # [Case 2] 源是静态数据 -> 走标准解析通道 (复刻原型)
            else:
                # 检查上下文版本，如果变了就重新解析 inputs
                # 注意：这里依赖 Context 暴露 version 属性，我们在 Context 类里已经加了
                current_version = getattr(self.context, 'version', 0) # 兼容性写法
                
                if current_version > last_seen_version:
                    # 重新解析 inputs 生成本地变量字典
                    cached_params = await schemas2obj(
                        self.node.data.inputs, 
                        self.context.variables
                    )
                    last_seen_version = current_version
                
                # 从解析好的本地参数中直接取值
                value = await get_value_by_path(cached_params, variable_path)
                if value is not None:
                    yield str(value)

    async def execute(self) -> NodeExecutionResult:
        stream = self.node.data.config.stream # 对应原型的 config.stream
        return_type = self.node.data.config.returnType # 对应原型的 config.returnType
        content_template = self.node.data.config.content

        # 场景 A: 开启流式 且 返回类型为 Text 且 有模板
        if stream and return_type == 'Text' and content_template:
            final_content = ""
            try:
                # 通知前端流开始
                await self.context.send('stream_start', StreamEvent(
                    node_id=self.node.id,
                    status='STREAMSTART'
                ))

                # 执行流式拼接
                async for chunk in self._stream_content_handler(content_template):
                    final_content += chunk
                    # 实时推送 chunk
                    await self.context.send('stream_chunk', StreamEvent(
                        node_id=self.node.id,
                        content=chunk,
                        status='STREAMING'
                    ))

                # 通知流结束
                await self.context.send('stream_end', StreamEvent(
                    node_id=self.node.id,
                    status='STREAMEND'
                ))

                # 流式处理完成后，生成最终的结构化 output
                final_params = await schemas2obj(self.node.data.inputs, self.context.variables)
                
                return NodeExecutionResult(input=final_params, data=NodeResultData(output=final_params, content=final_content))

            except Exception as e:
                print(f"Error in End node streaming: {e}")
                return NodeExecutionResult(data=NodeResultData(error_msg=str(e)))

        # 场景 B: 普通非流式返回
        else:
            final_params = await schemas2obj(self.node.data.inputs, self.context.variables)
            content = None
            if return_type == 'Text' and content_template:
                content = await get_value_by_expr_template(content_template, final_params)

            return NodeExecutionResult(input=final_params, data=NodeResultData(output=final_params, content=content))

# ============================================================================
# 4. Branch Node
# ============================================================================
@register_node(template=BRANCH_TEMPLATE)
class BranchNode(BaseNode):
    """
    分支节点。
    逻辑复刻：根据配置的条件列表，计算出激活的端口 ID (activated_port)。
    """
    
    # 复刻原型中的操作符映射
    OPERATOR_MAPPING = {
        1: lambda x, y: x == y,
        2: lambda x, y: x != y,
        3: lambda x, y: len(x) > y if hasattr(x, '__len__') else False,
        4: lambda x, y: len(x) >= y if hasattr(x, '__len__') else False,
        5: lambda x, y: len(x) < y if hasattr(x, '__len__') else False,
        6: lambda x, y: len(x) <= y if hasattr(x, '__len__') else False,
        7: lambda x, y: str(y) in str(x) if x is not None else False,
        8: lambda x, y: str(y) not in str(x) if x is not None else False,
        9: lambda x, y: x is None or x == '',
        10: lambda x, y: x is not None and x != ''
    }

    async def _get_val(self, param_schema: ParameterSchema) -> Any:
        """辅助函数：解析 ParameterSchema 的值 (可能是引用，也可能是字面量)"""
        # 利用 schemas2obj 解析单个字段
        # 我们构造一个临时的 list[ParameterSchema] 传给 schemas2obj
        # schemas2obj 返回 dict，我们取其中的值
        temp_key = "temp_val"
        # 为了不破坏原始对象，我们需要浅拷贝一下 schema 并赋予名字
        # 但 ParameterSchema 是 Pydantic，直接用 schemas2obj 处理单个项的内部逻辑比较复杂
        # 这里为了简单高效，直接复用 schemas2obj 的能力
        
        # 构造一个临时的 schema 列表
        schema_copy = param_schema.model_copy()
        schema_copy.name = temp_key
        
        result_dict = await schemas2obj([schema_copy], self.context.variables)
        return result_dict.get(temp_key)

    async def execute(self) -> NodeExecutionResult:
        activated_port = '-1' # 默认无匹配
        branchs = self.node.data.config.branchs or []

        for branch_index, branch in enumerate(branchs):
            logic = branch.logic # AND / OR
            conditions = branch.conditions

            if not conditions:
                continue
            
            # 计算该分支下所有条件的结果
            results = []
            for condition in conditions:
                # 原型逻辑：left 和 right 都是 value 对象
                # 这里的 left/right 是 ParameterSchema 类型
                left_val = await self._get_val(condition.left)
                right_val = await self._get_val(condition.right)
                operator_id = condition.operator

                op_func = self.OPERATOR_MAPPING.get(operator_id, lambda x, y: False)
                try:
                    res = op_func(left_val, right_val)
                except Exception:
                    res = False
                results.append(res)

            # 根据逻辑关系判断分支是否成立
            is_branch_active = False
            if logic == '&': # AND
                is_branch_active = all(results)
            elif logic == '|': # OR
                is_branch_active = any(results)
            
            if is_branch_active:
                activated_port = str(branch_index)
                break # 找到第一个满足的分支即停止 (if-else if 逻辑)

        # Branch 节点本身不输出数据，只决定流向
        return NodeExecutionResult(input={}, data=NodeResultData(output={}), activated_port=activated_port)
```

# ========== FILE: src/app/engine/workflow/nodes/loop.py ==========
```python
import asyncio
from typing import List, Dict, Any, Tuple
from uuid import uuid4

# 引入定义和基类
from ..definitions import WorkflowNode, NodeData, WorkflowEdge, WorkflowGraphDef, NodeExecutionResult, NodeResultData, ParameterSchema
from ..registry import register_node, BaseNode
from ...utils.parameter_schema_utils import schemas2obj
from ...utils.data_parser import merge_dicts_vanilla
from .template import LOOP_TEMPLATE

@register_node(template=LOOP_TEMPLATE)
class LoopNode(BaseNode):
    """
    循环节点。
    逻辑复刻：动态构建子工作流，为每次迭代注入变量，并聚合结果。
    """

    def _create_standard_sub_workflow(
        self, 
        loop_node_id: str,
        inputs: List[ParameterSchema], 
        outputs: List[ParameterSchema], # 这里的 outputs 是经过筛选后的 loop_sub_outputs
        iteration_index: int, 
        iteration_item: Any
    ) -> Tuple[str, str, WorkflowGraphDef]:
        """
        核心方法：为单次迭代创建一个标准化的子工作流图。
        """
        # 1. 获取子图结构
        # 注意：WorkflowNode.data.blocks 是 List[WorkflowNode]，即已经是 Pydantic 对象
        sub_nodes_list = [n.model_copy(deep=True) for n in (self.node.data.blocks or [])]
        sub_edges_list = [e.model_copy(deep=True) for e in (self.node.data.edges or [])]
        
        # 2. 合成 Start/End 节点 ID
        synthetic_start_id = f"loop_{loop_node_id}_start_{iteration_index}"
        synthetic_end_id = f"loop_{loop_node_id}_end_{iteration_index}"

        # 3. 创建合成 Start 节点
        # 它不需要 inputs/outputs 定义，因为它主要作为图的起点
        synthetic_start_node = WorkflowNode(
            id=synthetic_start_id,
            data=NodeData(
                registryId="Start",
                name=f"Loop-{iteration_index}-Start",
                inputs=[],
                outputs=[]
            )
        )

        # 4. 创建合成 End 节点
        # 它的 inputs 应该是 Loop 节点的“内部输出定义” (loop_sub_outputs)
        # 这样子图中的节点连接到这个 End 节点时，数据结构能对齐
        synthetic_end_node = WorkflowNode(
            id=synthetic_end_id,
            data=NodeData(
                registryId="End",
                name=f"Loop-{iteration_index}-End",
                inputs=outputs, 
                outputs=[]
            )
        )

        # 5. 边重定向逻辑 (核心)
        standardized_edges = []
        for edge in sub_edges_list:
            source_id = edge.sourceNodeID
            target_id = edge.targetNodeID

            # A. 入口重定向: Loop (inline-output) -> Node  ==>  SyntheticStart -> Node
            # 原型判断：sourcePortID == 'loop-function-inline-output'
            if source_id == loop_node_id and edge.sourcePortID == 'loop-function-inline-output':
                standardized_edges.append(WorkflowEdge(
                    sourceNodeID=synthetic_start_id,
                    targetNodeID=target_id,
                    sourcePortID="0", # Start 默认端口
                    targetPortID=edge.targetPortID
                ))
            
            # B. 出口重定向: Node -> Loop (inline-input)  ==>  Node -> SyntheticEnd
            # 原型判断：targetPortID == 'loop-function-inline-input'
            elif target_id == loop_node_id and edge.targetPortID == 'loop-function-inline-input':
                standardized_edges.append(WorkflowEdge(
                    sourceNodeID=source_id,
                    targetNodeID=synthetic_end_id,
                    sourcePortID=edge.sourcePortID,
                    targetPortID="0" # 连入 End 的通常都归一化为 0? 或者根据 ParameterSchema 匹配？
                    # 原型中 End 的 inputs 是动态生成的，边连接到对应 index 的 input。
                    # 但在这里，edge.targetPortID 是 'loop-function-inline-input'。
                    # 实际上，Loop 内部连线通常是连到具体的 Handle。
                    # 原型逻辑：targetPortID="0"。这意味着 SyntheticEnd 只有一个默认入口？
                    # 不，End 节点通常通过 inputs schema 接收数据。
                    # 在原型中，`synthetic_end_node['data']['inputs'] = outputs`。
                    # 这里的 outputs 是 loop_sub_outputs。
                    # 如果有多条线连出来，它们应该连到 End 的不同 input handle。
                    # 但原型代码里写死为了 `targetPortID="0"`。
                    # 这意味着原型假设 Loop 内部输出只汇聚到一个点，或者通过变量引用解决。
                    # 我们严格复刻原型的这行代码：`"targetPortID": "0"`
                ))
            
            # C. 内部边保持不变
            else:
                standardized_edges.append(edge)

        # 6. 组装图
        nodes = [synthetic_start_node] + sub_nodes_list + [synthetic_end_node]
        graph_def = WorkflowGraphDef(
            nodes=nodes,
            edges=standardized_edges
        )
        
        return synthetic_start_id, synthetic_end_id, graph_def

    async def execute(self) -> NodeExecutionResult:
        # 1. 解析基础配置
        loop_node_id = self.node.id
        loop_type = self.node.data.config.loopType or 'count'
        
        # 解析循环控制参数 (count / list)
        loop_params = await schemas2obj(
            [self.node.data.config.loopCount, self.node.data.config.loopList], 
            self.context.variables
        )
        
        # 确定迭代数据源
        loop_data = []
        if loop_type == 'count':
            count_key = self.node.data.config.loopCount.name if self.node.data.config.loopCount else "loopCount"
            count = int(loop_params.get(count_key) or 0)
            loop_data = range(count)
        else:
            list_key = self.node.data.config.loopList.name if self.node.data.config.loopList else "loopList"
            raw_list = loop_params.get(list_key)
            loop_data = raw_list if isinstance(raw_list, list) else []

        # 解析 Loop 节点自身的 inputs (作为子流程的上下文基础)
        node_input = await schemas2obj(self.node.data.inputs, self.context.variables)

        # 2. 分离输出定义 (LoopOutputs vs LoopSubOutputs)
        # 复刻原型逻辑：检查 value.type == 'ref' && source == 'loop-block-output'
        loop_outputs_schema = []     # 最终 Loop 节点对外输出的结构
        loop_sub_outputs_schema = [] # 子流程 End 节点需要收集的结构
        
        for schema in self.node.data.outputs:
            is_inner_output = False
            if schema.value and schema.value.type == 'ref':
                content = schema.value.content
                if isinstance(content, dict) and content.get('source') == 'loop-block-output':
                    is_inner_output = True
            
            if is_inner_output:
                # “拆壳”操作
                item_schema = schema.items # 既然是 Loop 输出，必然是 Array，其 items 定义了单次迭代的结构
                if not item_schema: 
                    # 如果定义不规范，回退使用 schema 本身或跳过
                    continue 

                # 构造子流程 End 节点的输入定义
                # 注意：这里我们需要把 SchemaBlueprint 转回 ParameterSchema
                # 并且名字要用 schema.name (外层名字)
                # 这样 merge_dicts_vanilla 才能正确聚合
                
                # 由于 ParameterSchema 继承自 SchemaBlueprint，我们可以直接构造
                # 但需要补全 name 等字段
                sub_output_item = ParameterSchema(
                    **item_schema.model_dump(exclude={'uid'}), # 排除 uid 避免冲突
                    name=schema.name,
                    # 复用外层的 value 定义 (虽然在 End 节点里它其实没用，因为数据是连线进来的)
                    # 但保持结构一致性
                    value=schema.value 
                )
                loop_sub_outputs_schema.append(sub_output_item)
            else:
                loop_outputs_schema.append(schema)

        all_iterations_results = []

        # 3. 串行执行迭代
        for index, item in enumerate(loop_data):
            # A. 构建子图
            start_id, end_id, sub_graph_def = self._create_standard_sub_workflow(
                loop_node_id,
                self.node.data.inputs,
                loop_sub_outputs_schema,
                index,
                item
            )

            # B. 准备子流程上下文
            # 注入循环变量 (index, item) 以及 Loop 节点的 inputs
            loop_variables = {
                "index": index,
                "item": item,
                **node_input
            }
            
            # 这里的 context key 必须是 loop_node_id
            # 这样子流程中的节点引用 {{LoopNodeID.item}} 才能解析正确
            context_for_subflow = {
                **self.context.variables,
                loop_node_id: loop_variables
            }

            # C. 创建子执行器并运行
            # 使用 self.context.create_sub_workflow_executor (IoC)
            sub_executor = self.context.create_sub_workflow_executor(
                workflow_data=sub_graph_def,
                parent_variables=context_for_subflow,
                payload=self.context.payload
            )
            
            # 执行子工作流
            # 这里调用的是 sub_executor.execute()，它是 Orchestrator 的方法
            # 它会返回 End 节点的 result

            iteration_result_raw = await sub_executor.execute()

            iteration_data = iteration_result_raw.output
            
            all_iterations_results.append(iteration_data)

        # 4. 聚合结果
        # 使用 merge_dicts_vanilla 将 list of dicts 转换为 dict of lists
        # [{"a": 1}, {"a": 2}] -> {"a": [1, 2]}
        loop_sub_result = await merge_dicts_vanilla(all_iterations_results)
        
        # 解析那些非内部引用的固定输出 (如果有)
        loop_def_result = await schemas2obj(loop_outputs_schema, self.context.variables)
        
        # 合并最终输出
        final_output = {**loop_sub_result, **loop_def_result}
        
        return NodeExecutionResult(input=node_input, data=NodeResultData(output=final_output))
```

# ========== FILE: src/app/engine/workflow/nodes/template.py ==========
```python
# app/engine/workflow/nodes/template.py
from typing import List, Optional, Literal
from pydantic import BaseModel, Field, ConfigDict
from ..definitions import WorkflowNode, NodeData, BaseNodeConfig, ExecutionPolicy, NodeTemplate, NodeCategory
from ...schemas.parameter_schema import ParameterSchema 
from ...schemas.form_schema import FormProperty

# ============================================================================
# 1. Start Node Template
# ============================================================================
class StartNodeConfig(BaseNodeConfig):
    model_config = ConfigDict(extra="forbid")

START_TEMPLATE = NodeTemplate(
    category=NodeCategory.COMMON,
    icon="play",
    data=NodeData(
        registryId="Start",
        name="开始",
        description="工作流的起始节点。",
        inputs=[],
        outputs=[], # Start 的 outputs 通常由用户定义
        config=StartNodeConfig()
    ),
    forms=[]
)

# ============================================================================
# 2. End / Output Node Template
# ============================================================================
class OutputNodeConfig(BaseNodeConfig):
    """End 节点和 Output 节点的配置结构"""
    stream: bool = Field(default=False, description="是否作为流式生产者")
    returnType: Optional[Literal["Object", "Text"]] = Field(default="Object", description="返回类型")
    content: Optional[str] = Field(None, description="输出内容的模板 (当 returnType=Text 时有效)")
    model_config = ConfigDict(extra="forbid")

OUTPUT_TEMPLATE = NodeTemplate(
    category=NodeCategory.COMMON,
    icon="stop",
    data=NodeData(
        registryId="Output",
        name="输出",
        description="中间输出节点。",
        inputs=[], 
        outputs=[],
        config=OutputNodeConfig(returnType="Object")
    ),
    forms=[
        FormProperty(
            label="输出方式",
            type="form",
            form_type="radio_group",
            output_key="config.returnType",
            props={"options": [{"label": "结构化对象", "value": "Object"}, {"label": "纯文本", "value": "Text"}]},
            show_expr=True,
            required_expr=True
        )
    ]
)

END_TEMPLATE = NodeTemplate(
    category=NodeCategory.COMMON,
    icon="stop",
    data=NodeData(
        registryId="End",
        name="结束",
        description="工作流的结束节点。",
        inputs=[], 
        outputs=[],
        config=OutputNodeConfig(returnType="Object")
    ),
    forms=[
        FormProperty(
            label="输出方式",
            type="form",
            form_type="radio_group",
            output_key="config.returnType",
            props={"options": [{"label": "结构化对象", "value": "Object"}, {"label": "纯文本", "value": "Text"}]},
            show_expr=True,
            required_expr=True
        )
    ]
)

# ============================================================================
# 3. Branch Node Template
# ============================================================================
class BranchLogic(str):
    AND = "&"
    OR = "|"

class BranchCondition(BaseModel):
    operator: int = Field(..., description="操作符ID (1-10)")
    left: ParameterSchema 
    right: ParameterSchema

class BranchGroup(BaseModel):
    id: Optional[str] = None
    logic: str = Field(default="&") # 使用字符串以便序列化
    conditions: List[BranchCondition] = []

class BranchNodeConfig(BaseNodeConfig):
    branchs: List[BranchGroup] = Field(default_factory=list)
    model_config = ConfigDict(extra="forbid")

BRANCH_TEMPLATE = NodeTemplate(
    category=NodeCategory.COMMON,
    icon="stop",
    data=NodeData(
        registryId="Branch",
        name="条件分支",
        description="条件分支节点。",
        inputs=[], 
        outputs=[],
        config=BranchNodeConfig()
    ),
    forms=[]
)

# ============================================================================
# 4. Loop Node Template
# ============================================================================
class LoopNodeConfig(BaseNodeConfig):
    loopType: Literal["count", "list"] = Field(default="count")
    loopCount: Optional[ParameterSchema] = None 
    loopList: Optional[ParameterSchema] = None
    model_config = ConfigDict(extra="forbid")

LOOP_TEMPLATE = NodeTemplate(
    category=NodeCategory.COMMON,
    icon="play",
    data=NodeData(
        registryId="Loop",
        name="循环",
        description="循环节点。",
        inputs=[],
        outputs=[],
        config=LoopNodeConfig()
    ),
    forms=[]
)
```

# ========== FILE: src/app/engine/workflow/orchestrator.py ==========
```python
import asyncio
import time
import re
from collections import deque
from typing import Dict, Any, List, Set, Optional, Deque, Protocol
from async_timeout import timeout

from .definitions import WorkflowGraphDef, WorkflowNode, NodeExecutionResult, NodeResultData, RuntimeStatus, ErrorBody,  ParameterSchema, StreamEvent
from .graph import WorkflowGraph
from .context import WorkflowContext, NodeState
from .registry import default_node_registry, WorkflowRuntimeContext
from .interceptor import NodeExecutionInterceptor, NextCall
from ..utils.parameter_schema_utils import schemas2obj
from ..utils.stream import Streamable

# 定义回调协议，供上层实现
class WorkflowCallbacks(Protocol):
    async def on_execution_start(self, workflow_def: WorkflowGraphDef) -> None: ...
    async def on_node_start(self, state: NodeState) -> None: ...
    async def on_node_finish(self, state: NodeState) -> None: ...
    async def on_node_error(self, state: NodeState) -> None: ...
    async def on_node_skipped(self, state: NodeState) -> None: ...
    async def on_stream_start(self, event: StreamEvent) -> None: ...
    async def on_stream_chunk(self, event: StreamEvent) -> None: ...
    async def on_stream_end(self, event: StreamEvent) -> None: ...
    async def on_execution_end(self, result: NodeResultData) -> None: ...
    async def on_event(self, type: str, data: Any) -> None: ... # 通用 fallback

class WorkflowOrchestrator(WorkflowRuntimeContext):
    def __init__(
        self, 
        workflow_def: WorkflowGraphDef,
        payload: Dict[str, Any] = None,
        callbacks: WorkflowCallbacks = None,
        parent_variables: Dict[str, Any] = None,
        external_context: Any = None,
        interceptors: List[NodeExecutionInterceptor] = None
    ):
        self.graph = WorkflowGraph(workflow_def)
        self.context_mgr = WorkflowContext(payload or {})
        self.callbacks = callbacks
        self._external_context = external_context
        self.interceptors = interceptors or []
        
        if parent_variables:
            self.context_mgr._variables.update(parent_variables)
        
        for node in self.graph.all_nodes:
            self.context_mgr.init_node_state(node.id)
            
        self.execution_queue: Deque[str] = deque([self.graph.start_node_id])
        self.running_tasks: Dict[str, asyncio.Task] = {} 
        self._variable_ref_cache: Dict[str, Dict] = {}
        self.stream_producers = self._identify_stream_producers()

    @property
    def variables(self) -> Dict[str, Any]:
        return self.context_mgr.variables

    @property
    def payload(self) -> Dict[str, Any]:
        return self.context_mgr.payload
        
    @property
    def version(self) -> int:
        return self.context_mgr.version

    @property
    def external_context(self) -> Any:
        return self._external_context

    async def send(self, type: str, data: Any = None):
        if self.callbacks:
            method_name = f"on_{type}"
            if hasattr(self.callbacks, method_name):
                await getattr(self.callbacks, method_name)(data)
            elif hasattr(self.callbacks, "on_event"):
                await self.callbacks.on_event(type, data)

    def create_sub_workflow_executor(self, workflow_data: Dict[str, Any], parent_variables: Dict[str, Any], payload: Dict[str, Any] = None) -> 'WorkflowOrchestrator':
        try:
            if isinstance(workflow_data, dict):
                graph_def = WorkflowGraphDef.model_validate(workflow_data)
            else:
                graph_def = workflow_data
        except Exception as e:
            raise ValueError(f"Invalid sub-workflow definition: {e}")

        return WorkflowOrchestrator(
            workflow_def=graph_def,
            payload=payload if payload is not None else self.payload,
            callbacks=self.callbacks,
            parent_variables=parent_variables,
            external_context=self._external_context,
            interceptors=self.interceptors
        )

    def get_ref_details(self, consumer_node_id: str, variable_path: str) -> Optional[Dict]:
        cache_key = f"{consumer_node_id}:{variable_path}"
        if cache_key in self._variable_ref_cache:
            return self._variable_ref_cache[cache_key]

        try:
            consumer_node = self.graph.get_node(consumer_node_id)
        except KeyError:
            return None

        top_level_var = variable_path.split('.')[0]
        inputs_schema = consumer_node.data.inputs
        found_ref = self._find_ref_in_schemas(inputs_schema, top_level_var)
        self._variable_ref_cache[cache_key] = found_ref
        return found_ref

    def _find_ref_in_schemas(self, schemas: List[ParameterSchema], target_name: str) -> Optional[Dict]:
        for item in schemas:
            if item.name == target_name:
                if item.value and item.value.type == 'ref':
                    content = item.value.content
                    if isinstance(content, dict):
                        return content
                return None 

            if item.properties:
                found = self._find_ref_in_schemas(item.properties, target_name)
                if found: return found
            
            if item.items and item.items.properties:
                nested_props = [
                    ParameterSchema(**p.model_dump()) 
                    for p in item.items.properties 
                    if hasattr(p, 'name')
                ]
                found = self._find_ref_in_schemas(nested_props, target_name)
                if found: return found
        return None

    def _identify_stream_producers(self) -> Set[str]:
        producers = set()
        for node in self.graph.all_nodes:
            config = node.data.config
            if config.returnType == 'Text' and config.stream:
                content_template = config.content or ''
                if not content_template: continue
                vars_in_template = re.findall(r'\{\{([^}]+)\}\}', content_template)
                for var_path in vars_in_template:
                    var_path = var_path.strip()
                    ref = self.get_ref_details(node.id, var_path)
                    if ref and 'blockID' in ref:
                        producers.add(ref['blockID'])
        return producers

    async def execute(self) -> NodeResultData:
        await self.send('execution_start', self.graph._def)
        final_result = None

        try:
            while self.execution_queue or self.running_tasks:
                nodes_to_run = []
                while self.execution_queue:
                    node_id = self.execution_queue.popleft()
                    state = self.context_mgr.get_node_state(node_id)
                    if state.status != 'PENDING': continue
                    self.context_mgr.update_node_state(node_id, status='RUNNING')
                    nodes_to_run.append(node_id)

                if nodes_to_run:
                    tasks_to_gather = []
                    for node_id in nodes_to_run:
                        task = asyncio.create_task(self._execute_node_wrapper(node_id))
                        self.running_tasks[node_id] = task
                        tasks_to_gather.append(task)
                    
                    if tasks_to_gather:
                        results = await asyncio.gather(*tasks_to_gather, return_exceptions=True)
                        for res in results:
                            if isinstance(res, list): 
                                self.execution_queue.extend(res)
                            elif isinstance(res, Exception):
                                print(f"[Orchestrator] Task error: {res}")

                if not self.execution_queue and self.running_tasks:
                    self.running_tasks = {nid: t for nid, t in self.running_tasks.items() if not t.done()}
                    if not self.running_tasks: continue
                    done, pending = await asyncio.wait(
                        self.running_tasks.values(),
                        return_when=asyncio.FIRST_COMPLETED
                    )
            
            end_node = self.graph.end_node
            st = self.context_mgr.get_node_state(end_node.id)
            if st.status == 'COMPLETED':
                final_result = st.result
                        
        except asyncio.CancelledError:
            raise
        except Exception as e:
            print(f"[Orchestrator] Critical engine error: {e}")
            raise e
        finally:
            if self.running_tasks:
                for nid, task in self.running_tasks.items():
                    if not task.done(): 
                        task.cancel()
                        # 如果这是个流式节点，我们还需要取消 broadcaster 内部的任务
                        # 因为 monitor_task 只是在等待 Event，取消它不会自动取消 LLM 生成
                        node_state = self.context_mgr.get_node_state(nid)
                        var = self.context_mgr.variables.get(nid)
                        if isinstance(var, Streamable):
                            await var.cancel() # 显式取消底层生成
                await asyncio.gather(*self.running_tasks.values(), return_exceptions=True)

        if not isinstance(final_result, NodeResultData):
            # 类型不匹配是严重错误，必须报错
            raise TypeError(f"End node must return NodeResultData. ")
        
        await self.send('execution_end', final_result)
        return final_result

    async def _execute_node_wrapper(self, node_id: str) -> List[str]:
        await self.send('node_start', self.context_mgr.get_node_state(node_id))
        node_def = self.graph.get_node(node_id)
        # --- 核心执行逻辑封装 (The Core) ---
        async def core_execution() -> NodeState:
            policy = node_def.data.config.executionPolicy
            retry_times = policy.retryTimes if policy and policy.switch else 0
            timeout_ms = policy.timeoutMs if policy and policy.switch else 180000
            timeout_sec = float(timeout_ms) / 1000.0
            executor_cls = default_node_registry.get(node_def.data.registryId)
            is_producer = node_id in self.stream_producers
            executor = executor_cls(self, node_def, is_producer)
            result: NodeExecutionResult = None
            is_success = False
            last_exc = None
            # 记录统一的开始时间
            s_time = time.time()
            # 以上步骤出错说明是非法节点就别想继续执行和容错了。
            try:
                for attempt in range(retry_times + 1):
                    try:
                        async with timeout(timeout_sec):
                            result = await executor.execute()
                            is_success = True
                            break
                    except Exception as e:
                        last_exc = e
                if not is_success:
                    raise last_exc or Exception("Execution failed with unknown error")
                e_time = time.time() - s_time
                if isinstance(result.data, Streamable):
                    node_state = self.context_mgr.update_node_state(node_id, input=result.input, status='STREAMTASK')
                    # data是流式 Broadcaster
                    broadcaster = result.data
                    self.context_mgr.set_variable(node_id, broadcaster)
                    # 计算启动消耗了多少时间，计算剩余超时时间
                    remaining_timeout = max(0.1, timeout_sec - e_time)
                    
                    # 将 s_time 传进去，以便结束时计算总耗时
                    monitor_task = asyncio.create_task(self._wait_for_stream(node_id, broadcaster, remaining_timeout, s_time))
                    self.running_tasks[node_id] = monitor_task
                    return node_state
                else:
                    # 成功逻辑：注入 runtimeStatus
                    if policy and policy.switch and policy.processType in [2, 3]:
                        result.data.output["runtimeStatus"] = RuntimeStatus(
                            isSuccess=True, errorBody=None
                        ).model_dump()
                    node_state = self.context_mgr.update_node_state(
                        node_id, 
                        status='COMPLETED', 
                        input=result.input,
                        result=result.data, 
                        activated_port=result.activated_port,
                        executed_time=e_time
                    )
                    
                    self.context_mgr.set_variable(node_id, result.data.output)
                    await self.send('node_finish', node_state)
                    return node_state
                
            except Exception as e:
                # D. 失败处理 (Failure Handling within Core)
                # 现在的 _handle_node_failure 返回 NodeState
                return await self._handle_node_failure(node_id, e, policy)
                
        # --- 责任链构建 (The Chain) ---
        # 初始链条就是核心执行逻辑
        chain: NextCall = core_execution

        # 倒序包装：列表最后的一个拦截器，最先包裹 core_execution
        # 列表第一个拦截器（如 Trace），最后包裹，因此它在洋葱的最外层
        for interceptor in reversed(self.interceptors):
            # 使用默认参数捕获闭包变量，防止循环变量泄漏问题
            def wrap(curr=interceptor, nxt=chain):
                return curr.intercept(node_def, self.context_mgr, nxt)
            chain = wrap

        # --- 触发执行 ---
        try:
            final_state = await chain()
            # 调度触发条件
            # 逻辑：只有在成功(COMPLETED) 或 流式任务启动(STREAMTASK) 时触发后续
            # 流任务也继续调度，让不依赖它的节点继续执行，依赖节点会自动等流结果
            # FAILED 状态（且未被策略挽救）不应触发后续
            if final_state.status in ("COMPLETED", "STREAMTASK"):
                return await self._evaluate_successors(node_id)
            return []
        except Exception as e:
            # 理论上 interceptors 和 core 应该捕获所有异常
            # 如果走到这里，说明是系统级严重错误，工作流在该分支直接终止。
            print(f"[Orchestrator] Critical unhandled error in node {node_id}: {e}")
            node_state = self.context_mgr.update_node_state(node_id, status='FAILED', result=NodeResultData(error_msg=str(e)))
            await self.send('node_error', node_state)
            return []

    async def _handle_node_failure(self, node_id: str, error: Exception, policy: Any) -> NodeState:
        process_type = policy.processType if policy and policy.switch else 1
        
        if process_type in [2, 3]:
            node_def = self.graph.get_node(node_id)
            try:
                base_output = await schemas2obj(node_def.data.outputs, self.variables)
            except:
                base_output = {}

            status = RuntimeStatus(
                isSuccess=False,
                errorBody=ErrorBody(
                    message=str(error),
                    type=type(error).__name__,
                    data=policy.dataOnErr or ""
                )
            )

            final_data = NodeResultData(
                output={
                    **base_output,
                    "runtimeStatus": status.model_dump()
                }
            )
            activated_port = "0" if process_type == 2 else "error"

            node_state = self.context_mgr.update_node_state(
                node_id, 
                status='COMPLETED', 
                result=final_data,
                activated_port=activated_port
            )
            self.context_mgr.set_variable(node_id, final_data.output)
            await self.send('node_finish', node_state)
            return node_state
            
        node_state = self.context_mgr.update_node_state(node_id, status='FAILED', result=NodeResultData(error_msg=str(error)))
        await self.send('node_error', node_state)
        return node_state

    async def _wait_for_stream(self, node_id: str, broadcaster: Streamable, timeout_sec: float, start_time: float):
        try:
            # 如果超时，它会抛出 asyncio.TimeoutError 并自动取消内部的 await
            output = await asyncio.wait_for(
                broadcaster.get_result(), 
                timeout=timeout_sec
            )
            # 计算包含启动+流传输的总耗时
            total_duration = time.time() - start_time
            node_state = self.context_mgr.update_node_state(
                node_id, 
                status='COMPLETED', 
                result=NodeResultData(output=output),
                executed_time=total_duration
            )
            self.context_mgr.set_variable(node_id, output)
            await self.send('node_finish', node_state)
            next_nodes = await self._evaluate_successors(node_id)
            self.execution_queue.extend(next_nodes)
        except asyncio.TimeoutError:
            # [超时处理]
            print(f"[Orchestrator] Node {node_id} timed out after {timeout_sec}s.")
            
            # 1. 掐断上游：调用 Broadcaster 的 cancel
            await broadcaster.cancel()
            
            # 2. 标记失败
            node_state = self.context_mgr.update_node_state(
                node_id, 
                status='FAILED',
                result=NodeResultData(error_msg=f"Execution timed out after {timeout_sec}s")
            )
            # 发送错误事件
            await self.send('node_error', node_state)
        except Exception as e:
            print(f"[Orchestrator] Stream task for {node_id} failed: {e}")
            node_state = self.context_mgr.update_node_state(node_id, status='FAILED', result=NodeResultData(error_msg=str(e)))
            await self.send('node_error', node_state)

    async def _evaluate_successors(self, completed_node_id: str) -> List[str]:
        nodes_to_queue = []
        successors = self.graph.get_successors(completed_node_id)
        for succ_id in successors:
            await self._evaluate_single_node(succ_id, nodes_to_queue)
        return nodes_to_queue

    async def _evaluate_single_node(self, node_id: str, queue: List[str]):
        state = self.context_mgr.get_node_state(node_id)
        if state.status != 'PENDING': return

        predecessors = self.graph.get_predecessors(node_id)
        if not predecessors: return

        ready_states = {'COMPLETED', 'SKIPPED', 'STREAMTASK'}
        for pred_id in predecessors:
            pred_state = self.context_mgr.get_node_state(pred_id)
            if pred_state.status not in ready_states: return

        is_active = False
        for pred_id in predecessors:
            pred_state = self.context_mgr.get_node_state(pred_id)
            if pred_state.status in {'COMPLETED', 'STREAMTASK'}:
                targets = self.graph.get_targets_from_port(pred_id, pred_state.activated_port)
                if node_id in targets:
                    is_active = True
                    break 

        if is_active:
            queue.append(node_id)
        else:
            all_predecessors_finished = True
            for pred_id in predecessors:
                pred_state = self.context_mgr.get_node_state(pred_id)
                if pred_state.status not in {'COMPLETED', 'SKIPPED'}:
                    all_predecessors_finished = False
                    break
            
            if all_predecessors_finished:
                node_state = self.context_mgr.update_node_state(node_id, status='SKIPPED')
                await self.send('node_skipped', node_state)
                recursive_new_nodes = await self._evaluate_successors(node_id)
                queue.extend(recursive_new_nodes)
```

# ========== FILE: src/app/engine/workflow/registry.py ==========
```python
from typing import Dict, Type, Any, Optional, Protocol, Union
from .definitions import WorkflowNode, NodeExecutionResult, NodeResultData, NodeData, NodeTemplate

# ============================================================================
# 1. 协议定义 (Protocols)
# ============================================================================

class WorkflowRuntimeContext(Protocol):
    """
    [依赖倒置] 定义节点在执行过程中能访问的引擎能力。
    这对应原型中节点类里的 `self.executor`。
    任何传递给节点的 'executor' 或 'context' 必须实现此协议。
    """
    @property
    def variables(self) -> Dict[str, Any]:
        """获取当前工作流的变量池 (Read/Write)"""
        ...
    
    @property
    def payload(self) -> Dict[str, Any]:
        """获取工作流的全局入参"""
        ...

    @property
    def version(self) -> int:
        ...

    @property
    def external_context(self) -> Any:
        ...

    async def send(self, type: str, data: Any = None) -> None:
        """发送事件通知 (对应原型的 executor.send)"""
        ...
        
    def create_sub_workflow_executor(self, workflow_data: Dict[str, Any], context: Dict[str, Any]) -> Any:
        """
        [关键] 允许 Loop 节点创建子工作流执行器。
        返回的对象应具有 execute() 方法。
        """
        ...

    def get_ref_details(self, consumer_node_id: str, variable_path: str) -> Optional[Dict]:
        """允许节点查询变量引用的源头 (用于 End 节点的流式拼接)"""
        ...

class NodeExecutor(Protocol):
    """
    [核心契约] 所有节点实现类（无论是基础的还是扩展的）必须遵循的接口。
    """
    def __init__(
        self, 
        context: WorkflowRuntimeContext, 
        node: WorkflowNode, 
        is_stream_producer: bool
    ):
        """
        初始化节点执行器。
        :param context: 运行时上下文，提供变量访问、事件发送等能力
        :param node: 当前节点的完整定义 (Pydantic Model)
        :param is_stream_producer: 标记当前节点是否被下游流式消费 (图编译阶段计算得出)
        """
        ...

    async def execute(self) -> NodeExecutionResult:
        """
        执行节点逻辑。
        :return: 标准化的执行结果
        """
        ...

class BaseNode:
    """
    所有节点的通用基类。
    负责处理标准的初始化逻辑，将参数绑定到实例属性。
    """
    # 静态属性，存储该节点类型的模版定义
    template: NodeTemplate = None 

    def __init__(
        self, 
        context: WorkflowRuntimeContext, 
        node: WorkflowNode, 
        is_stream_producer: bool
    ):
        self.context = context
        self.node = node
        self.node.data.config = self.template.data.config.model_validate(node.data.config.model_dump())
        self.is_stream_producer = is_stream_producer
        
# ============================================================================
# 3. 注册中心 (Registry)
# ============================================================================

class NodeRegistry:
    """
    节点注册中心。
    """
    def __init__(self):
        self._executors: Dict[str, Type[NodeExecutor]] = {}
        self._templates: Dict[str, NodeTemplate] = {}

    def register(self, template: NodeTemplate):
        """
        [Upgrade] 注册节点，必须提供完整的 NodeTemplate。
        """
        if not isinstance(template, NodeTemplate):
            raise TypeError("Must register with a NodeTemplate instance.")
        
        registryId = template.data.registryId

        def decorator(cls):
            # 将 template 绑定到类上
            cls.template = template
            
            self._executors[registryId] = cls
            self._templates[registryId] = template
            return cls
        return decorator

    def get(self, registry_id: str) -> Type[NodeExecutor]:
        executor_cls = self._executors.get(registry_id)
        if not executor_cls:
            raise ValueError(f"No executor registered for node registry id '{registry_id}'.")
        return executor_cls

    def has(self, registry_id: str) -> bool:
        return registry_id in self._executors

    def get_all_templates(self) -> Dict[str, NodeTemplate]:
        """获取所有注册节点的模版"""
        return self._templates

# ============================================================================
# 4. 全局实例与辅助函数 (Global Instance & Helpers)
# ============================================================================

# 创建一个默认的全局注册表，方便上层直接使用装饰器
default_node_registry = NodeRegistry()
register_node = default_node_registry.register
```

